{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Feature Store Notebook showing use of Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of an AWS blog that shows how to use \"Time Travel\" by leveraging SageMaker Feature Store. This particular notebook (#2) is used to generate aggregate data attributes (i.e. averages and sums) from the raw transaction data generated in previous notebook (#1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.Session().client(service_name='sagemaker')\n",
    "smfs_runtime = boto3.Session().client(service_name='sagemaker-featurestore-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by Deleting Feature Groups that we will re-create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog\n"
     ]
    }
   ],
   "source": [
    "# Use SageMaker default bucket\n",
    "BUCKET = sagemaker.Session().default_bucket()\n",
    "BASE_PREFIX = \"sagemaker-featurestore-blog\"\n",
    "\n",
    "FEATURE_GROUP = \"cc-agg-batch-fg\"\n",
    "\n",
    "# Note that FeatureStore will append this to base prefix -> \"{account_id}/sagemaker/{region}/offline-store/\"\n",
    "OFFLINE_STORE_BASE_URI = f's3://{BUCKET}/{BASE_PREFIX}'\n",
    "\n",
    "print(OFFLINE_STORE_BASE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted feature group\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sm_client.delete_feature_group(FeatureGroupName=FEATURE_GROUP) \n",
    "    print('deleted feature group')\n",
    "except:\n",
    "    print('Error: feature group not deleted; it may not exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recreate the Feature Groups using Schema definition files\n",
    "Each feature group contains configuration parameters for Offline and Online stores. The feature group uses a schema definition file (JSON) that dictates the feature names and types. Below we display these local schema files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema files on in the local 'schema' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Aggregated features for each credit card, streamed intraday\"\u001b[39;49;00m,\r\n",
      "    \u001b[94m\"features\"\u001b[39;49;00m: [\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"tid\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"string\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Transaction ID (Unique)\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"cc_num\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"bigint\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Credit Card Number (Unique)\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"num_trans_last_7d\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"bigint\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Aggregated Metric: Average number of transactions for the card aggregated by past 7 days\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"avg_amt_last_7d\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"double\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Aggregated Metric: Average transaction amount for the card aggregated by past 7 days\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"event_time\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"string\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Externally generated event timestamp\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"trans_time\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"double\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Required feature for transaction timestamp\"\u001b[39;49;00m\r\n",
      "          }\r\n",
      "      ],\r\n",
      "    \r\n",
      "      \u001b[94m\"record_identifier_feature_name\"\u001b[39;49;00m: \u001b[33m\"cc_num\"\u001b[39;49;00m,\r\n",
      "      \u001b[94m\"event_time_feature_name\"\u001b[39;49;00m: \u001b[33m\"trans_time\"\u001b[39;49;00m,\r\n",
      "      \u001b[94m\"tags\"\u001b[39;49;00m: [{\u001b[94m\"Key\"\u001b[39;49;00m: \u001b[33m\"Environment\"\u001b[39;49;00m, \u001b[94m\"Value\"\u001b[39;49;00m : \u001b[33m\"DEV\"\u001b[39;49;00m}, \r\n",
      "               {\u001b[94m\"Key\"\u001b[39;49;00m: \u001b[33m\"IngestionType\"\u001b[39;49;00m, \u001b[94m\"Value\"\u001b[39;49;00m: \u001b[33m\"Batch\"\u001b[39;49;00m},\r\n",
      "               {\u001b[94m\"Key\"\u001b[39;49;00m: \u001b[33m\"CostCenter\"\u001b[39;49;00m, \u001b[94m\"Value\"\u001b[39;49;00m: \u001b[33m\"C18\"\u001b[39;49;00m}]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize schema/cc-agg-batch-fg-schema.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_to_defs(filename):\n",
    "    schema = json.loads(open(filename).read())\n",
    "    \n",
    "    feature_definitions = []\n",
    "    \n",
    "    for col in schema['Features']:\n",
    "        feature = {'FeatureName': col['name']}\n",
    "        if col['type'] == 'double':\n",
    "            feature['FeatureType'] = 'Fractional'\n",
    "        elif col['type'] == 'bigint':\n",
    "            feature['FeatureType'] = 'Integral'\n",
    "        else:\n",
    "            feature['FeatureType'] = 'String'\n",
    "        feature_definitions.append(feature)\n",
    "\n",
    "    return feature_definitions, schema['record_identifier_feature_name'], schema['event_time_feature_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_group_from_schema(filename, fg_name, role_arn=None, s3_uri=None):\n",
    "    schema = json.loads(open(filename).read())\n",
    "    \n",
    "    feature_defs = []\n",
    "    \n",
    "    for col in schema['features']:\n",
    "        feature = {'FeatureName': col['name']}\n",
    "        if col['type'] == 'double':\n",
    "            feature['FeatureType'] = 'Fractional'\n",
    "        elif col['type'] == 'bigint':\n",
    "            feature['FeatureType'] = 'Integral'\n",
    "        else:\n",
    "            feature['FeatureType'] = 'String'\n",
    "        feature_defs.append(feature)\n",
    "\n",
    "    record_identifier_name = schema['record_identifier_feature_name']\n",
    "    event_time_name = schema['event_time_feature_name']\n",
    "\n",
    "    if role_arn is None:\n",
    "        role_arn = get_execution_role()\n",
    "\n",
    "    if s3_uri is None:\n",
    "        offline_config = {}\n",
    "    else:\n",
    "        print(f'Creating Offline Store at: {s3_uri}')\n",
    "        offline_config = {'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': s3_uri}}}\n",
    "        \n",
    "    sm_client.create_feature_group(\n",
    "        FeatureGroupName = fg_name,\n",
    "        RecordIdentifierFeatureName = record_identifier_name,\n",
    "        EventTimeFeatureName = event_time_name,\n",
    "        FeatureDefinitions = feature_defs,\n",
    "        Description = schema['description'],\n",
    "        Tags = schema['tags'],\n",
    "        OnlineStoreConfig = {'EnableOnlineStore': True},\n",
    "        RoleArn = role_arn,\n",
    "        **offline_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the new Feature Groups using the schema definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Offline Store at: s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog\n"
     ]
    }
   ],
   "source": [
    "create_feature_group_from_schema('schema/cc-agg-batch-fg-schema.json', FEATURE_GROUP, \n",
    "                                 role_arn=role, s3_uri=OFFLINE_STORE_BASE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure the new Feature Groups exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupSummaries': [{'FeatureGroupName': 'transaction-feature-group-28-23-38-18',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/transaction-feature-group-28-23-38-18',\n",
       "   'CreationTime': datetime.datetime(2021, 4, 28, 23, 38, 24, 613000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'transaction-feature-group-28-23-34-33',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/transaction-feature-group-28-23-34-33',\n",
       "   'CreationTime': datetime.datetime(2021, 4, 28, 23, 34, 33, 532000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'transaction-feature-group-28-19-38-11',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/transaction-feature-group-28-19-38-11',\n",
       "   'CreationTime': datetime.datetime(2021, 4, 28, 19, 39, 32, 781000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'transaction-feature-group-27-20-08-49',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/transaction-feature-group-27-20-08-49',\n",
       "   'CreationTime': datetime.datetime(2021, 4, 27, 20, 8, 58, 393000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'pyspark-fg',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/pyspark-fg',\n",
       "   'CreationTime': datetime.datetime(2021, 4, 20, 18, 54, 48, 557000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created'},\n",
       "  {'FeatureGroupName': 'orders-feature-group-21-14-45-34',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/orders-feature-group-21-14-45-34',\n",
       "   'CreationTime': datetime.datetime(2021, 4, 21, 14, 47, 4, 578000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'identity-feature-group-28-23-38-18',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/identity-feature-group-28-23-38-18',\n",
       "   'CreationTime': datetime.datetime(2021, 4, 28, 23, 38, 18, 989000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'identity-feature-group-28-23-34-33',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/identity-feature-group-28-23-34-33',\n",
       "   'CreationTime': datetime.datetime(2021, 4, 28, 23, 34, 33, 124000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'identity-feature-group-28-19-38-11',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/identity-feature-group-28-19-38-11',\n",
       "   'CreationTime': datetime.datetime(2021, 4, 28, 19, 39, 30, 241000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'identity-feature-group-27-20-08-49',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/identity-feature-group-27-20-08-49',\n",
       "   'CreationTime': datetime.datetime(2021, 4, 27, 20, 8, 57, 684000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}}],\n",
       " 'NextToken': 'cIws2QhTXUIa8bi8WLP3qirWMETqiu37tspr0RLbV7qddfqxwr2+9hj2mAzWEfMzvvAg4bdZ3qUzSiO4bsZ+kL3kuvJwHhcmTFKD0JAMCI5i4adolsuG+dTn3Kpk8E97WxObeyYxXlDuGRBJY9/8hAAC+ylYXratUqZEer7jbsTXksbkIWFyDjnRaPV4KiUzeSSCdIi0RKDOMi5oBjIPTapgF6D51NOWeJWV2SsgVs6MpVRBAPF4dqNqKiwJH4kqmmabESAHblcZvfQfifIT34l2glrEmyXCzl7/SKiFw2wFMHlOBhitXFzsV6mG6caw7wx3Q24bHeZxVBfP76fuTjfTXfBeOZl5UQHUQZbauh/Ux6saO5FywfwCcWBUS/p50kWvg363fkDX5EZXxFXgEJ/09ZOWHxvqsS00iN3PXOhVNZuAOTkb+GK86Huu7wqHHyElPTb3Hku5KWRM+DyD1+KrlVTFX/Eg1AHRF+/XPOseWt7bCta05xPO5k1SotXS1nm1OizPjGntRpeWtPPoeFj9NCciejVrXSVAGLhms0MzS60YRGH8/GO8KZh/dOy9DxxD',\n",
       " 'ResponseMetadata': {'RequestId': '301890c7-9151-49e7-8d4c-de2be5550766',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '301890c7-9151-49e7-8d4c-de2be5550766',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '3273',\n",
       "   'date': 'Fri, 30 Apr 2021 21:57:25 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.list_feature_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe each feature group\n",
    "Note that each feature group gets its own ARN, allowing you to manage IAM policies that control access to individual feature groups. The feature names and types are displayed, and the record identifier and event time features are called out specifically. Notice that there is only an `OnlineStoreConfig` and no `OfflineStoreConfig`, as we have decided not to replicate features offline for these groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/cc-agg-batch-fg',\n",
       " 'FeatureGroupName': 'cc-agg-batch-fg',\n",
       " 'RecordIdentifierFeatureName': 'cc_num',\n",
       " 'EventTimeFeatureName': 'trans_time',\n",
       " 'FeatureDefinitions': [{'FeatureName': 'tid', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'cc_num', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'num_trans_last_7d', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'avg_amt_last_7d', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'event_time', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'trans_time', 'FeatureType': 'Fractional'}],\n",
       " 'CreationTime': datetime.datetime(2021, 4, 30, 21, 56, 10, 133000, tzinfo=tzlocal()),\n",
       " 'OnlineStoreConfig': {'EnableOnlineStore': True},\n",
       " 'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': 's3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog',\n",
       "   'ResolvedOutputS3Uri': 's3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog/572539092864/sagemaker/us-east-1/offline-store/cc-agg-batch-fg-1619819770/data'},\n",
       "  'DisableGlueTableCreation': False,\n",
       "  'DataCatalogConfig': {'TableName': 'cc-agg-batch-fg-1619819770',\n",
       "   'Catalog': 'AwsDataCatalog',\n",
       "   'Database': 'sagemaker_featurestore'}},\n",
       " 'RoleArn': 'arn:aws:iam::572539092864:role/service-role/AmazonSageMaker-ExecutionRole-20200407T174741',\n",
       " 'FeatureGroupStatus': 'Created',\n",
       " 'Description': 'Aggregated features for each credit card, streamed intraday',\n",
       " 'ResponseMetadata': {'RequestId': 'e4e2cbd2-48b4-4f0a-9848-8cb68242d44b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'e4e2cbd2-48b4-4f0a-9848-8cb68242d44b',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1287',\n",
       "   'date': 'Fri, 30 Apr 2021 21:58:05 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.describe_feature_group(FeatureGroupName=FEATURE_GROUP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Batch Ingestion\n",
    "**This section of the notebook aggregates raw features into new derived features that is used for Fraud Detection model training/inference.**\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Create PySpark Processing Script](#Create-PySpark-Processing-Script)\n",
    "1. [Run SageMaker Processing Job](#Run-SageMaker-Processing-Job)\n",
    "1. [Explore Aggregated Features](#Explore-Aggregated-Features)\n",
    "1. [Validate Feature Group for Records](#Validate-Feature-Group-for-Records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Background\n",
    "\n",
    "- This notebook takes raw credit card transactions data (csv) generated by \n",
    "[notebook 1](./1_generate_creditcard_transactions.ipynb) and aggregates the raw features to create new features (ratios) by running a <b>SageMaker Processing</b> PySpark Job. These aggregated features alongside the raw original features will be leveraged in the historical query (\"Time Travel\") notebook in the last step (see notebook [notebook 3](./3_featurestore_timetravel_historical_query.ipynb)).\n",
    "\n",
    "- As part of the Spark job, we also select the latest daily aggregated features - `num_trans_last_7d` and `avg_amt_last_7d` grouped by `cc_num` (credit card number) and populate these features into the <b>SageMaker Online Feature Store</b> as a feature group. This feature group (`cc-agg-batch-fg`) will be created in this notebook via the PySpark script below.\n",
    "\n",
    "- [Amazon SageMaker Processing](https://aws.amazon.com/about-aws/whats-new/2020/09/amazon-sagemaker-processing-now-supports-built-in-spark-containers-for-big-data-processing/) lets customers run analytics jobs for data engineering and model evaluation on Amazon SageMaker easily and at scale. It provides a fully managed Spark environment for data processing or feature engineering workloads.\n",
    "\n",
    "<img src=\"./images/batch_ingestion.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import logging\n",
    "import random\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker version: 2.38.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Using SageMaker version: {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Batch Aggregation using SageMaker PySpark Processing Job]\n"
     ]
    }
   ],
   "source": [
    "logger.info('[Batch Aggregation using SageMaker PySpark Processing Job]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-featurestore-blog/raw\n",
      "sagemaker-featurestore-blog/aggregated\n"
     ]
    }
   ],
   "source": [
    "# Setup S3 prefixes for Spark Job\n",
    "\n",
    "INPUT_KEY_PREFIX = os.path.join(BASE_PREFIX, 'raw')\n",
    "OUTPUT_KEY_PREFIX = os.path.join(BASE_PREFIX, 'aggregated')\n",
    "LOCAL_DIR = './data'\n",
    "\n",
    "print(INPUT_KEY_PREFIX)\n",
    "print(OUTPUT_KEY_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Create PySpark Script\n",
    "This PySpark script does the following:\n",
    "\n",
    "1. Aggregates raw features to derive new features (ratios).\n",
    "2. Saves the aggregated features alongside the original raw features into a CSV file and writes it to S3 - will be used in the next step for model training.\n",
    "3. Groups the aggregated features by credit card number and picks selected aggregated features to write to SageMaker Feature Store (Online). <br>\n",
    "<b>Note: </b> The feature group was created in the previous notebook (`1_setup.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch_aggregation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch_aggregation.py\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, TimestampType, LongType\n",
    "from pyspark.sql.functions import desc, dense_rank\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from  argparse import Namespace, ArgumentParser\n",
    "from pyspark.sql.window import Window\n",
    "import argparse\n",
    "import logging\n",
    "import boto3\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "TOTAL_UNIQUE_USERS = 10000 \n",
    "FEATURE_GROUP = 'cc-agg-batch-fg'\n",
    "\n",
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "feature_store_client = boto3.client(service_name='sagemaker-featurestore-runtime')\n",
    "\n",
    "\n",
    "def parse_args() -> Namespace:\n",
    "    parser = ArgumentParser(description='Spark Job Input and Output Args')\n",
    "    parser.add_argument('--s3_input_bucket', type=str, help='S3 Input Bucket')\n",
    "    parser.add_argument('--s3_input_key_prefix', type=str, help='S3 Input Key Prefix')\n",
    "    parser.add_argument('--s3_output_bucket', type=str, help='S3 Output Bucket')\n",
    "    parser.add_argument('--s3_output_key_prefix', type=str, help='S3 Output Key Prefix')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "    \n",
    "\n",
    "def define_schema() -> StructType:\n",
    "    schema = StructType([StructField('tid', StringType(), True),\n",
    "                         StructField('event_time', TimestampType(), True),\n",
    "                         StructField('cc_num', LongType(), True),\n",
    "                         StructField('amount', DoubleType(), True),\n",
    "                         StructField('fraud_label', StringType(), True)])\n",
    "    return schema\n",
    "\n",
    "\n",
    "def aggregate_features(args: Namespace, schema: StructType, spark: SparkSession) -> DataFrame:\n",
    "    logger.info('[Read Raw Transactions Data as Spark DataFrame]')\n",
    "    transactions_df = spark.read.csv(f's3a://{os.path.join(args.s3_input_bucket, args.s3_input_key_prefix)}', \\\n",
    "                                     header=False, \\\n",
    "                                     schema=schema)\n",
    "    \n",
    "    logger.info('[Aggregate Transactions to Derive New Features using Spark SQL]')\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT *, \\\n",
    "           avg_amt_last_60m/avg_amt_last_7d AS amt_ratio1, \\\n",
    "           amount/avg_amt_last_7d AS amt_ratio2, \\\n",
    "           num_trans_last_60m/num_trans_last_7d AS count_ratio \\\n",
    "    FROM \\\n",
    "        ( \\\n",
    "        SELECT *, \\\n",
    "               COUNT(*) OVER w1 as num_trans_last_60m, \\\n",
    "               AVG(amount) OVER w1 as avg_amt_last_60m, \\\n",
    "               COUNT(*) OVER w2 as num_trans_last_7d, \\\n",
    "               AVG(amount) OVER w2 as avg_amt_last_7d \\\n",
    "        FROM transactions_df \\\n",
    "        WINDOW \\\n",
    "               w1 AS (PARTITION BY cc_num order by cast(event_time AS timestamp) RANGE INTERVAL 60 MINUTES PRECEDING), \\\n",
    "               w2 AS (PARTITION BY cc_num order by cast(event_time AS timestamp) RANGE INTERVAL 7 DAYS PRECEDING) \\\n",
    "        ) \n",
    "    \"\"\"\n",
    "    \n",
    "    transactions_df.registerTempTable('transactions_df')\n",
    "    aggregated_features = spark.sql(query)\n",
    "    return aggregated_features\n",
    "\n",
    "\n",
    "def write_to_s3(args: Namespace, aggregated_features: DataFrame) -> None:\n",
    "    logger.info('[Write Aggregated Features to S3]')\n",
    "    aggregated_features.coalesce(1) \\\n",
    "                       .write.format('com.databricks.spark.csv') \\\n",
    "                       .option('header', True) \\\n",
    "                       .mode('overwrite') \\\n",
    "                       .option('sep', ',') \\\n",
    "                       .save('s3a://' + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix))\n",
    "    \n",
    "def group_by_card_number(aggregated_features: DataFrame) -> DataFrame: \n",
    "    logger.info('[Group Aggregated Features by Card Number]')\n",
    "    window = Window.partitionBy('cc_num').orderBy(desc('event_time'))\n",
    "    sorted_df = aggregated_features.withColumn('rank', dense_rank().over(window))\n",
    "    grouped_df = sorted_df.filter(sorted_df.rank == 1).drop(sorted_df.rank)\n",
    "    sliced_df = grouped_df.select('tid', 'cc_num', 'num_trans_last_7d', 'avg_amt_last_7d', 'event_time')\n",
    "    return sliced_df\n",
    "\n",
    "\n",
    "def transform_row(sliced_df: DataFrame) -> list:\n",
    "    logger.info('[Transform Spark DataFrame Row to SageMaker Feature Store Record]')\n",
    "    records = []\n",
    "    for row in sliced_df.rdd.collect():\n",
    "        record = []\n",
    "        tid, cc_num, num_trans_last_7d, avg_amt_last_7d, event_time = row\n",
    "        if cc_num:\n",
    "            record.append({'ValueAsString': str(tid), 'FeatureName': 'tid'})\n",
    "            record.append({'ValueAsString': str(cc_num), 'FeatureName': 'cc_num'})\n",
    "            record.append({'ValueAsString': str(num_trans_last_7d), 'FeatureName': 'num_trans_last_7d'})\n",
    "            record.append({'ValueAsString': str(round(avg_amt_last_7d, 2)), 'FeatureName': 'avg_amt_last_7d'})\n",
    "            record.append({'ValueAsString': str(event_time), 'FeatureName': 'event_time'})\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "\n",
    "def write_to_feature_store(records: list) -> None:\n",
    "    logger.info('[Write Grouped Features to SageMaker Online Feature Store]')\n",
    "    success, fail = 0, 0\n",
    "    for record in records:\n",
    "        event_time_feature = {\n",
    "                'FeatureName': 'trans_time',\n",
    "                'ValueAsString': str(int(round(time.time())))\n",
    "            }\n",
    "        record.append(event_time_feature)\n",
    "        response = feature_store_client.put_record(FeatureGroupName=FEATURE_GROUP, Record=record)\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "    logger.info('Success = {}'.format(success))\n",
    "    logger.info('Fail = {}'.format(fail))\n",
    "    assert fail == 0\n",
    "\n",
    "\n",
    "def run_spark_job():\n",
    "    spark = SparkSession.builder.appName('PySparkJob').getOrCreate()\n",
    "    args = parse_args()\n",
    "    schema = define_schema()\n",
    "    aggregated_features = aggregate_features(args, schema, spark)\n",
    "    write_to_s3(args, aggregated_features)\n",
    "    sliced_df = group_by_card_number(aggregated_features)\n",
    "    records = transform_row(sliced_df)\n",
    "    write_to_feature_store(records)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run_spark_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Run SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "spark_processor = PySparkProcessor(base_job_name='sagemaker-processing', \n",
    "                                   framework_version='2.4', # spark version\n",
    "                                   role=role, \n",
    "                                   instance_count=1, \n",
    "                                   instance_type='ml.r5.4xlarge', \n",
    "                                   env={'AWS_DEFAULT_REGION': boto3.Session().region_name},\n",
    "                                   max_runtime_in_seconds=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "spark_processor.run(submit_app='batch_aggregation.py', \n",
    "                    arguments=['--s3_input_bucket', BUCKET, \n",
    "                               '--s3_input_key_prefix', INPUT_KEY_PREFIX, \n",
    "                               '--s3_output_bucket', BUCKET, \n",
    "                               '--s3_output_key_prefix', OUTPUT_KEY_PREFIX],\n",
    "                    spark_event_logs_s3_uri='s3://{}/logs'.format(BUCKET),\n",
    "                    logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
