{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SageMaker Feature Store and Apache Spark to generate point-in-time queries to implement Time Travel\n",
    "The following notebook uses SageMaker Feature Store and Apache Spark to build out a set of Dataframes and queries that provide a pattern for using \"Time Travel\" capabilities. We will demonstrate how to build a \"point-in-time\" feature sets by starting with raw transactional data, joining that data with records from the Offline Store, and then building an \"entity\" dataset to define the items we care about and the timestamp of reference. Techniques include building Spark Dataframes, using outer and inner table joins, using query filters to prune items outside our timeframe, and finally ReduceByKey to reduce the final the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Faker library to help generate timestamps within a given range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faker\n",
    "from faker import Faker\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import datediff\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import max as sql_max\n",
    "from pyspark.sql.functions import min as sql_min\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import sagemaker_pyspark\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123456\n",
    "faker = Faker()\n",
    "faker.seed_locale('en_US', 0)\n",
    "faker.seed_instance(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "BUCKET = sagemaker.Session().default_bucket()\n",
    "print(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_PREFIX = \"sagemaker-featurestore-blog\"\n",
    "\n",
    "OFFLINE_STORE_BASE_URI = f's3://{BUCKET}/{BASE_PREFIX}'\n",
    "\n",
    "AGG_PREFIX = os.path.join(BASE_PREFIX, 'aggregated')\n",
    "print(f'S3 Aggregated Prefix: {AGG_PREFIX}')\n",
    "\n",
    "AGG_FEATURES_PATH_S3 = f\"s3://{BUCKET}/{AGG_PREFIX}/\"\n",
    "AGG_FEATURES_PATH_PARQUET = f\"s3a://{BUCKET}/{AGG_PREFIX}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "file_list = S3Downloader.list(AGG_FEATURES_PATH_S3)\n",
    "\n",
    "print(f'Using S3 path: {AGG_FEATURES_PATH_S3}')\n",
    "print(\"Found files: \\n\" + \"\\n\".join(file_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's retreive our credit card transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read.options(Header=True).csv(AGG_FEATURES_PATH_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.printSchema()\n",
    "transactions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 5 random rows from dataframe\n",
    "show_fraction = float(5.0 / 500000.0)\n",
    "print(\"Fraction: %2f\" % show_fraction)\n",
    "print(transactions_df.sample(withReplacement=False, fraction=show_fraction, seed=3).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Sagemaker Client to retrieve info about Feature Group\n",
    "We will use the `describe_feature_group` method to lookup the S3 Uri location of the Offline Store data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.Session().client(service_name='sagemaker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify name of the Feature Group that contains aggregated features for our transaction data\n",
    "FEATURE_GROUP = 'cc-agg-batch-fg'\n",
    "\n",
    "feature_group_info = sm_client.describe_feature_group(FeatureGroupName=FEATURE_GROUP)\n",
    "feature_group_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup S3 Location of Offline Store\n",
    "\n",
    "resolved_offline_store_s3_location = feature_group_info['OfflineStoreConfig']['S3StorageConfig']['ResolvedOutputS3Uri']\n",
    "\n",
    "# Spark's Parquet file reader requires replacement of 's3' with 's3a'\n",
    "offline_store_s3a_uri = resolved_offline_store_s3_location.replace(\"s3:\", \"s3a:\")\n",
    "\n",
    "print(offline_store_s3a_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Offline Store data\n",
    "feature_store_df = spark.read.parquet(offline_store_s3a_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_df.printSchema()\n",
    "feature_store_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an enhanced set of features by joining raw transaction data with aggregate features from the Offline Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the raw transactons table to the aggregate feature table \n",
    "\n",
    "enhanced_df = (transactions_df.join(feature_store_df, transactions_df.tid == feature_store_df.tid, \"left_outer\")\n",
    "    .drop(transactions_df.tid)\n",
    "    .drop(transactions_df.cc_num)\n",
    "    .drop(transactions_df.consumer_id)\n",
    "    .drop(transactions_df.num_trans_last_7d)\n",
    "    .drop(transactions_df.avg_amt_last_7d)\n",
    "    .drop(transactions_df.event_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_df.printSchema()\n",
    "enhanced_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Time Travel query from Studio\n",
    "\n",
    "Now that we have an enhanced dataframe with all our transaction data, we can start building the time travel query. We begin be creating an Entity Dataframe which identifies the consumer_ids of interest, coupled with an event_time which represents our cutoff time for that entity. We also define a staleness window which prevents us from using data older than some limit that we define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num samples in entity dataframe\n",
    "NUM_RANDOM_SAMPLES = 500\n",
    "\n",
    "cid_list = transactions_df.rdd.map(lambda x: x.consumer_id).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_sample = random.sample(cid_list, NUM_RANDOM_SAMPLES) \n",
    "print(len(cid_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of tuples containing consumer IDs with faked timestamps within our time window\n",
    "start = datetime.datetime.strptime('2021-01-31 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "end = datetime.datetime.strptime('2021-01-31 23:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "samples = list()\n",
    "for r in range(NUM_RANDOM_SAMPLES):\n",
    "    row = []\n",
    "    fake_timestamp = faker.date_time_between(start_date=start, end_date=end, tzinfo=None).strftime('%Y-%m-%d %H:00:00')\n",
    "    row.append(cid_sample[r])\n",
    "    row.append(fake_timestamp)\n",
    "    samples.append(row)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and show the Entity Dataframe\n",
    "# (e.g. the dataframe that defines our set of credit card numbers and timestamps for our point-in-time queries)\n",
    "\n",
    "entity_df_schema = StructType([\n",
    "    StructField('consumer_id', StringType(), False),\n",
    "    StructField('joindate', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity data frame\n",
    "\n",
    "entity_df = spark.createDataFrame(samples, entity_df_schema)\n",
    "entity_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Improvement: \n",
    "# This first dataframe filter serves as a performance optimization to reduce the size of dataset\n",
    "# We compute the overall min and max times for the initial filtering, in one pass\n",
    "\n",
    "# entity_df used to define bounded time window\n",
    "minmax_time = entity_df.agg(sql_min(\"joindate\"), sql_max(\"joindate\")).collect()\n",
    "print(minmax_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_time, max_time = minmax_time[0][\"min(joindate)\"], minmax_time[0][\"max(joindate)\"]\n",
    "print(f'min_time: {min_time}')\n",
    "print(f'max_time: {max_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before filter, count: \" + str(enhanced_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Filter out records from after query max_time and before staleness window prior to the min_time\n",
    "# NOTE: This is a performance optimization; doing this prior to individual {consumer_id, joindate} filtering will be faster\n",
    "\n",
    "# Choose a \"staleness\" window of time before which we want to ignore records\n",
    "allowed_staleness_days = 4\n",
    "\n",
    "# Eliminate Credit Cards (entities) who do NOT have any relevant records within our time window \n",
    "# this window represents the {max_time - min_time} delta, plus our staleness window (4 days)\n",
    "\n",
    "# Via the staleness check, we are actually removing items when event_time is MORE than 4 days before min_time\n",
    "# Usage: datediff ( enddate, startdate ) - returns days\n",
    "\n",
    "filtered = enhanced_df.filter(\n",
    "    (enhanced_df.event_time <= max_time) & \n",
    "    (datediff(lit(min_time), enhanced_df.event_time) <= allowed_staleness_days)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.printSchema()\n",
    "print(\"After filter, count: \" + str(filtered.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.select(\"cc_num\", \"consumer_id\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join filtered dataframe with generated entity dataframe; drop duplicate consumer_id field\n",
    "\n",
    "joined = filtered.join(entity_df, filtered.consumer_id == entity_df.consumer_id, \"inner\").drop(entity_df.consumer_id)\n",
    "print(\"Joined count: \" + str(joined.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out data from after query time or before query time minus staleness window\n",
    "# this query removes events outside the time window FOR the SPECIFIC CC (customer)\n",
    "drop_future_and_stale = joined.filter(\n",
    "    (joined.event_time <= entity_df.joindate)\n",
    "    & (datediff(entity_df.joindate, joined.event_time) <= allowed_staleness_days)\n",
    ")\n",
    "\n",
    "print(\"After drop stale, count: \" + str(drop_future_and_stale.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_future_and_stale.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use reduceByKey to group by consumer_id and keep most recent record\n",
    "take_latest = (\n",
    "    drop_future_and_stale.rdd.map(lambda x: (x.consumer_id, x)) \n",
    "    .reduceByKey(\n",
    "        lambda x, y: x if ((x.event_time) >= (y.event_time)) else y\n",
    "    )  #  We could have used api_invocation_time as tie-breaker\n",
    "    .values()  # drop keys\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "latest_df = take_latest.toDF(drop_future_and_stale.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop extra columns\n",
    "columns_to_drop = [\"write_time\", \"is_deleted\", \"year\", \"month\", \"day\", \"hour\", \"query_time\", \"api_invocation_time\"]\n",
    "final_df = latest_df.drop(*columns_to_drop)\n",
    "\n",
    "print('Final count: ' + str(final_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final query results\n",
    "\n",
    "final_df.show(10)\n",
    "\n",
    "# To save query result to s3:\n",
    "# OUTPUT_PATH = f\"s3://{BUCKET}/{PREFIX}/test_query_output\"\n",
    "# final_df.write.parquet(OUTPUT_PATH, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
