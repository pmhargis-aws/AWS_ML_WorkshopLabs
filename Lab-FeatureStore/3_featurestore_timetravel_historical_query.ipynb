{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SageMaker Feature Store and Apache Spark to generate point-in-time queries to implement Time Travel\n",
    "The following notebook uses SageMaker Feature Store and Apache Spark to build out a set of Dataframes and queries that provide a pattern for using \"Time Travel\" capabilities. We will demonstrate how to build a \"point-in-time\" feature sets by starting with raw transactional data, joining that data with records from the Offline Store, and then building an \"entity\" dataset to define the items we care about and the timestamp of reference. Techniques include building Spark Dataframes, using outer and inner table joins, using query filters to prune items outside our timeframe, and finally ReduceByKey to reduce the final the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Faker library to help generate timestamps within a given range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faker\n",
    "from faker import Faker\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import datediff\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import max as sql_max\n",
    "from pyspark.sql.functions import min as sql_min\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import sagemaker_pyspark\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123456\n",
    "faker = Faker()\n",
    "faker.seed_locale('en_US', 0)\n",
    "faker.seed_instance(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "BUCKET = sagemaker.Session().default_bucket()\n",
    "print(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_PREFIX = \"sagemaker-featurestore-blog\"\n",
    "\n",
    "OFFLINE_STORE_BASE_URI = f's3://{BUCKET}/{BASE_PREFIX}'\n",
    "\n",
    "RAW_PREFIX = os.path.join(BASE_PREFIX, 'raw')\n",
    "AGG_PREFIX = os.path.join(BASE_PREFIX, 'aggregated')\n",
    "\n",
    "RAW_FEATURES_PATH_S3 = f\"s3://{BUCKET}/{RAW_PREFIX}/\"\n",
    "RAW_FEATURES_PATH_PARQUET = f\"s3a://{BUCKET}/{RAW_PREFIX}/\"\n",
    "print(f'S3 Raw Transactions S3 path: {RAW_FEATURES_PATH_S3}')\n",
    "\n",
    "AGG_FEATURES_PATH_S3 = f\"s3://{BUCKET}/{AGG_PREFIX}/\"\n",
    "AGG_FEATURES_PATH_PARQUET = f\"s3a://{BUCKET}/{AGG_PREFIX}/\"\n",
    "print(f'S3 Aggregated Data S3 Path: {AGG_FEATURES_PATH_S3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Store Group requires ISO-8601 string format: yyyy-MM-dd'T'HH:mm:ssZ\n",
    "# when the EventTime required attribute is type String\n",
    "\n",
    "ISO_8601_DATETIME_FORMAT = \"%Y-%m-%dT%H:%M:%SZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "# Find raw transaction data files\n",
    "raw_file_list = S3Downloader.list(RAW_FEATURES_PATH_S3)\n",
    "print(\"Found raw transaction files: \\n\" + \"\\n\".join(raw_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find aggregate data files\n",
    "\n",
    "agg_file_list = S3Downloader.list(AGG_FEATURES_PATH_S3)\n",
    "print(\"Found aggregated files: \\n\" + \"\\n\".join(agg_file_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's retreive our credit card transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, TimestampType, LongType\n",
    "\n",
    "raw_schema = StructType([StructField('tid', StringType(), True),\n",
    "                    StructField('event_time', StringType(), True),\n",
    "                    StructField('cc_num', LongType(), True),\n",
    "                    StructField('consumer_id', StringType(), True),\n",
    "                    StructField('amount', DoubleType(), True),\n",
    "                    StructField('fraud_label', StringType(), True)])\n",
    "\n",
    "# Build path to transactions data file\n",
    "raw_file = os.path.join(RAW_FEATURES_PATH_PARQUET, 'transactions.csv')\n",
    "print(raw_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read.csv(raw_file, header=True, schema=raw_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.printSchema()\n",
    "transactions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show several random rows from dataframe\n",
    "show_fraction = float(5.0 / 500000.0)\n",
    "print(\"Fraction: %2f\" % show_fraction)\n",
    "transactions_sample = transactions_df.sample(withReplacement=False, fraction=show_fraction, seed=3).collect()\n",
    "transactions_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Sagemaker Client to retrieve info about Feature Group\n",
    "We will use the `describe_feature_group` method to lookup the S3 Uri location of the Offline Store data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.Session().client(service_name='sagemaker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify name of the Feature Group that contains aggregated features for our transaction data\n",
    "FEATURE_GROUP = 'cc-agg-batch-fg'\n",
    "\n",
    "feature_group_info = sm_client.describe_feature_group(FeatureGroupName=FEATURE_GROUP)\n",
    "feature_group_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup S3 Location of Offline Store\n",
    "\n",
    "resolved_offline_store_s3_location = feature_group_info['OfflineStoreConfig']['S3StorageConfig']['ResolvedOutputS3Uri']\n",
    "\n",
    "# Spark's Parquet file reader requires replacement of 's3' with 's3a'\n",
    "offline_store_s3a_uri = resolved_offline_store_s3_location.replace(\"s3:\", \"s3a:\")\n",
    "\n",
    "print(offline_store_s3a_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Offline Store data\n",
    "feature_store_df = spark.read.parquet(offline_store_s3a_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_df.printSchema()\n",
    "feature_store_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove records marked as deleted (is_deleted attribute)\n",
    "\n",
    "feature_store_active_df = feature_store_df.filter(feature_store_df.is_deleted == 'false').drop(feature_store_df.tid)\n",
    "print(feature_store_active_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_active_df.select('consumer_id', 'avg_amt_last_7d', 'event_time', 'write_time').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an enhanced set of features by joining raw transaction data with aggregate features from the Offline Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the raw transactons table to the aggregate feature table \n",
    "\n",
    "enhanced_df = (transactions_df.join(feature_store_active_df, \n",
    "        transactions_df.consumer_id == feature_store_active_df.consumer_id, \n",
    "        \"left_outer\")\n",
    "    .drop(transactions_df.tid)\n",
    "    .drop(transactions_df.consumer_id)\n",
    "    .drop(transactions_df.cc_num)\n",
    "    .drop(transactions_df.event_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_df.printSchema()\n",
    "enhanced_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(enhanced_df.select('consumer_id', 'amount', 'num_trans_last_7d', 'event_time').collect(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Time Travel Query\n",
    "\n",
    "Now that we have an enhanced dataframe with all our transaction data, we can start building the time travel query. We begin be creating an Entity Dataframe which identifies the consumer_ids of interest, coupled with an event_time which represents our cutoff time for that entity. We also define a staleness window which prevents us from using data older than some limit that we define."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Entity Dataframe will consist of real Consumer IDs and real event timestamps\n",
    "\n",
    "First, we need to create an Entity Dataframe consisting of a list or our \"target\" Consumer IDs, plus a set of realistic timestamps (event_time) to run the point-in-time queries. We also will use a unique (distinct) set of Consumer IDs so that when we run `reduceByKey` later, we will retain a record for each Consumer ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num samples in entity dataframe\n",
    "NUM_RANDOM_SAMPLES = 1000   # was 500\n",
    "\n",
    "# Create set of tuples with consumer_id and event_time \n",
    "cid_ts_tuples = feature_store_active_df.rdd.map(lambda r: (r.consumer_id, r.event_time)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_samples = random.sample(cid_ts_tuples, NUM_RANDOM_SAMPLES) \n",
    "print(len(cid_samples))\n",
    "cid_samples[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use sample tuples to build Entity Dataframe containing consumer IDs with actual timestamps\n",
    "\n",
    "start = datetime.datetime.strptime('2021-03-17T00:00:00Z', ISO_8601_DATETIME_FORMAT)\n",
    "end = datetime.datetime.strptime('2021-03-31T23:59:59Z', ISO_8601_DATETIME_FORMAT)\n",
    "\n",
    "samples = list()\n",
    "for r in range(NUM_RANDOM_SAMPLES):\n",
    "    row = []\n",
    "    row.append(cid_samples[r][0])  # consumer_id\n",
    "    ##row.append(cid_samples[r][1])  # event_time\n",
    "\n",
    "    # Generate fake timestamp within the above data range\n",
    "    fake_timestamp = faker.date_time_between(start_date=start, end_date=end, tzinfo=None).strftime(ISO_8601_DATETIME_FORMAT)\n",
    "    row.append(fake_timestamp)\n",
    "    \n",
    "    samples.append(row)\n",
    "\n",
    "print('Count of samples: ' + str(len(samples)))\n",
    "samples[0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the actual Entity Dataframe\n",
    "# (e.g. the dataframe that defines our set of Consumer IDs and timestamps for our point-in-time queries)\n",
    "\n",
    "entity_df_schema = StructType([\n",
    "    StructField('consumer_id', StringType(), False),\n",
    "    StructField('joindate', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity data frame\n",
    "\n",
    "entity_df = spark.createDataFrame(samples, entity_df_schema)\n",
    "entity_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Improvement: \n",
    "# This first dataframe filter serves as a performance optimization to reduce the size of dataset\n",
    "# We compute the overall min and max times for the initial filtering, in one pass\n",
    "\n",
    "# entity_df used to define bounded time window\n",
    "minmax_time = entity_df.agg(sql_min(\"joindate\"), sql_max(\"joindate\")).collect()\n",
    "print(minmax_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_time, max_time = minmax_time[0][\"min(joindate)\"], minmax_time[0][\"max(joindate)\"]\n",
    "print(f'min_time: {min_time}')\n",
    "print(f'max_time: {max_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OPTIONAL \n",
    "\n",
    "# NOTE: This filter is simply a performance optimization\n",
    "# Filter out records from after query max_time and before staleness window prior to the min_time\n",
    "# doing this prior to individual {consumer_id, joindate} filtering will speed up subsequent filters\n",
    "\n",
    "# Choose a \"staleness\" window of time before which we want to ignore records\n",
    "allowed_staleness_days = 14\n",
    "\n",
    "# Eliminate Credit Cards (entities) who do NOT have any relevant records within our time window \n",
    "# this window represents the {max_time - min_time} delta, plus our staleness window\n",
    "\n",
    "# Via the staleness check, we are actually removing items when event_time is MORE than N days before min_time\n",
    "# Usage: datediff ( enddate, startdate ) - returns days\n",
    "\n",
    "filtered = enhanced_df.filter(\n",
    "    (enhanced_df.event_time <= max_time) & \n",
    "    (datediff(lit(min_time), enhanced_df.event_time) <= allowed_staleness_days)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.printSchema()\n",
    "print(\"After filter, count: \" + str(filtered.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(filtered.select('consumer_id', 'amount', 'event_time').collect(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join filtered dataframe with generated entity dataframe; drop duplicate consumer_id field\n",
    "\n",
    "joined = filtered.join(entity_df, filtered.consumer_id == entity_df.consumer_id, \"inner\")\n",
    "joined_df = joined.drop(entity_df.consumer_id)\n",
    "\n",
    "print(\"Joined count: \" + str(joined_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(joined_df.select('consumer_id', 'amount', 'event_time').collect(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out data from after query time or before query time minus staleness window\n",
    "# this query removes events outside the time window for the SPECIFIC Consumer (customer)\n",
    "\n",
    "allowed_staleness_days = 7\n",
    "\n",
    "drop_future_and_stale = joined_df.filter(\n",
    "    (joined_df.event_time <= entity_df.joindate) &\n",
    "    (datediff(entity_df.joindate, joined_df.event_time) <= allowed_staleness_days)\n",
    ")\n",
    "\n",
    "print(\"After drop stale, count: \" + str(drop_future_and_stale.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.sample(drop_future_and_stale.select('consumer_id', 'amount', 'event_time').collect(), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use reduceByKey to group by consumer_id and keep most recent record\n",
    "take_latest = (\n",
    "    drop_future_and_stale.rdd.map(lambda x: (x.consumer_id, x)) \n",
    "    .reduceByKey(\n",
    "        lambda x, y: x if ((x.event_time) >= (y.event_time)) else y\n",
    "    )  #  We could have used api_invocation_time as tie-breaker\n",
    "    .values()  # drop keys\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "latest_df = take_latest.toDF(drop_future_and_stale.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop extra columns\n",
    "columns_to_drop = [\"joindate\", \"write_time\", \"is_deleted\", \"year\", \"month\", \"day\", \"hour\", \"query_time\", \"api_invocation_time\"]\n",
    "final_df = latest_df.drop(*columns_to_drop)\n",
    "\n",
    "print('Final count: ' + str(final_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show final query results\n",
    "\n",
    "final_df.show(10)\n",
    "\n",
    "# To save query result to s3:\n",
    "# OUTPUT_PATH = f\"s3://{BUCKET}/{PREFIX}/test_query_output\"\n",
    "# final_df.write.parquet(OUTPUT_PATH, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
