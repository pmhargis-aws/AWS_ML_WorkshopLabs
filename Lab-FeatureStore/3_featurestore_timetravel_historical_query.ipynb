{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SageMaker Feature Store and Apache Spark to generate point-in-time queries to implement Time Travel\n",
    "The following notebook uses SageMaker Feature Store and Apache Spark to build out a set of Dataframes and queries that provide a pattern for using \"Time Travel\" capabilities. We will demonstrate how to build a \"point-in-time\" feature sets by starting with raw transactional data, joining that data with records from the Offline Store, and then building an \"entity\" dataset to define the items we care about and the timestamp of reference. Techniques include building Spark Dataframes, using outer and inner table joins, using query filters to prune items outside our timeframe, and finally ReduceByKey to reduce the final the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Faker library to help generate timestamps within a given range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /home/ec2-user/anaconda3/envs/python3/include/python3.6m/UNKNOWN\n",
      "sysconfig: /home/ec2-user/anaconda3/envs/python3/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Requirement already satisfied: Faker in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (8.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from Faker) (2.8.1)\n",
      "Requirement already satisfied: text-unidecode==1.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from Faker) (1.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.4->Faker) (1.14.0)\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /home/ec2-user/anaconda3/envs/python3/include/python3.6m/UNKNOWN\n",
      "sysconfig: /home/ec2-user/anaconda3/envs/python3/include/python3.6m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faker\n",
    "from faker import Faker\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import datediff\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import max as sql_max\n",
    "from pyspark.sql.functions import min as sql_min\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import sagemaker_pyspark\n",
    "import datetime\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.4\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123456\n",
    "faker = Faker()\n",
    "faker.seed_locale('en_US', 0)\n",
    "faker.seed_instance(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-572539092864\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "BUCKET = sagemaker.Session().default_bucket()\n",
    "print(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Aggregated Prefix: sagemaker-featurestore-blog/aggregated\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BASE_PREFIX = \"sagemaker-featurestore-blog\"\n",
    "\n",
    "OFFLINE_STORE_BASE_URI = f's3://{BUCKET}/{BASE_PREFIX}'\n",
    "\n",
    "AGG_PREFIX = os.path.join(BASE_PREFIX, 'aggregated')\n",
    "print(f'S3 Aggregated Prefix: {AGG_PREFIX}')\n",
    "\n",
    "AGG_FEATURES_PATH_S3 = f\"s3://{BUCKET}/{AGG_PREFIX}/\"\n",
    "AGG_FEATURES_PATH_PARQUET = f\"s3a://{BUCKET}/{AGG_PREFIX}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using S3 path: s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog/aggregated/\n",
      "Found files: \n",
      "s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog/aggregated/_SUCCESS\n",
      "s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog/aggregated/part-00000-c8e0ecfd-2bff-4ceb-adb4-01368ce19237-c000.csv\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "file_list = S3Downloader.list(AGG_FEATURES_PATH_S3)\n",
    "\n",
    "print(f'Using S3 path: {AGG_FEATURES_PATH_S3}')\n",
    "print(\"Found files: \\n\" + \"\\n\".join(file_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's retreive our credit card transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read.options(Header=True).csv(AGG_FEATURES_PATH_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tid: string (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- cc_num: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- fraud_label: string (nullable = true)\n",
      " |-- num_trans_last_60m: string (nullable = true)\n",
      " |-- avg_amt_last_60m: string (nullable = true)\n",
      " |-- num_trans_last_7d: string (nullable = true)\n",
      " |-- avg_amt_last_7d: string (nullable = true)\n",
      " |-- amt_ratio1: string (nullable = true)\n",
      " |-- amt_ratio2: string (nullable = true)\n",
      " |-- count_ratio: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500001"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_df.printSchema()\n",
    "transactions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+------+-----------+------------------+----------------+-----------------+------------------+--------------------+--------------------+------------------+\n",
      "|                 tid|          event_time|          cc_num|amount|fraud_label|num_trans_last_60m|avg_amt_last_60m|num_trans_last_7d|   avg_amt_last_7d|          amt_ratio1|          amt_ratio2|       count_ratio|\n",
      "+--------------------+--------------------+----------------+------+-----------+------------------+----------------+-----------------+------------------+--------------------+--------------------+------------------+\n",
      "|bea07140254b4d15b...|2021-01-01T20:44:...|4006080197832643| 674.3|          0|                 1|           674.3|                1|             674.3|                 1.0|                 1.0|               1.0|\n",
      "|ecb4025cc15072ea0...|2021-01-01T20:47:...|4006080197832643| 59.33|          0|                 2|         366.815|                2|           366.815|                 1.0| 0.16174365824734538|               1.0|\n",
      "|765a27d5b12fd812e...|2021-01-02T20:16:...|4006080197832643| 14.24|          0|                 1|           14.24|                3|            249.29|0.057122227125035105|0.057122227125035105|0.3333333333333333|\n",
      "|8bf2dc1a9773118ef...|2021-01-02T22:30:...|4006080197832643| 50.29|          0|                 1|           50.29|                4|            199.54|   0.252029668236945|   0.252029668236945|              0.25|\n",
      "|b8f8d7da31934bc4f...|2021-01-03T12:24:...|4006080197832643| 18.49|          0|                 1|           18.49|                5|163.32999999999998|   0.113206391967183|   0.113206391967183|               0.2|\n",
      "+--------------------+--------------------+----------------+------+-----------+------------------+----------------+-----------------+------------------+--------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Sagemaker Client to retrieve info about Feature Group\n",
    "We will use the `describe_feature_group` method to lookup the S3 Uri location of the Offline Store data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.Session().client(service_name='sagemaker')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/cc-agg-batch-fg',\n",
       " 'FeatureGroupName': 'cc-agg-batch-fg',\n",
       " 'RecordIdentifierFeatureName': 'cc_num',\n",
       " 'EventTimeFeatureName': 'trans_time',\n",
       " 'FeatureDefinitions': [{'FeatureName': 'tid', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'cc_num', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'num_trans_last_7d', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'avg_amt_last_7d', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'event_time', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'trans_time', 'FeatureType': 'Fractional'}],\n",
       " 'CreationTime': datetime.datetime(2021, 4, 30, 21, 56, 10, 133000, tzinfo=tzlocal()),\n",
       " 'OnlineStoreConfig': {'EnableOnlineStore': True},\n",
       " 'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': 's3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog',\n",
       "   'ResolvedOutputS3Uri': 's3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog/572539092864/sagemaker/us-east-1/offline-store/cc-agg-batch-fg-1619819770/data'},\n",
       "  'DisableGlueTableCreation': False,\n",
       "  'DataCatalogConfig': {'TableName': 'cc-agg-batch-fg-1619819770',\n",
       "   'Catalog': 'AwsDataCatalog',\n",
       "   'Database': 'sagemaker_featurestore'}},\n",
       " 'RoleArn': 'arn:aws:iam::572539092864:role/service-role/AmazonSageMaker-ExecutionRole-20200407T174741',\n",
       " 'FeatureGroupStatus': 'Created',\n",
       " 'OfflineStoreStatus': {'Status': 'Active'},\n",
       " 'Description': 'Aggregated features for each credit card, streamed intraday',\n",
       " 'ResponseMetadata': {'RequestId': '690e9106-ac82-4397-ad6d-1cbd9e5345c3',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '690e9106-ac82-4397-ad6d-1cbd9e5345c3',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1328',\n",
       "   'date': 'Fri, 30 Apr 2021 22:12:10 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify name of the Feature Group that contains aggregated features for our transaction data\n",
    "FEATURE_GROUP = 'cc-agg-batch-fg'\n",
    "\n",
    "feature_group_info = sm_client.describe_feature_group(FeatureGroupName=FEATURE_GROUP)\n",
    "feature_group_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog/572539092864/sagemaker/us-east-1/offline-store/cc-agg-batch-fg-1619819770/data\n"
     ]
    }
   ],
   "source": [
    "# Lookup S3 Location of Offline Store\n",
    "\n",
    "resolved_offline_store_s3_location = feature_group_info['OfflineStoreConfig']['S3StorageConfig']['ResolvedOutputS3Uri']\n",
    "\n",
    "# Spark's Parquet file reader requires replacement of 's3' with 's3a'\n",
    "offline_store_s3a_uri = resolved_offline_store_s3_location.replace(\"s3:\", \"s3a:\")\n",
    "\n",
    "print(offline_store_s3a_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Offline Store data\n",
    "feature_store_df = spark.read.parquet(offline_store_s3a_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tid: string (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- num_trans_last_7d: long (nullable = true)\n",
      " |-- avg_amt_last_7d: double (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- trans_time: double (nullable = true)\n",
      " |-- write_time: timestamp (nullable = true)\n",
      " |-- api_invocation_time: timestamp (nullable = true)\n",
      " |-- is_deleted: boolean (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_store_df.printSchema()\n",
    "feature_store_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+\n",
      "|                 tid|          cc_num|num_trans_last_7d|avg_amt_last_7d|         event_time|   trans_time|          write_time|api_invocation_time|is_deleted|year|month|day|hour|\n",
      "+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+\n",
      "|13e125378d6ce52e7...|4006080197832643|               14|          429.3|2021-01-31 02:29:52|1.619820251E9|2021-04-30 22:09:...|2021-04-30 22:04:11|     false|2021|    4| 30|  22|\n",
      "|17f1f7526762fa019...|4008569092490794|               14|         341.62|2021-01-31 02:15:13|1.619820252E9|2021-04-30 22:09:...|2021-04-30 22:04:11|     false|2021|    4| 30|  22|\n",
      "|e66dfa41d179d32b7...|4015965906982664|               14|         838.93|2021-01-31 21:42:18|1.619820252E9|2021-04-30 22:09:...|2021-04-30 22:04:11|     false|2021|    4| 30|  22|\n",
      "|9fd567039a945b5a8...|4026315850364369|               12|         147.79|2021-01-31 05:12:42|1.619820252E9|2021-04-30 22:09:...|2021-04-30 22:04:11|     false|2021|    4| 30|  22|\n",
      "|856033e37b9c9c5ea...|4029536382899161|               10|         104.18|2021-01-31 08:58:04|1.619820252E9|2021-04-30 22:09:...|2021-04-30 22:04:11|     false|2021|    4| 30|  22|\n",
      "+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_store_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create an enhanced set of features by joining raw transaction data with aggregate features from the Offline Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the raw transactons table to the aggregate feature table \n",
    "\n",
    "enhanced_df = (transactions_df.join(feature_store_df, transactions_df.tid == feature_store_df.tid, \"left_outer\")\n",
    "    .drop(transactions_df.tid)\n",
    "    .drop(transactions_df.cc_num)\n",
    "    .drop(transactions_df.num_trans_last_7d)\n",
    "    .drop(transactions_df.avg_amt_last_7d)\n",
    "    .drop(transactions_df.event_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: string (nullable = true)\n",
      " |-- fraud_label: string (nullable = true)\n",
      " |-- num_trans_last_60m: string (nullable = true)\n",
      " |-- avg_amt_last_60m: string (nullable = true)\n",
      " |-- amt_ratio1: string (nullable = true)\n",
      " |-- amt_ratio2: string (nullable = true)\n",
      " |-- count_ratio: string (nullable = true)\n",
      " |-- tid: string (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- num_trans_last_7d: long (nullable = true)\n",
      " |-- avg_amt_last_7d: double (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- trans_time: double (nullable = true)\n",
      " |-- write_time: timestamp (nullable = true)\n",
      " |-- api_invocation_time: timestamp (nullable = true)\n",
      " |-- is_deleted: boolean (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500001"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_df.printSchema()\n",
    "enhanced_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Time Travel query from Studio\n",
    "\n",
    "##### Simplified query for datasets up to 100 GB\n",
    "\n",
    "SELECT *\n",
    "FROM\n",
    "    (SELECT *,\n",
    "         row_number()\n",
    "        OVER (PARTITION BY EventTime\n",
    "    ORDER BY  EventTime desc, Api_Invocation_Time DESC, write_time DESC) AS row_number\n",
    "    FROM sagemaker_featurestore.identity-feature-group-03-20-32-44-1614803787\n",
    "    where EventTime <= timestamp '<timestamp>')\n",
    "    -- replace timestamp '<timestamp>' with just <timestamp>  if EventTimeFeature is of type fractional\n",
    "WHERE row_number = 1 and\n",
    "NOT is_deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Entity Dataframe that spans the intended time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Num samples in entity dataframe\n",
    "NUM_RANDOM_SAMPLES = 500\n",
    "\n",
    "cc_num_list = transactions_df.rdd.map(lambda x: x.cc_num).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_num_sample = random.sample(cc_num_list, NUM_RANDOM_SAMPLES)\n",
    "#print(cc_num_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of tuples containing real credit card numbers (cc_num) with faked timestamps within our time window\n",
    "start = datetime.datetime.strptime('2021-01-31 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "end = datetime.datetime.strptime('2021-01-31 23:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "samples = list()\n",
    "for r in range(NUM_RANDOM_SAMPLES):\n",
    "    row = []\n",
    "    fake_timestamp = faker.date_time_between(start_date=start, end_date=end, tzinfo=None).strftime('%Y-%m-%d %H:00:00')\n",
    "    row.append(cc_num_sample[r])\n",
    "    row.append(fake_timestamp)\n",
    "    samples.append(row)\n",
    "    \n",
    "#print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and show the Entity Dataframe\n",
    "# (e.g. the dataframe that defines our set of credit card numbers and timestamps for our point-in-time queries)\n",
    "\n",
    "entity_df_schema = StructType([\n",
    "    StructField('cc_num', StringType(), False),\n",
    "    StructField('joindate', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------------+\n",
      "|          cc_num|           joindate|\n",
      "+----------------+-------------------+\n",
      "|4557052810125286|2021-01-31 10:00:00|\n",
      "|4705794167507670|2021-01-31 01:00:00|\n",
      "|4035461743633799|2021-01-31 06:00:00|\n",
      "|4133239202304420|2021-01-31 00:00:00|\n",
      "|4553580746227556|2021-01-31 02:00:00|\n",
      "|4623491090561556|2021-01-31 01:00:00|\n",
      "|4972070993122414|2021-01-31 09:00:00|\n",
      "|4774849075253886|2021-01-31 01:00:00|\n",
      "|4583082767257095|2021-01-31 04:00:00|\n",
      "|4104163068719676|2021-01-31 08:00:00|\n",
      "+----------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create entity data frame\n",
    "\n",
    "entity_df = spark.createDataFrame(samples, entity_df_schema)\n",
    "entity_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(min(joindate)='2021-01-31 00:00:00', max(joindate)='2021-01-31 22:00:00')]\n"
     ]
    }
   ],
   "source": [
    "# Performance Improvement: \n",
    "# This first dataframe filter serves as a performance optimization to reduce the size of dataset\n",
    "# We compute the overall min and max times for the initial filtering, in one pass\n",
    "\n",
    "# entity_df used to define bounded time window\n",
    "minmax_time = entity_df.agg(sql_min(\"joindate\"), sql_max(\"joindate\")).collect()\n",
    "print(minmax_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_time: 2021-01-31 00:00:00\n",
      "max_time: 2021-01-31 22:00:00\n"
     ]
    }
   ],
   "source": [
    "min_time, max_time = minmax_time[0][\"min(joindate)\"], minmax_time[0][\"max(joindate)\"]\n",
    "print(f'min_time: {min_time}')\n",
    "print(f'max_time: {max_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filter, count: 500001\n"
     ]
    }
   ],
   "source": [
    "print(\"Before filter, count: \" + str(enhanced_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 1.87 ms, total: 1.87 ms\n",
      "Wall time: 45.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Filter out records from after query max_time and before staleness window prior to the min_time\n",
    "# NOTE: This is a performance optimization; doing this prior to individual {cc_num, query_time} filtering will be faster\n",
    "\n",
    "# Choose a \"staleness\" window of time before which we want to ignore records\n",
    "allowed_staleness_days = 4\n",
    "\n",
    "# Eliminate Credit Cards (entities) who do NOT have any relevant records within our time window \n",
    "# this window represents the {max_time - min_time} delta, plus our staleness window (4 days)\n",
    "\n",
    "# Via the staleness check, we are actually removing items when event_time is MORE than 4 days before min_time\n",
    "# Usage: datediff ( enddate, startdate ) - returns days\n",
    "\n",
    "filtered = enhanced_df.filter(\n",
    "    (enhanced_df.event_time <= max_time) & \n",
    "    (datediff(lit(min_time), enhanced_df.event_time) <= allowed_staleness_days)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: string (nullable = true)\n",
      " |-- fraud_label: string (nullable = true)\n",
      " |-- num_trans_last_60m: string (nullable = true)\n",
      " |-- avg_amt_last_60m: string (nullable = true)\n",
      " |-- amt_ratio1: string (nullable = true)\n",
      " |-- amt_ratio2: string (nullable = true)\n",
      " |-- count_ratio: string (nullable = true)\n",
      " |-- tid: string (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- num_trans_last_7d: long (nullable = true)\n",
      " |-- avg_amt_last_7d: double (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- trans_time: double (nullable = true)\n",
      " |-- write_time: timestamp (nullable = true)\n",
      " |-- api_invocation_time: timestamp (nullable = true)\n",
      " |-- is_deleted: boolean (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n",
      "After filter, count: 8749\n"
     ]
    }
   ],
   "source": [
    "filtered.printSchema()\n",
    "print(\"After filter, count: \" + str(filtered.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------------------+------------------+-------------------+--------------------+-------------------+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+\n",
      "|amount|fraud_label|num_trans_last_60m|  avg_amt_last_60m|         amt_ratio1|          amt_ratio2|        count_ratio|                 tid|          cc_num|num_trans_last_7d|avg_amt_last_7d|         event_time|   trans_time|          write_time|api_invocation_time|is_deleted|year|month|day|hour|\n",
      "+------+-----------+------------------+------------------+-------------------+--------------------+-------------------+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+\n",
      "|424.77|          0|                 2|212.77499999999998| 0.4956283016513455|  0.9894397071669231|0.14285714285714285|13e125378d6ce52e7...|4006080197832643|               14|          429.3|2021-01-31 02:29:52|1.619820251E9|2021-04-30 22:09:...|2021-04-30 22:04:11|     false|2021|    4| 30|  22|\n",
      "|872.98|          0|                 1|            872.98|  2.555444505312988|   2.555444505312988|0.07142857142857142|17f1f7526762fa019...|4008569092490794|               14|         341.62|2021-01-31 02:15:13|1.619820252E9|2021-04-30 22:09:...|2021-04-30 22:04:11|     false|2021|    4| 30|  22|\n",
      "| 84.31|          0|                 1|             84.31|0.09356851055045722| 0.09356851055045722|                0.1|e18d8157521be24a4...|4014600948537520|               10|         901.05|2021-01-31 12:44:43|1.619820252E9|2021-04-30 22:09:...|2021-04-30 22:04:11|     false|2021|    4| 30|  22|\n",
      "| 10.31|          0|                 2|           322.445| 0.3843549780969693|0.012289537205352087|0.14285714285714285|e66dfa41d179d32b7...|4015965906982664|               14|         838.93|2021-01-31 21:42:18|1.619820252E9|2021-04-30 22:09:...|2021-04-30 22:04:11|     false|2021|    4| 30|  22|\n",
      "| 88.78|          0|                 2|55.495000000000005|0.14083320644513941|  0.2253026771456794|0.15384615384615385|96415fbb471f627d7...|4016674905670309|               13|         394.05|2021-01-31 19:43:20|1.619820252E9|2021-04-30 22:09:...|2021-04-30 22:04:11|     false|2021|    4| 30|  22|\n",
      "+------+-----------+------------------+------------------+-------------------+--------------------+-------------------+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|          cc_num|\n",
      "+----------------+\n",
      "|4006080197832643|\n",
      "|4008569092490794|\n",
      "|4014600948537520|\n",
      "|4015965906982664|\n",
      "|4016674905670309|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered.select(\"cc_num\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined count: 451\n"
     ]
    }
   ],
   "source": [
    "# Join filtered dataframe with generated entity dataframe; drop duplicate cc_num field\n",
    "\n",
    "joined = filtered.join(entity_df, filtered.cc_num == entity_df.cc_num, \"inner\").drop(entity_df.cc_num)\n",
    "print(\"Joined count: \" + str(joined.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------------------+----------------+-------------------+-------------------+--------------------+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+-------------------+\n",
      "|amount|fraud_label|num_trans_last_60m|avg_amt_last_60m|         amt_ratio1|         amt_ratio2|         count_ratio|                 tid|          cc_num|num_trans_last_7d|avg_amt_last_7d|         event_time|   trans_time|          write_time|api_invocation_time|is_deleted|year|month|day|hour|           joindate|\n",
      "+------+-----------+------------------+----------------+-------------------+-------------------+--------------------+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+-------------------+\n",
      "| 23.54|          0|                 1|           23.54|0.06904551126849195|0.06904551126849195| 0.09090909090909091|5d2f35432b07d8069...|4052306884957267|               11|         340.93|2021-01-28 15:10:55|1.619820299E9|2021-04-30 22:09:...|2021-04-30 22:04:59|     false|2021|    4| 30|  22|2021-01-31 10:00:00|\n",
      "|  64.5|          0|                 1|            64.5|0.46680122550399256|0.46680122550399256|  0.1111111111111111|f2fc9c94f2524e677...|4311601971379095|                9|         138.17|2021-01-31 02:21:25|1.619820253E9|2021-04-30 22:09:...|2021-04-30 22:04:13|     false|2021|    4| 30|  22|2021-01-31 07:00:00|\n",
      "|  5.31|          0|                 1|            5.31|0.04640188753440817|0.04640188753440817| 0.08333333333333333|1542ffa4ad2763b06...|4461687009675701|               12|         114.44|2021-01-30 19:21:23|1.619820254E9|2021-04-30 22:09:...|2021-04-30 22:04:13|     false|2021|    4| 30|  22|2021-01-31 01:00:00|\n",
      "| 98.57|          0|                 1|           98.57|0.05998288000043274|0.05998288000043274|  0.1111111111111111|6e1fa3884e6c7fd32...|4569922763238722|                9|         1643.3|2021-01-29 13:31:31|1.619820345E9|2021-04-30 22:09:...|2021-04-30 22:05:45|     false|2021|    4| 30|  22|2021-01-31 01:00:00|\n",
      "|431.75|          0|                 1|          431.75| 0.7024948028927693| 0.7024948028927693|0.058823529411764705|4bb561daecb12695c...|4936052901371668|               17|          614.6|2021-01-30 17:54:28|1.619820325E9|2021-04-30 22:09:...|2021-04-30 22:05:25|     false|2021|    4| 30|  22|2021-01-31 18:00:00|\n",
      "+------+-----------+------------------+----------------+-------------------+-------------------+--------------------+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After drop stale, count: 237\n"
     ]
    }
   ],
   "source": [
    "# Filter out data from after query time or before query time minus staleness window\n",
    "# this query removes events outside the time window FOR the SPECIFIC CC (customer)\n",
    "drop_future_and_stale = joined.filter(\n",
    "    (joined.event_time <= entity_df.joindate)\n",
    "    & (datediff(entity_df.joindate, joined.event_time) <= allowed_staleness_days)\n",
    ")\n",
    "\n",
    "print(\"After drop stale, count: \" + str(drop_future_and_stale.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+------------------+----------------+-------------------+-------------------+--------------------+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+-------------------+\n",
      "|amount|fraud_label|num_trans_last_60m|avg_amt_last_60m|         amt_ratio1|         amt_ratio2|         count_ratio|                 tid|          cc_num|num_trans_last_7d|avg_amt_last_7d|         event_time|   trans_time|          write_time|api_invocation_time|is_deleted|year|month|day|hour|           joindate|\n",
      "+------+-----------+------------------+----------------+-------------------+-------------------+--------------------+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+-------------------+\n",
      "| 23.54|          0|                 1|           23.54|0.06904551126849195|0.06904551126849195| 0.09090909090909091|5d2f35432b07d8069...|4052306884957267|               11|         340.93|2021-01-28 15:10:55|1.619820299E9|2021-04-30 22:09:...|2021-04-30 22:04:59|     false|2021|    4| 30|  22|2021-01-31 10:00:00|\n",
      "|  64.5|          0|                 1|            64.5|0.46680122550399256|0.46680122550399256|  0.1111111111111111|f2fc9c94f2524e677...|4311601971379095|                9|         138.17|2021-01-31 02:21:25|1.619820253E9|2021-04-30 22:09:...|2021-04-30 22:04:13|     false|2021|    4| 30|  22|2021-01-31 07:00:00|\n",
      "|  5.31|          0|                 1|            5.31|0.04640188753440817|0.04640188753440817| 0.08333333333333333|1542ffa4ad2763b06...|4461687009675701|               12|         114.44|2021-01-30 19:21:23|1.619820254E9|2021-04-30 22:09:...|2021-04-30 22:04:13|     false|2021|    4| 30|  22|2021-01-31 01:00:00|\n",
      "| 98.57|          0|                 1|           98.57|0.05998288000043274|0.05998288000043274|  0.1111111111111111|6e1fa3884e6c7fd32...|4569922763238722|                9|         1643.3|2021-01-29 13:31:31|1.619820345E9|2021-04-30 22:09:...|2021-04-30 22:05:45|     false|2021|    4| 30|  22|2021-01-31 01:00:00|\n",
      "|431.75|          0|                 1|          431.75| 0.7024948028927693| 0.7024948028927693|0.058823529411764705|4bb561daecb12695c...|4936052901371668|               17|          614.6|2021-01-30 17:54:28|1.619820325E9|2021-04-30 22:09:...|2021-04-30 22:05:25|     false|2021|    4| 30|  22|2021-01-31 18:00:00|\n",
      "+------+-----------+------------------+----------------+-------------------+-------------------+--------------------+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "drop_future_and_stale.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use reduceByKey to group by cc_num and keep most recent record\n",
    "take_latest = (\n",
    "    drop_future_and_stale.rdd.map(lambda x: (x.cc_num, x)) \n",
    "    .reduceByKey(\n",
    "        lambda x, y: x if ((x.event_time) >= (y.event_time)) else y\n",
    "    )  #  We could have used api_invocation_time as tie-breaker\n",
    "    .values()  # drop keys\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "latest_df = take_latest.toDF(drop_future_and_stale.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final count: 235\n"
     ]
    }
   ],
   "source": [
    "# Drop extra columns\n",
    "columns_to_drop = [\"write_time\", \"is_deleted\", \"year\", \"month\", \"day\", \"hour\", \"query_time\", \"api_invocation_time\"]\n",
    "final_df = latest_df.drop(*columns_to_drop)\n",
    "\n",
    "print('Final count: ' + str(final_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------------+----------------+--------------------+--------------------+-------------------+--------------------+----------------+-----------------+---------------+-------------------+-------------+-------------------+\n",
      "| amount|fraud_label|num_trans_last_60m|avg_amt_last_60m|          amt_ratio1|          amt_ratio2|        count_ratio|                 tid|          cc_num|num_trans_last_7d|avg_amt_last_7d|         event_time|   trans_time|           joindate|\n",
      "+-------+-----------+------------------+----------------+--------------------+--------------------+-------------------+--------------------+----------------+-----------------+---------------+-------------------+-------------+-------------------+\n",
      "|  67.67|          0|                 1|           67.67| 0.09865522063299029| 0.09865522063299029|0.08333333333333333|c93044647cadf7d3c...|4468967287415001|               12|         685.92|2021-01-30 11:25:27| 1.61982034E9|2021-01-31 02:00:00|\n",
      "|4732.31|          0|                 1|         4732.31|  10.713457082193974|  10.713457082193974|0.07142857142857142|5bcfc378446e2c505...|4629183456621801|               14|         441.72|2021-01-30 10:19:32|1.619820279E9|2021-01-31 07:00:00|\n",
      "|6454.31|          0|                 1|         6454.31|  3.5714041324078476|  3.5714041324078476|0.08333333333333333|cc2d4a6e6f4b08ac0...|4886639367016405|               12|        1807.22|2021-01-29 18:04:13|1.619820325E9|2021-01-31 01:00:00|\n",
      "|7627.75|          0|                 1|         7627.75|  2.9279381451573228|  2.9279381451573228|0.09090909090909091|eefab8ef9f54b168c...|4567719632788206|               11|        2605.16|2021-01-31 07:46:24|1.619820292E9|2021-01-31 09:00:00|\n",
      "|  86.34|          0|                 1|           86.34| 0.14664398834379497| 0.14664398834379497|0.14285714285714285|eb263b630aa89aa31...|4552620344950208|                7|         588.77|2021-01-30 10:13:38|1.619820292E9|2021-01-31 03:00:00|\n",
      "|   0.26|          0|                 1|            0.26|3.180826953838248...|3.180826953838248...|0.08333333333333333|3306e07ff153bbd29...|4598032646878408|               12|          817.4|2021-01-31 14:54:42|1.619820315E9|2021-01-31 17:00:00|\n",
      "|  62.02|          0|                 1|           62.02| 0.14133142926592127| 0.14133142926592127| 0.1111111111111111|01685326d2c65fb2f...|4496477163819009|                9|         438.83|2021-01-30 10:24:59|1.619820287E9|2021-01-31 07:00:00|\n",
      "|  22.54|          0|                 1|           22.54|  0.1372771935523164|  0.1372771935523164|0.08333333333333333|da1d12b4fbf03f8a5...|4262786636477209|               12|         164.19|2021-01-31 02:50:07|1.619820366E9|2021-01-31 18:00:00|\n",
      "|  57.66|          0|                 1|           57.66|   0.384067141810431|   0.384067141810431|0.16666666666666666|cbec608e37a751269...|4908121585057010|                6|         150.13|2021-01-31 15:51:28|1.619820325E9|2021-01-31 16:00:00|\n",
      "|  67.97|          0|                 1|           67.97|  0.4400517936493923|  0.4400517936493923|0.09090909090909091|27541b3e0c6dbe9d2...|4825119159404212|               11|         154.46|2021-01-31 17:49:06|1.619820302E9|2021-01-31 18:00:00|\n",
      "+-------+-----------+------------------+----------------+--------------------+--------------------+-------------------+--------------------+----------------+-----------------+---------------+-------------------+-------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show final query results\n",
    "\n",
    "final_df.show(10)\n",
    "\n",
    "# To save query result to s3:\n",
    "# OUTPUT_PATH = f\"s3://{BUCKET}/{PREFIX}/test_query_output\"\n",
    "# final_df.write.parquet(OUTPUT_PATH, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
