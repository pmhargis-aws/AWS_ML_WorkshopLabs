{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c8c874",
   "metadata": {},
   "source": [
    "### Use SageMaker Feature Store and Apache Spark to generate point-in-time queries\n",
    "The following notebook uses SageMaker Feature Store and Apache Spark to build out a set of Dataframes and queries that provide a pattern for historical lookup capabilities. We will demonstrate how to build a \"point-in-time\" feature sets by starting with raw transactional data, joining that data with records from the Offline Store, and then building an \"entity\" dataset to define the items we care about and the timestamp of reference. Techniques include building Spark Dataframes, using outer and inner table joins, using Dataframe filters to prune items outside our timeframe, and finally using Spark `reduceByKey` to reduce the final the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03431550",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f60c2635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.42.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker as sm\n",
    "sm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae62668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyspark and build Spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import datediff\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import max as sql_max\n",
    "from pyspark.sql.functions import min as sql_min\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FractionalType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import sagemaker_pyspark\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf144c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .config(\"spark.executor.memory\", '1g')\n",
    "    .config('spark.executor.cores', '16')\n",
    "    .config(\"spark.driver.memory\",'8g')\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a3bffdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daa2ca24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-572539092864\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "\n",
    "# If you need an instance of FeatureStore runtime:\n",
    "#featurestore_runtime = boto_session.client(service_name='sagemaker-featurestore-runtime', region_name=region)\n",
    "\n",
    "#feature_store_session = Session(\n",
    "#    boto_session=boto_session,\n",
    "#    sagemaker_client=sagemaker_client,\n",
    "#    sagemaker_featurestore_runtime_client=featurestore_runtime\n",
    "#)\n",
    "\n",
    "BUCKET = sagemaker_session.default_bucket()\n",
    "print(BUCKET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7fd93fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Raw Transactions S3 path: s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog/raw/\n",
      "S3 Aggregated Data S3 Path: s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog/aggregated/\n"
     ]
    }
   ],
   "source": [
    "# Setup config variables, paths, names, etc.\n",
    "import os\n",
    "\n",
    "BASE_PREFIX = \"sagemaker-featurestore-blog\"\n",
    "\n",
    "OFFLINE_STORE_BASE_URI = f's3://{BUCKET}/{BASE_PREFIX}'\n",
    "\n",
    "RAW_PREFIX = os.path.join(BASE_PREFIX, 'raw')\n",
    "AGG_PREFIX = os.path.join(BASE_PREFIX, 'aggregated')\n",
    "\n",
    "RAW_FEATURES_PATH_S3 = f\"s3://{BUCKET}/{RAW_PREFIX}/\"\n",
    "RAW_FEATURES_PATH_PARQUET = f\"s3a://{BUCKET}/{RAW_PREFIX}/\"\n",
    "print(f'S3 Raw Transactions S3 path: {RAW_FEATURES_PATH_S3}')\n",
    "\n",
    "AGG_FEATURES_PATH_S3 = f\"s3://{BUCKET}/{AGG_PREFIX}/\"\n",
    "AGG_FEATURES_PATH_PARQUET = f\"s3a://{BUCKET}/{AGG_PREFIX}/\"\n",
    "print(f'S3 Aggregated Data S3 Path: {AGG_FEATURES_PATH_S3}')\n",
    "\n",
    "CONS_FEATURE_GROUP = \"consumer-fg\"\n",
    "CARD_FEATURE_GROUP = \"credit-card-fg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d67a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Store Group requires ISO-8601 string format: yyyy-MM-dd'T'HH:mm:ssZ\n",
    "# when the EventTime required attribute is type String\n",
    "\n",
    "ISO_8601_DATETIME_FORMAT = \"%Y-%m-%dT%H:%M:%SZ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb0415b",
   "metadata": {},
   "source": [
    "## Generate and ingest agg features for a credit card fg and a consumer fg\n",
    "This section can be moved to another preparation notebook to be run before the point in time query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d7049",
   "metadata": {},
   "source": [
    "#### Let's retreive our credit card transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f51f387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog/raw/transactions.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, TimestampType, LongType\n",
    "\n",
    "raw_schema = StructType([StructField('tid', StringType(), True),\n",
    "                    StructField('event_time', StringType(), True),\n",
    "                    StructField('cc_num', LongType(), True),\n",
    "                    StructField('consumer_id', StringType(), True),\n",
    "                    StructField('amount', DoubleType(), True),\n",
    "                    StructField('fraud_label', StringType(), True)])\n",
    "\n",
    "# Build path to transactions data file\n",
    "raw_file = os.path.join(RAW_FEATURES_PATH_PARQUET, 'transactions.csv')\n",
    "print(raw_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0baebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read.csv(raw_file, header=True, schema=raw_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1adda5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tid: string (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- consumer_id: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- fraud_label: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_df.printSchema()\n",
    "transactions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18b14ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.createOrReplaceTempView('trans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a50dd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def agg(by_col, lookback_days, start_day, end_day):\n",
    "    transactions_df.createOrReplaceTempView('trans')\n",
    "    all_agg_rows = None\n",
    "    for curr_day in range(start_day, end_day + 1):\n",
    "        min_day = max(1, (curr_day - lookback_days +1))\n",
    "        print(f'aggregating \"{by_col}\" for day {curr_day:02d}, look back to {min_day:02d} beginning of day...')\n",
    "        start_time = f'2021-03-{min_day:02d}T00:00:00Z'\n",
    "        end_time = f'2021-03-{curr_day:02d}T23:59:59Z'\n",
    "        event_time = end_time\n",
    "\n",
    "        sub_query = f'SELECT {by_col}, '\n",
    "        sub_query += f'COUNT(*) as num_trans_last_{lookback_days}d, AVG(amount) as avg_amt_last_{lookback_days}d FROM trans'\n",
    "        sub_query += f' where event_time >= \"{start_time}\" and event_time <= \"{end_time}\" GROUP BY {by_col}'\n",
    "\n",
    "        d_query = f'select distinct({by_col}) from trans '\n",
    "\n",
    "        total_query = f'select a.{by_col}, b.num_trans_last_{lookback_days}d, b.avg_amt_last_{lookback_days}d from ({d_query}) a left join ({sub_query}) b on a.{by_col} = b.{by_col}'\n",
    "        print(f' Using query: {total_query}\\n')\n",
    "        total_df = spark.sql(total_query)\n",
    "\n",
    "        # add a column to flag all of these records with an event time of the running of this \"daily batch job\"\n",
    "        total_df = total_df.withColumn('event_time', lit(event_time))\n",
    "#         print(f' {total_df.count()} rows')\n",
    "        \n",
    "        if all_agg_rows is None:\n",
    "            all_agg_rows = spark.createDataFrame([], total_df.schema)\n",
    "        all_agg_rows = all_agg_rows.union(total_df)\n",
    "        del total_df\n",
    "        \n",
    "    return all_agg_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7c02551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregating \"cc_num\" for day 01, look back to 01 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-01T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 02, look back to 01 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-02T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 03, look back to 01 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-03T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 04, look back to 01 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-04T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 05, look back to 01 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-05T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 06, look back to 01 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-06T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 07, look back to 01 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-07T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 08, look back to 02 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-02T00:00:00Z\" and event_time <= \"2021-03-08T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 09, look back to 03 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-03T00:00:00Z\" and event_time <= \"2021-03-09T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 10, look back to 04 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-04T00:00:00Z\" and event_time <= \"2021-03-10T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 11, look back to 05 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-05T00:00:00Z\" and event_time <= \"2021-03-11T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 12, look back to 06 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-06T00:00:00Z\" and event_time <= \"2021-03-12T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 13, look back to 07 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-07T00:00:00Z\" and event_time <= \"2021-03-13T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 14, look back to 08 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-08T00:00:00Z\" and event_time <= \"2021-03-14T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 15, look back to 09 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-09T00:00:00Z\" and event_time <= \"2021-03-15T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 16, look back to 10 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-10T00:00:00Z\" and event_time <= \"2021-03-16T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 17, look back to 11 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-11T00:00:00Z\" and event_time <= \"2021-03-17T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 18, look back to 12 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-12T00:00:00Z\" and event_time <= \"2021-03-18T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 19, look back to 13 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-13T00:00:00Z\" and event_time <= \"2021-03-19T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 20, look back to 14 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-14T00:00:00Z\" and event_time <= \"2021-03-20T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 21, look back to 15 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-15T00:00:00Z\" and event_time <= \"2021-03-21T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 22, look back to 16 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-16T00:00:00Z\" and event_time <= \"2021-03-22T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregating \"cc_num\" for day 23, look back to 17 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-17T00:00:00Z\" and event_time <= \"2021-03-23T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 24, look back to 18 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-18T00:00:00Z\" and event_time <= \"2021-03-24T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 25, look back to 19 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-19T00:00:00Z\" and event_time <= \"2021-03-25T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 26, look back to 20 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-20T00:00:00Z\" and event_time <= \"2021-03-26T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 27, look back to 21 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-21T00:00:00Z\" and event_time <= \"2021-03-27T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 28, look back to 22 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-22T00:00:00Z\" and event_time <= \"2021-03-28T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 29, look back to 23 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-23T00:00:00Z\" and event_time <= \"2021-03-29T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 30, look back to 24 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-24T00:00:00Z\" and event_time <= \"2021-03-30T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 31, look back to 25 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-25T00:00:00Z\" and event_time <= \"2021-03-31T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 01, look back to 01 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-01T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 02, look back to 02 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-02T00:00:00Z\" and event_time <= \"2021-03-02T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 03, look back to 03 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-03T00:00:00Z\" and event_time <= \"2021-03-03T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 04, look back to 04 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-04T00:00:00Z\" and event_time <= \"2021-03-04T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 05, look back to 05 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-05T00:00:00Z\" and event_time <= \"2021-03-05T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 06, look back to 06 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-06T00:00:00Z\" and event_time <= \"2021-03-06T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 07, look back to 07 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-07T00:00:00Z\" and event_time <= \"2021-03-07T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 08, look back to 08 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-08T00:00:00Z\" and event_time <= \"2021-03-08T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 09, look back to 09 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-09T00:00:00Z\" and event_time <= \"2021-03-09T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 10, look back to 10 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-10T00:00:00Z\" and event_time <= \"2021-03-10T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 11, look back to 11 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-11T00:00:00Z\" and event_time <= \"2021-03-11T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 12, look back to 12 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-12T00:00:00Z\" and event_time <= \"2021-03-12T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 13, look back to 13 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-13T00:00:00Z\" and event_time <= \"2021-03-13T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregating \"cc_num\" for day 14, look back to 14 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-14T00:00:00Z\" and event_time <= \"2021-03-14T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 15, look back to 15 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-15T00:00:00Z\" and event_time <= \"2021-03-15T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 16, look back to 16 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-16T00:00:00Z\" and event_time <= \"2021-03-16T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 17, look back to 17 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-17T00:00:00Z\" and event_time <= \"2021-03-17T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 18, look back to 18 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-18T00:00:00Z\" and event_time <= \"2021-03-18T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 19, look back to 19 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-19T00:00:00Z\" and event_time <= \"2021-03-19T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 20, look back to 20 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-20T00:00:00Z\" and event_time <= \"2021-03-20T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 21, look back to 21 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-21T00:00:00Z\" and event_time <= \"2021-03-21T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 22, look back to 22 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-22T00:00:00Z\" and event_time <= \"2021-03-22T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 23, look back to 23 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-23T00:00:00Z\" and event_time <= \"2021-03-23T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 24, look back to 24 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-24T00:00:00Z\" and event_time <= \"2021-03-24T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 25, look back to 25 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-25T00:00:00Z\" and event_time <= \"2021-03-25T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 26, look back to 26 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-26T00:00:00Z\" and event_time <= \"2021-03-26T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 27, look back to 27 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-27T00:00:00Z\" and event_time <= \"2021-03-27T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 28, look back to 28 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-28T00:00:00Z\" and event_time <= \"2021-03-28T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 29, look back to 29 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-29T00:00:00Z\" and event_time <= \"2021-03-29T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 30, look back to 30 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-30T00:00:00Z\" and event_time <= \"2021-03-30T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"cc_num\" for day 31, look back to 31 beginning of day...\n",
      " Using query: select a.cc_num, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(cc_num) from trans ) a left join (SELECT cc_num, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-31T00:00:00Z\" and event_time <= \"2021-03-31T23:59:59Z\" GROUP BY cc_num) b on a.cc_num = b.cc_num\n",
      "\n",
      "aggregating \"consumer_id\" for day 01, look back to 01 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-01T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 02, look back to 01 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-02T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 03, look back to 01 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-03T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 04, look back to 01 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-04T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 05, look back to 01 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-05T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 06, look back to 01 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-06T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 07, look back to 01 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-07T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 08, look back to 02 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-02T00:00:00Z\" and event_time <= \"2021-03-08T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 09, look back to 03 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-03T00:00:00Z\" and event_time <= \"2021-03-09T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 10, look back to 04 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-04T00:00:00Z\" and event_time <= \"2021-03-10T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 11, look back to 05 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-05T00:00:00Z\" and event_time <= \"2021-03-11T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 12, look back to 06 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-06T00:00:00Z\" and event_time <= \"2021-03-12T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 13, look back to 07 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-07T00:00:00Z\" and event_time <= \"2021-03-13T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 14, look back to 08 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-08T00:00:00Z\" and event_time <= \"2021-03-14T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregating \"consumer_id\" for day 15, look back to 09 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-09T00:00:00Z\" and event_time <= \"2021-03-15T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 16, look back to 10 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-10T00:00:00Z\" and event_time <= \"2021-03-16T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 17, look back to 11 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-11T00:00:00Z\" and event_time <= \"2021-03-17T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 18, look back to 12 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-12T00:00:00Z\" and event_time <= \"2021-03-18T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 19, look back to 13 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-13T00:00:00Z\" and event_time <= \"2021-03-19T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 20, look back to 14 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-14T00:00:00Z\" and event_time <= \"2021-03-20T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 21, look back to 15 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-15T00:00:00Z\" and event_time <= \"2021-03-21T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 22, look back to 16 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-16T00:00:00Z\" and event_time <= \"2021-03-22T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 23, look back to 17 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-17T00:00:00Z\" and event_time <= \"2021-03-23T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 24, look back to 18 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-18T00:00:00Z\" and event_time <= \"2021-03-24T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 25, look back to 19 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-19T00:00:00Z\" and event_time <= \"2021-03-25T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 26, look back to 20 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-20T00:00:00Z\" and event_time <= \"2021-03-26T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 27, look back to 21 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-21T00:00:00Z\" and event_time <= \"2021-03-27T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 28, look back to 22 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-22T00:00:00Z\" and event_time <= \"2021-03-28T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 29, look back to 23 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-23T00:00:00Z\" and event_time <= \"2021-03-29T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 30, look back to 24 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-24T00:00:00Z\" and event_time <= \"2021-03-30T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 31, look back to 25 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_7d, b.avg_amt_last_7d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_7d, AVG(amount) as avg_amt_last_7d FROM trans where event_time >= \"2021-03-25T00:00:00Z\" and event_time <= \"2021-03-31T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 01, look back to 01 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-01T00:00:00Z\" and event_time <= \"2021-03-01T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 02, look back to 02 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-02T00:00:00Z\" and event_time <= \"2021-03-02T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 03, look back to 03 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-03T00:00:00Z\" and event_time <= \"2021-03-03T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 04, look back to 04 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-04T00:00:00Z\" and event_time <= \"2021-03-04T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 05, look back to 05 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-05T00:00:00Z\" and event_time <= \"2021-03-05T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 06, look back to 06 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-06T00:00:00Z\" and event_time <= \"2021-03-06T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 07, look back to 07 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-07T00:00:00Z\" and event_time <= \"2021-03-07T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 08, look back to 08 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-08T00:00:00Z\" and event_time <= \"2021-03-08T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 09, look back to 09 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-09T00:00:00Z\" and event_time <= \"2021-03-09T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 10, look back to 10 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-10T00:00:00Z\" and event_time <= \"2021-03-10T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 11, look back to 11 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-11T00:00:00Z\" and event_time <= \"2021-03-11T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 12, look back to 12 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-12T00:00:00Z\" and event_time <= \"2021-03-12T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 13, look back to 13 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-13T00:00:00Z\" and event_time <= \"2021-03-13T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 14, look back to 14 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-14T00:00:00Z\" and event_time <= \"2021-03-14T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 15, look back to 15 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-15T00:00:00Z\" and event_time <= \"2021-03-15T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregating \"consumer_id\" for day 16, look back to 16 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-16T00:00:00Z\" and event_time <= \"2021-03-16T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 17, look back to 17 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-17T00:00:00Z\" and event_time <= \"2021-03-17T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 18, look back to 18 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-18T00:00:00Z\" and event_time <= \"2021-03-18T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 19, look back to 19 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-19T00:00:00Z\" and event_time <= \"2021-03-19T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 20, look back to 20 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-20T00:00:00Z\" and event_time <= \"2021-03-20T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 21, look back to 21 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-21T00:00:00Z\" and event_time <= \"2021-03-21T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 22, look back to 22 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-22T00:00:00Z\" and event_time <= \"2021-03-22T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 23, look back to 23 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-23T00:00:00Z\" and event_time <= \"2021-03-23T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 24, look back to 24 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-24T00:00:00Z\" and event_time <= \"2021-03-24T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 25, look back to 25 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-25T00:00:00Z\" and event_time <= \"2021-03-25T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 26, look back to 26 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-26T00:00:00Z\" and event_time <= \"2021-03-26T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 27, look back to 27 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-27T00:00:00Z\" and event_time <= \"2021-03-27T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 28, look back to 28 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-28T00:00:00Z\" and event_time <= \"2021-03-28T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 29, look back to 29 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-29T00:00:00Z\" and event_time <= \"2021-03-29T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 30, look back to 30 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-30T00:00:00Z\" and event_time <= \"2021-03-30T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n",
      "aggregating \"consumer_id\" for day 31, look back to 31 beginning of day...\n",
      " Using query: select a.consumer_id, b.num_trans_last_1d, b.avg_amt_last_1d from (select distinct(consumer_id) from trans ) a left join (SELECT consumer_id, COUNT(*) as num_trans_last_1d, AVG(amount) as avg_amt_last_1d FROM trans where event_time >= \"2021-03-31T00:00:00Z\" and event_time <= \"2021-03-31T23:59:59Z\" GROUP BY consumer_id) b on a.consumer_id = b.consumer_id\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cc_num_rows_7 = agg('cc_num', 7, 1, 31)\n",
    "cc_num_rows_1 = agg('cc_num', 1, 1, 31)\n",
    "consumer_rows_7 = agg('consumer_id', 7, 1, 31)\n",
    "consumer_rows_1 = agg('consumer_id', 1, 1, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6264c8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.29 ms, sys: 642 s, total: 1.93 ms\n",
      "Wall time: 11.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Add a temporary id column to each df, so that we can then join them column-wise\n",
    "\n",
    "cc_num_rows_7 = cc_num_rows_7.withColumn(\"_tmp_id\", monotonically_increasing_id())\n",
    "cc_num_rows_1 = cc_num_rows_1.withColumn(\"_tmp_id\", monotonically_increasing_id())\n",
    "\n",
    "consumer_rows_7 = consumer_rows_7.withColumn(\"_tmp_id\", monotonically_increasing_id())\n",
    "consumer_rows_1 = consumer_rows_1.withColumn(\"_tmp_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1f2e99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.54 ms, sys: 761 s, total: 2.3 ms\n",
      "Wall time: 28.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cc_num_all = cc_num_rows_7.join(cc_num_rows_1.drop('cc_num').drop('event_time'), \"_tmp_id\", \"outer\").drop('_tmp_id')\n",
    "consumer_all = consumer_rows_7.join(consumer_rows_1.drop('consumer_id').drop('event_time'), \"_tmp_id\", \"outer\").drop('_tmp_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b515e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1622040338807'),\n",
       " ('spark.driver.extraClassPath',\n",
       "  '/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemakerruntime-1.11.835.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/hadoop-auth-2.8.1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/hadoop-aws-2.8.1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/sagemaker-spark_2.11-spark_2.4.0-1.4.2.dev0.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sts-1.11.835.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/hadoop-annotations-2.8.1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/hadoop-common-2.8.1.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-kms-1.11.835.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-core-1.11.835.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-sagemaker-1.11.835.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/aws-java-sdk-s3-1.11.835.jar:/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker_pyspark/jars/htrace-core4-4.0.1-incubating.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.driver.host', 'ip-172-16-8-25.ec2.internal'),\n",
       " ('spark.executor.memory', '1g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.executor.cores', '16'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '34845')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89de3f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+------------------+--------------------+-----------------+------------------+\n",
      "|          cc_num|num_trans_last_7d|   avg_amt_last_7d|          event_time|num_trans_last_1d|   avg_amt_last_1d|\n",
      "+----------------+-----------------+------------------+--------------------+-----------------+------------------+\n",
      "|4457570037098706|                5|           167.798|2021-03-31T23:59:59Z|             null|              null|\n",
      "|4733882799298846|               10|203.76700000000002|2021-03-31T23:59:59Z|                2|38.330000000000005|\n",
      "|4491810891527863|               11| 785.8854545454545|2021-03-31T23:59:59Z|                3|221.84666666666666|\n",
      "|4775078220934872|               12|1014.2400000000002|2021-03-31T23:59:59Z|                6|1604.6950000000004|\n",
      "|4693114584822046|               15| 1366.900666666667|2021-03-31T23:59:59Z|             null|              null|\n",
      "|4966380846906071|               11| 184.2709090909091|2021-03-31T23:59:59Z|                4|           445.295|\n",
      "|4020225441904860|               16|1106.6156249999997|2021-03-31T23:59:59Z|                3|174.87333333333333|\n",
      "|4168054039061319|                6| 919.1633333333333|2021-03-31T23:59:59Z|                3| 7.706666666666667|\n",
      "|4444726495789808|               13| 637.7046153846154|2021-03-31T23:59:59Z|                2|55.084999999999994|\n",
      "|4917509454869797|                9|1167.8577777777778|2021-03-31T23:59:59Z|             null|              null|\n",
      "+----------------+-----------------+------------------+--------------------+-----------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 882 ms, sys: 573 ms, total: 1.46 s\n",
      "Wall time: 1min 2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "cc_num_all.orderBy(cc_num_all.event_time.desc()).show(10)\n",
    "cc_num_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac91e363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+------------------+--------------------+-----------------+-----------------+\n",
      "|       consumer_id|num_trans_last_7d|   avg_amt_last_7d|          event_time|num_trans_last_1d|  avg_amt_last_1d|\n",
      "+------------------+-----------------+------------------+--------------------+-----------------+-----------------+\n",
      "|GGVX95242869782633|               20| 948.3839999999998|2021-03-31T23:59:59Z|                3|2383.983333333333|\n",
      "|QRTQ16845213506987|               19| 209.7015789473684|2021-03-31T23:59:59Z|                3|61.22666666666667|\n",
      "|HPQZ79432746178012|               10|          1083.131|2021-03-31T23:59:59Z|                1|           264.36|\n",
      "|YKLN71031286579847|               20|           1103.91|2021-03-31T23:59:59Z|                3|            44.01|\n",
      "|OHGZ44766020135918|               31|490.17161290322593|2021-03-31T23:59:59Z|                5|          223.952|\n",
      "|TDMV74965427985907|               40| 354.3527499999999|2021-03-31T23:59:59Z|                7|32.06857142857143|\n",
      "|QIWM21922454041109|               10|           882.213|2021-03-31T23:59:59Z|                1|             5.96|\n",
      "|ESIM73234049983240|               19|  686.738947368421|2021-03-31T23:59:59Z|                3|           183.97|\n",
      "|SWMS31894220449488|               20|479.33299999999997|2021-03-31T23:59:59Z|                2|            98.14|\n",
      "|PGRR85640123050075|               10| 70.28799999999998|2021-03-31T23:59:59Z|                1|            99.13|\n",
      "+------------------+-----------------+------------------+--------------------+-----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 870 ms, sys: 516 ms, total: 1.39 s\n",
      "Wall time: 55.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "consumer_all.orderBy(consumer_all.event_time.desc()).show(10)\n",
    "consumer_all.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e2ab03",
   "metadata": {},
   "source": [
    "### Now, we will ingest data into the Feature Store\n",
    "\n",
    "We will use Spark to parallelize the ingest of data into the Feature Store, first for consumers and second for credit cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88a2f271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config \n",
    "\n",
    "def ingest_df_to_fg(feature_group_name, rows, columns):\n",
    "    rows = list(rows)\n",
    "    session = boto3.session.Session()\n",
    "    runtime = session.client(service_name='sagemaker-featurestore-runtime',\n",
    "                    config=Config(retries = {'max_attempts': 10, 'mode': 'standard'}))\n",
    "    for index, row in enumerate(rows):\n",
    "        record = [{\"FeatureName\": column, \"ValueAsString\": str(row[column])} \\\n",
    "                   for column in row.__fields__ if row[column] != None]\n",
    "        resp = runtime.put_record(FeatureGroupName=feature_group_name, Record=record)\n",
    "        if not resp['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            raise (f'PutRecord failed: {resp}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fae4ab1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 491 ms, sys: 245 ms, total: 736 ms\n",
      "Wall time: 1min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "columns = ['cc_num','event_time','num_trans_last_7d','num_trans_last_1d','avg_amt_last_7d','avg_amt_last_1d']\n",
    "cc_num_all.foreachPartition(lambda rows: ingest_df_to_fg(CARD_FEATURE_GROUP, rows, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7d5de2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 501 ms, sys: 286 ms, total: 787 ms\n",
      "Wall time: 56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "columns = ['consumer_id','event_time','num_trans_last_7d','num_trans_last_1d','avg_amt_last_7d','avg_amt_last_1d']\n",
    "consumer_all.foreachPartition(lambda rows: ingest_df_to_fg(CONS_FEATURE_GROUP, rows, columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c78d2",
   "metadata": {},
   "source": [
    "## Perform point-in-time correct query\n",
    "We begin by creating an Entity Dataframe which identifies the consumer_ids of interest, coupled with an event_time which represents our cutoff time for that entity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9af604",
   "metadata": {},
   "source": [
    "#### The Entity Dataframe will consist of real Consumer IDs and real event timestamps\n",
    "\n",
    "First, we need to create an Entity Dataframe consisting of a list or our \"target\" Consumer IDs, plus a set of realistic timestamps (event_time) to run the point-in-time queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1819cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_1w_df = spark.sql('select * from trans where event_time >= \"2021-03-25T00:00:00Z\" and event_time <= \"2021-03-31T23:59:59Z\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a0da8d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_ts_tuples = last_1w_df.rdd.map(lambda r: (r.consumer_id, r.cc_num, r.event_time, r.amount, int(r.fraud_label))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e18c7ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22547"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cid_ts_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03930a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the actual Entity Dataframe\n",
    "# (e.g. the dataframe that defines our set of Consumer IDs and timestamps for our point-in-time queries)\n",
    "\n",
    "entity_df_schema = StructType([\n",
    "    StructField('consumer_id', StringType(), False),\n",
    "    StructField('cc_num', StringType(), False),\n",
    "    StructField('query_date', StringType(), False),\n",
    "    StructField('amount', FloatType(), False),\n",
    "    StructField('fraud_label', IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c412902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------------+--------------------+------+-----------+-------+\n",
      "|       consumer_id|          cc_num|          query_date|amount|fraud_label|_tmp_id|\n",
      "+------------------+----------------+--------------------+------+-----------+-------+\n",
      "|QFFO43633815728040|4538707466326165|2021-03-25T00:00:01Z|  1.15|          0|      0|\n",
      "|UUEG06702648357115|4631117361256523|2021-03-25T00:00:03Z|668.13|          0|      1|\n",
      "|TJOJ29599331508229|4717798993183248|2021-03-25T00:00:05Z| 60.88|          0|      2|\n",
      "|MFZR34661365402183|4865185401569996|2021-03-25T00:00:10Z|453.83|          0|      3|\n",
      "|WERB44601853246125|4375373891114563|2021-03-25T00:00:36Z|896.26|          0|      4|\n",
      "|IELN97823885632568|4597025692084773|2021-03-25T00:00:39Z| 58.31|          0|      5|\n",
      "|FYTX19736334250684|4988283988187142|2021-03-25T00:02:42Z|5724.3|          0|      6|\n",
      "|MFUQ42148144236605|4387073979354976|2021-03-25T00:03:59Z|  57.9|          0|      7|\n",
      "|LHTC75807017825780|4238424338006123|2021-03-25T00:04:50Z|701.17|          0|      8|\n",
      "|KWQC10715079779133|4357232209332097|2021-03-25T00:05:13Z| 56.57|          0|      9|\n",
      "+------------------+----------------+--------------------+------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create entity data frame\n",
    "\n",
    "entity_df = spark.createDataFrame(cid_ts_tuples, entity_df_schema)\n",
    "entity_df = entity_df.withColumn(\"_tmp_id\", monotonically_increasing_id())\n",
    "\n",
    "entity_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a665d55a",
   "metadata": {},
   "source": [
    "#### Use Sagemaker Client to find the location of the offline store in S3\n",
    "We will use the `describe_feature_group` method to lookup the S3 Uri location of the Offline Store data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a657f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-blog/572539092864/sagemaker/us-east-1/offline-store/consumer-fg-1622040537/data\n"
     ]
    }
   ],
   "source": [
    "# Lookup S3 Location of Offline Store\n",
    "\n",
    "feature_group_info = sagemaker_client.describe_feature_group(FeatureGroupName=CONS_FEATURE_GROUP)\n",
    "resolved_offline_store_s3_location = feature_group_info['OfflineStoreConfig']['S3StorageConfig']['ResolvedOutputS3Uri']\n",
    "\n",
    "# Spark's Parquet file reader requires replacement of 's3' with 's3a'\n",
    "offline_store_s3a_uri = resolved_offline_store_s3_location.replace(\"s3:\", \"s3a:\")\n",
    "\n",
    "print(offline_store_s3a_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c8084",
   "metadata": {},
   "source": [
    "#### Read the offline store into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e4bd896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.35 ms, sys: 725 s, total: 2.07 ms\n",
      "Wall time: 5.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read Offline Store data\n",
    "feature_store_df = spark.read.parquet(offline_store_s3a_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42a1cd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- consumer_id: string (nullable = true)\n",
      " |-- num_trans_last_7d: long (nullable = true)\n",
      " |-- avg_amt_last_7d: double (nullable = true)\n",
      " |-- num_trans_last_1d: long (nullable = true)\n",
      " |-- avg_amt_last_1d: double (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- write_time: timestamp (nullable = true)\n",
      " |-- api_invocation_time: timestamp (nullable = true)\n",
      " |-- is_deleted: boolean (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_store_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3afc391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 844 s, sys: 454 s, total: 1.3 ms\n",
      "Wall time: 4.06 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27497"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "feature_store_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21336b0",
   "metadata": {},
   "source": [
    "#### Remove records marked as deleted (is_deleted attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98080268",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_active_df = feature_store_df.filter(~feature_store_df.is_deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b43330ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+--------------------+----------+\n",
      "|       consumer_id|   avg_amt_last_7d|          event_time|          write_time|is_deleted|\n",
      "+------------------+------------------+--------------------+--------------------+----------+\n",
      "|MOXJ24964715530559| 75.09666666666666|2021-03-02T23:59:59Z|2021-05-26 15:06:...|     false|\n",
      "|ACQG48444531254282| 5513.706666666666|2021-03-02T23:59:59Z|2021-05-26 15:06:...|     false|\n",
      "|OOZF04050723251487| 141.0366666666667|2021-03-02T23:59:59Z|2021-05-26 15:06:...|     false|\n",
      "|ALVF79373394142214|             350.0|2021-03-02T23:59:59Z|2021-05-26 15:06:...|     false|\n",
      "|ZEQV63475057933816|2596.5850000000005|2021-03-02T23:59:59Z|2021-05-26 15:06:...|     false|\n",
      "+------------------+------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_store_active_df.select('consumer_id', 'avg_amt_last_7d', 'event_time', 'write_time', 'is_deleted').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "685b3c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MOXJ24964715530559\n"
     ]
    }
   ],
   "source": [
    "row1 = feature_store_active_df.first()\n",
    "test_consumer_id = row1['consumer_id']\n",
    "print(test_consumer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "50946995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+--------------------+-----------------------+-------------------+\n",
      "|consumer_id       |avg_amt_last_7d   |event_time          |write_time             |api_invocation_time|\n",
      "+------------------+------------------+--------------------+-----------------------+-------------------+\n",
      "|MOXJ24964715530559|85.655            |2021-03-01T23:59:59Z|2021-05-26 15:06:50.869|2021-05-26 15:06:25|\n",
      "|MOXJ24964715530559|75.09666666666666 |2021-03-02T23:59:59Z|2021-05-26 15:06:50.843|2021-05-26 15:06:25|\n",
      "|MOXJ24964715530559|125.03400000000002|2021-03-03T23:59:59Z|2021-05-26 15:06:51.017|2021-05-26 15:06:37|\n",
      "|MOXJ24964715530559|226.69500000000002|2021-03-04T23:59:59Z|2021-05-26 15:06:51.017|2021-05-26 15:06:25|\n",
      "|MOXJ24964715530559|184.52625         |2021-03-05T23:59:59Z|2021-05-26 15:06:51.015|2021-05-26 15:06:29|\n",
      "|MOXJ24964715530559|414.56666666666666|2021-03-06T23:59:59Z|2021-05-26 15:07:17.895|2021-05-26 15:06:41|\n",
      "|MOXJ24964715530559|579.6627272727272 |2021-03-07T23:59:59Z|2021-05-26 15:06:51.048|2021-05-26 15:06:38|\n",
      "|MOXJ24964715530559|625.933           |2021-03-08T23:59:59Z|2021-05-26 15:06:50.97 |2021-05-26 15:06:37|\n",
      "|MOXJ24964715530559|623.094           |2021-03-09T23:59:59Z|2021-05-26 15:06:51.011|2021-05-26 15:06:37|\n",
      "|MOXJ24964715530559|596.061           |2021-03-10T23:59:59Z|2021-05-26 15:07:17.895|2021-05-26 15:06:50|\n",
      "+------------------+------------------+--------------------+-----------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_store_active_df.select('consumer_id', 'avg_amt_last_7d', 'event_time', 'write_time', 'api_invocation_time')\\\n",
    "    .where(feature_store_active_df.consumer_id == test_consumer_id)\\\n",
    "    .orderBy('event_time','write_time')\\\n",
    "    .show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bbabf9",
   "metadata": {},
   "source": [
    "#### Filter out history that is outside of our target time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae6e0cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_time: 2021-03-25T00:00:01Z, max_time: 2021-03-31T23:58:22Z, staleness days: 14\n"
     ]
    }
   ],
   "source": [
    "# NOTE: This filter is simply a performance optimization\n",
    "# Filter out records from after query max_time and before staleness window prior to the min_time\n",
    "# doing this prior to individual {consumer_id, joindate} filtering will speed up subsequent filters\n",
    "\n",
    "# Choose a \"staleness\" window of time before which we want to ignore records\n",
    "allowed_staleness_days = 14\n",
    "\n",
    "# Eliminate history that is outside of our time window \n",
    "# this window represents the {max_time - min_time} delta, plus our staleness window\n",
    "\n",
    "# entity_df used to define bounded time window\n",
    "minmax_time = entity_df.agg(sql_min(\"query_date\"), sql_max(\"query_date\")).collect()\n",
    "min_time, max_time = minmax_time[0][\"min(query_date)\"], minmax_time[0][\"max(query_date)\"]\n",
    "print(f'min_time: {min_time}, max_time: {max_time}, staleness days: {allowed_staleness_days}')\n",
    "\n",
    "# Via the staleness check, we are actually removing items when event_time is MORE than N days before min_time\n",
    "# Usage: datediff ( enddate, startdate ) - returns days\n",
    "\n",
    "filtered = feature_store_active_df.filter(\n",
    "    (feature_store_active_df.event_time <= max_time) & \n",
    "    (datediff(lit(min_time), feature_store_active_df.event_time) <= allowed_staleness_days)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d9850f",
   "metadata": {},
   "source": [
    "#### Perform the actual point-in-time correct history query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6bafe580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+--------------------+-----------------------+\n",
      "|consumer_id       |query_date          |avg_amt_last_7d   |event_time          |write_time             |\n",
      "+------------------+--------------------+------------------+--------------------+-----------------------+\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|338.835           |2021-03-29T23:59:59Z|2021-05-26 15:07:17.894|\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|326.344           |2021-03-28T23:59:59Z|2021-05-26 15:06:50.839|\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|415.32454545454544|2021-03-27T23:59:59Z|2021-05-26 15:06:51.072|\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|305.768           |2021-03-26T23:59:59Z|2021-05-26 15:06:51.065|\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|875.9472727272729 |2021-03-25T23:59:59Z|2021-05-26 15:07:17.898|\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|948.347           |2021-03-24T23:59:59Z|2021-05-26 15:06:50.968|\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|942.423           |2021-03-23T23:59:59Z|2021-05-26 15:07:17.898|\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|1024.3700000000001|2021-03-22T23:59:59Z|2021-05-26 15:06:51    |\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|1022.423          |2021-03-21T23:59:59Z|2021-05-26 15:06:50.899|\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|1001.8650000000001|2021-03-20T23:59:59Z|2021-05-26 15:06:50.858|\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|1055.4270000000001|2021-03-19T23:59:59Z|2021-05-26 15:06:50.858|\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|394.98400000000004|2021-03-18T23:59:59Z|2021-05-26 15:07:17.897|\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|357.809           |2021-03-17T23:59:59Z|2021-05-26 15:06:50.864|\n",
      "|MOXJ24964715530559|2021-03-30T17:08:23Z|338.835           |2021-03-29T23:59:59Z|2021-05-26 15:07:17.894|\n",
      "|MOXJ24964715530559|2021-03-30T17:08:23Z|326.344           |2021-03-28T23:59:59Z|2021-05-26 15:06:50.839|\n",
      "+------------------+--------------------+------------------+--------------------+-----------------------+\n",
      "only showing top 15 rows\n",
      "\n",
      "CPU times: user 21.1 ms, sys: 2.5 ms, total: 23.6 ms\n",
      "Wall time: 10.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t_joined = (filtered.join(entity_df, filtered.consumer_id == entity_df.consumer_id, 'inner')\n",
    "    .drop(entity_df.consumer_id)\n",
    "    .drop(entity_df._tmp_id))\n",
    "\n",
    "# Filter out data from after query time to remove future data leakage.\n",
    "# Also filter out data that is older than our allowed staleness window (days before each query time)\n",
    "\n",
    "drop_future_and_stale_df = t_joined.filter(\n",
    "    (t_joined.event_time <= entity_df.query_date)\n",
    "    & (datediff(entity_df.query_date, t_joined.event_time) <= allowed_staleness_days))\n",
    "\n",
    "drop_future_and_stale_df.select('consumer_id','query_date','avg_amt_last_7d','event_time','write_time')\\\n",
    "    .where(drop_future_and_stale_df.consumer_id == test_consumer_id)\\\n",
    "    .orderBy(col('query_date').desc(),col('event_time').desc(),col('write_time').desc())\\\n",
    "    .show(15,False)\n",
    "\n",
    "# Group by record id and query timestamp, select only the latest remaining record by event time,\n",
    "# using write time as a tie breaker to account for any more recent backfills or data corrections.\n",
    "\n",
    "latest = drop_future_and_stale_df.rdd.map(lambda x: (f'{x.consumer_id}-{x.query_date}', x))\\\n",
    "            .reduceByKey(\n",
    "                lambda x, y: x if (x.event_time, x.write_time) > (y.event_time, y.write_time) else y).values()\n",
    "latest_df = latest.toDF(drop_future_and_stale_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c673e820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+--------------------+-----------------------+\n",
      "|consumer_id       |query_date          |avg_amt_last_7d   |event_time          |write_time             |\n",
      "+------------------+--------------------+------------------+--------------------+-----------------------+\n",
      "|MOXJ24964715530559|2021-03-31T09:42:23Z|338.835           |2021-03-29T23:59:59Z|2021-05-26 15:07:17.894|\n",
      "|MOXJ24964715530559|2021-03-30T17:08:23Z|338.835           |2021-03-29T23:59:59Z|2021-05-26 15:07:17.894|\n",
      "|MOXJ24964715530559|2021-03-30T00:27:10Z|338.835           |2021-03-29T23:59:59Z|2021-05-26 15:07:17.894|\n",
      "|MOXJ24964715530559|2021-03-29T08:10:42Z|326.344           |2021-03-28T23:59:59Z|2021-05-26 15:06:50.839|\n",
      "|MOXJ24964715530559|2021-03-28T15:41:12Z|415.32454545454544|2021-03-27T23:59:59Z|2021-05-26 15:06:51.072|\n",
      "|MOXJ24964715530559|2021-03-27T22:49:54Z|305.768           |2021-03-26T23:59:59Z|2021-05-26 15:06:51.065|\n",
      "|MOXJ24964715530559|2021-03-27T05:41:33Z|305.768           |2021-03-26T23:59:59Z|2021-05-26 15:06:51.065|\n",
      "|MOXJ24964715530559|2021-03-26T13:05:36Z|875.9472727272729 |2021-03-25T23:59:59Z|2021-05-26 15:07:17.898|\n",
      "|MOXJ24964715530559|2021-03-25T20:17:08Z|948.347           |2021-03-24T23:59:59Z|2021-05-26 15:06:50.968|\n",
      "|MOXJ24964715530559|2021-03-25T03:52:39Z|948.347           |2021-03-24T23:59:59Z|2021-05-26 15:06:50.968|\n",
      "+------------------+--------------------+------------------+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latest_df.select('consumer_id', 'query_date', 'avg_amt_last_7d', 'event_time', 'write_time')\\\n",
    "    .where(latest_df.consumer_id == test_consumer_id)\\\n",
    "    .orderBy(col('query_date').desc(),col('event_time').desc(),col('write_time').desc())\\\n",
    "    .show(15,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26589ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22546"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c37ca565",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ('api_invocation_time','write_time','is_deleted','cc_num','year','month','day','hour')\n",
    "latest_df = latest_df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fdf9bcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+-----------------+------------------+\n",
      "|          query_date|          event_time|   avg_amt_last_7d|num_trans_last_7d|       consumer_id|\n",
      "+--------------------+--------------------+------------------+-----------------+------------------+\n",
      "|2021-03-25T00:00:03Z|2021-03-24T23:59:59Z| 691.0175999999999|               25|UUEG06702648357115|\n",
      "|2021-03-25T00:00:05Z|2021-03-24T23:59:59Z|465.65999999999997|               30|TJOJ29599331508229|\n",
      "|2021-03-25T00:09:12Z|2021-03-24T23:59:59Z|          432.0545|               20|MYWU90501165273252|\n",
      "|2021-03-25T00:15:37Z|2021-03-24T23:59:59Z|          855.8095|               40|ZLUQ21854565784963|\n",
      "|2021-03-25T00:18:08Z|2021-03-24T23:59:59Z| 572.3636666666664|               60|TMBQ32501873277485|\n",
      "+--------------------+--------------------+------------------+-----------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latest_df.select('query_date','event_time','avg_amt_last_7d','num_trans_last_7d','consumer_id').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0f56ad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- consumer_id: string (nullable = true)\n",
      " |-- num_trans_last_7d: long (nullable = true)\n",
      " |-- avg_amt_last_7d: double (nullable = true)\n",
      " |-- num_trans_last_1d: long (nullable = true)\n",
      " |-- avg_amt_last_1d: double (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- query_date: string (nullable = false)\n",
      " |-- amount: float (nullable = false)\n",
      " |-- fraud_label: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "latest_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb130a",
   "metadata": {},
   "source": [
    "## Create a sample training dataset with point-in-time queries against two feature groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342576af",
   "metadata": {},
   "source": [
    "### Reusable function for point-in-time correct queries against a single feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6516e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_feature_values_one_fg(\n",
    "    fg_name: str, entity_df: DataFrame, spark: SparkSession,\n",
    "    allowed_staleness_days: int = 14,\n",
    "    remove_extra_columns: bool = True) -> DataFrame:\n",
    "    \n",
    "    # Get metadata for source feature group\n",
    "    sm_client = boto3.Session().client(service_name='sagemaker')\n",
    "    feature_group_info = sm_client.describe_feature_group(FeatureGroupName=fg_name)\n",
    "\n",
    "    # Get the names of this feature group's RecordId and EventTime features\n",
    "    record_id_name = feature_group_info['RecordIdentifierFeatureName']\n",
    "    event_time_name = feature_group_info['EventTimeFeatureName']\n",
    "    \n",
    "    # Get S3 Location of this feature group's offline store. \n",
    "    # Note Spark's Parquet file reader requires replacement of 's3' with 's3a'\n",
    "    resolved_offline_store_s3_location = \\\n",
    "        feature_group_info['OfflineStoreConfig']['S3StorageConfig']['ResolvedOutputS3Uri']\n",
    "    offline_store_s3a_uri = resolved_offline_store_s3_location.replace(\"s3:\", \"s3a:\")\n",
    "\n",
    "    # Read the offline store into a dataframe\n",
    "    feature_store_df = spark.read.parquet(offline_store_s3a_uri)\n",
    "    \n",
    "    # Filter out deleted records, if any\n",
    "    fs_active_df = feature_store_df.filter(~feature_store_df.is_deleted)\n",
    "    \n",
    "    # Determine min and max time of query timestamps\n",
    "    minmax_time = entity_df.agg(sql_min(\"query_time\"), sql_max(\"query_time\")).collect()\n",
    "    min_time, max_time = minmax_time[0][\"min(query_time)\"], minmax_time[0][\"max(query_time)\"]\n",
    "    \n",
    "    # Remove all rows that are outside of our time window, allowing for a buffer of staleness days\n",
    "    filtered_df = fs_active_df.filter(\n",
    "        (fs_active_df[event_time_name] <= max_time) & \n",
    "        (datediff(lit(min_time), fs_active_df[event_time_name]) <= allowed_staleness_days))\n",
    "    \n",
    "    # Join on record id between the input entity dataframe and the feature history dataframe\n",
    "    joined_df = filtered_df.join(entity_df, \n",
    "                              filtered_df[record_id_name] == entity_df[record_id_name], 'inner')\\\n",
    "                                .drop(entity_df[record_id_name])\n",
    "\n",
    "    # Filter out data from after query time to remove future data leakage\n",
    "    # Also filter out data that is beyond our allowed staleness window (days before each query time)\n",
    "    drop_future_and_stale_df = joined_df.filter(\n",
    "        (joined_df[event_time_name] <= entity_df.query_time)\n",
    "        & (datediff(entity_df.query_time, joined_df[event_time_name]) <= allowed_staleness_days))\n",
    "\n",
    "    # Group by composite key (to uniquely identify the combination of an entity id and a query timestamp),\n",
    "    # and keep only the very latest remaining feature vector coming closest to the input timestamp.\n",
    "    # Use write time as a tie breaker to account for any more recent backfills or data corrections.\n",
    "    latest = drop_future_and_stale_df.rdd.map(lambda x: (f'{x[record_id_name]}-{x.query_time}', x))\\\n",
    "                .reduceByKey(\n",
    "                    lambda x, y: x if (x[event_time_name], x.write_time) > \n",
    "                                      (y[event_time_name], y.write_time) else y).values()\n",
    "    latest_df = latest.toDF(drop_future_and_stale_df.schema)\n",
    "    \n",
    "    # Clean up excess columns\n",
    "    if remove_extra_columns:\n",
    "        cols_to_drop = ('api_invocation_time','write_time','is_deleted',\n",
    "                        record_id_name,'query_time',event_time_name,\n",
    "                        'year','month','day','hour')\n",
    "        latest_df = latest_df.drop(*cols_to_drop)\n",
    "    \n",
    "    # Return results of point in time query\n",
    "    return latest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97ad137",
   "metadata": {},
   "source": [
    "### Use a handful of transactions from our transactions history as a base \n",
    "Note that we select a pair of entity identifiers, `consumer_id` and `cc_num` to drive corresponding \n",
    "queries against feature value history for those entities. We also add a monotonically increasing temporary\n",
    "identifier to the query dataset. This will let us do an accurate final join of the results of each\n",
    "entity-specific point-in-time query result into a combined training dataset containing multiple \n",
    "feature vectors for each transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7bb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count = 10\n",
    "my_entity_df = transactions_df.select('consumer_id','cc_num','event_time','amount','fraud_label')\\\n",
    "                    .orderBy(col('event_time').desc()).limit(sample_count)\n",
    "my_entity_cols = ['consumer_id','cc_num','query_time','amount','fraud_label']\n",
    "my_entity_df = my_entity_df.toDF(*my_entity_cols)\n",
    "my_entity_df = my_entity_df.withColumn(\"_tmp_id\", monotonically_increasing_id())\n",
    "my_entity_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180ca0c",
   "metadata": {},
   "source": [
    "### Do a point-in-time correct query to retrieve Consumer features for each transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc680c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_df = get_historical_feature_values_one_fg(CONS_FEATURE_GROUP, my_entity_df, spark)\n",
    "cons_df.show(sample_count, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9270bc87",
   "metadata": {},
   "source": [
    "### Do a point-in-time correct query to retrieve Credit Card features for each transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3431468",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_df = get_historical_feature_values_one_fg(CARD_FEATURE_GROUP, my_entity_df, spark)\n",
    "card_df.show(sample_count, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8bdc2b",
   "metadata": {},
   "source": [
    "### Join the feature vectors from each feature group to form the final training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a34922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop id's, as they aren't used for training\n",
    "cols_to_drop = ('cc_num','consumer_id')\n",
    "\n",
    "# In this example, the column names are not unique. Add a prefix so they can be distinct in the training dataset\n",
    "new_card_col_names = ('card_num_trans_last_7d', 'card_avg_amt_last_7d', \n",
    "                 'card_num_trans_last_1d', 'card_avg_amt_last_1d', 'amount', 'fraud_label', '_tmp_id')\n",
    "card_df = card_df.drop(*cols_to_drop).toDF(*new_card_col_names)\n",
    "card_df.show(sample_count)\n",
    "\n",
    "new_cons_col_names = ('cons_num_trans_last_7d', 'cons_avg_amt_last_7d', \n",
    "                 'cons_num_trans_last_1d', 'cons_avg_amt_last_1d', 'amount', 'fraud_label', '_tmp_id')\n",
    "cons_df = cons_df.drop(*cols_to_drop).toDF(*new_cons_col_names)\n",
    "cons_df.show(sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc28524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the feature vectors from each entity into a single training dataset\n",
    "training_df = card_df.drop('fraud_label').drop('amount').join(cons_df, \"_tmp_id\", \"outer\").drop(\"_tmp_id\").fillna(0)\n",
    "training_df.show(sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc54d74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
