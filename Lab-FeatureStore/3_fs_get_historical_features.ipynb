{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "935399a4",
   "metadata": {},
   "source": [
    "### Use SageMaker Feature Store and Apache Spark to generate Point-In-Time queries\n",
    "\n",
    "The following notebook uses SageMaker Feature Store and Apache Spark to build out a set of Dataframes and queries that provide a pattern for historical lookup capabilities. We will demonstrate how to build \"point-in-time\" feature sets by starting with raw transactional data, joining that data with records from the Offline Feature Store, and then building an Entity Dataframe to define the items we care about and the timestamp of reference. Techniques include building Spark Dataframes, using outer and inner table joins, using Dataframe filters to prune items outside our timeframe, and finally using Spark `reduceByKey` to reduce the final the dataset. \n",
    "\n",
    "Notes: This notebook relies on the outputs of the other two notebooks in this repo, which do the following:\n",
    "- `1_generate_creditcard_transactions.ipynb` : generates raw transaction data for credit cards and consumers\n",
    "- `2_create_feature_groups.ipynb` : creates two feature groups which are populated below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6c422",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6259579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sm\n",
    "sm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42abfb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyspark and build Spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import datediff\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import max as sql_max\n",
    "from pyspark.sql.functions import min as sql_min\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FractionalType\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import sagemaker_pyspark\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53ea07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .config(\"spark.executor.memory\", '1g')\n",
    "    .config('spark.executor.cores', '16')\n",
    "    .config(\"spark.driver.memory\",'8g')\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a33160",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932bd603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "\n",
    "BUCKET = sagemaker_session.default_bucket()\n",
    "print(BUCKET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup config variables, paths, names, etc.\n",
    "import os\n",
    "\n",
    "BASE_PREFIX = \"sagemaker-featurestore-blog\"\n",
    "\n",
    "OFFLINE_STORE_BASE_URI = f's3://{BUCKET}/{BASE_PREFIX}'\n",
    "\n",
    "RAW_PREFIX = os.path.join(BASE_PREFIX, 'raw')\n",
    "AGG_PREFIX = os.path.join(BASE_PREFIX, 'aggregated')\n",
    "\n",
    "RAW_FEATURES_PATH_S3 = f\"s3://{BUCKET}/{RAW_PREFIX}/\"\n",
    "RAW_FEATURES_PATH_PARQUET = f\"s3a://{BUCKET}/{RAW_PREFIX}/\"\n",
    "print(f'S3 Raw Transactions S3 path: {RAW_FEATURES_PATH_S3}')\n",
    "\n",
    "AGG_FEATURES_PATH_S3 = f\"s3://{BUCKET}/{AGG_PREFIX}/\"\n",
    "AGG_FEATURES_PATH_PARQUET = f\"s3a://{BUCKET}/{AGG_PREFIX}/\"\n",
    "print(f'S3 Aggregated Data S3 Path: {AGG_FEATURES_PATH_S3}')\n",
    "\n",
    "CONS_FEATURE_GROUP = \"consumer-fg\"\n",
    "CARD_FEATURE_GROUP = \"credit-card-fg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1350a894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Store Group requires ISO-8601 string format: yyyy-MM-dd'T'HH:mm:ssZ\n",
    "# when the EventTime required attribute is type String\n",
    "\n",
    "ISO_8601_DATETIME_FORMAT = \"%Y-%m-%dT%H:%M:%SZ\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a37268a",
   "metadata": {},
   "source": [
    "## Generate and ingest agg features for a credit card fg and a consumer fg\n",
    "This section can be moved to another preparation notebook to be run before the point in time query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa35c247",
   "metadata": {},
   "source": [
    "#### Let's retreive our credit card transaction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3986aa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, TimestampType, LongType\n",
    "\n",
    "raw_schema = StructType([StructField('tid', StringType(), True),\n",
    "                    StructField('event_time', StringType(), True),\n",
    "                    StructField('cc_num', LongType(), True),\n",
    "                    StructField('consumer_id', StringType(), True),\n",
    "                    StructField('amount', DoubleType(), True),\n",
    "                    StructField('fraud_label', StringType(), True)])\n",
    "\n",
    "# Build path to transactions data file\n",
    "raw_file = os.path.join(RAW_FEATURES_PATH_PARQUET, 'transactions.csv')\n",
    "print(raw_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92658fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read.csv(raw_file, header=True, schema=raw_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614dc033",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.printSchema()\n",
    "transactions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb344ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df.createOrReplaceTempView('trans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104962e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def agg(by_col, lookback_days, start_day, end_day):\n",
    "    transactions_df.createOrReplaceTempView('trans')\n",
    "    all_agg_rows = None\n",
    "    for curr_day in range(start_day, end_day + 1):\n",
    "        min_day = max(1, (curr_day - lookback_days +1))\n",
    "        print(f'aggregating \"{by_col}\" for day {curr_day:02d}, look back to {min_day:02d} beginning of day...')\n",
    "        start_time = f'2021-03-{min_day:02d}T00:00:00Z'\n",
    "        end_time = f'2021-03-{curr_day:02d}T23:59:59Z'\n",
    "        event_time = end_time\n",
    "\n",
    "        sub_query = f'SELECT {by_col}, '\n",
    "        sub_query += f'COUNT(*) as num_trans_last_{lookback_days}d, AVG(amount) as avg_amt_last_{lookback_days}d FROM trans'\n",
    "        sub_query += f' where event_time >= \"{start_time}\" and event_time <= \"{end_time}\" GROUP BY {by_col}'\n",
    "\n",
    "        d_query = f'select distinct({by_col}) from trans '\n",
    "\n",
    "        total_query = f'select a.{by_col}, b.num_trans_last_{lookback_days}d, b.avg_amt_last_{lookback_days}d from ({d_query}) a left join ({sub_query}) b on a.{by_col} = b.{by_col}'\n",
    "        print(f' Using query: {total_query}\\n')\n",
    "        total_df = spark.sql(total_query)\n",
    "\n",
    "        # add a column to flag all of these records with an event time of the running of this \"daily batch job\"\n",
    "        total_df = total_df.withColumn('event_time', lit(event_time))\n",
    "#         print(f' {total_df.count()} rows')\n",
    "        \n",
    "        if all_agg_rows is None:\n",
    "            all_agg_rows = spark.createDataFrame([], total_df.schema)\n",
    "        all_agg_rows = all_agg_rows.union(total_df)\n",
    "        del total_df\n",
    "        \n",
    "    return all_agg_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd2902",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_num_rows_7 = agg('cc_num', 7, 1, 31)\n",
    "cc_num_rows_1 = agg('cc_num', 1, 1, 31)\n",
    "consumer_rows_7 = agg('consumer_id', 7, 1, 31)\n",
    "consumer_rows_1 = agg('consumer_id', 1, 1, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "# Add a temporary id column to each df, so that we can then join them column-wise\n",
    "\n",
    "cc_num_rows_7 = cc_num_rows_7.withColumn(\"_tmp_id\", monotonically_increasing_id())\n",
    "cc_num_rows_1 = cc_num_rows_1.withColumn(\"_tmp_id\", monotonically_increasing_id())\n",
    "\n",
    "consumer_rows_7 = consumer_rows_7.withColumn(\"_tmp_id\", monotonically_increasing_id())\n",
    "consumer_rows_1 = consumer_rows_1.withColumn(\"_tmp_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fbae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cc_num_all = cc_num_rows_7.join(cc_num_rows_1.drop('cc_num').drop('event_time'), \"_tmp_id\", \"outer\").drop('_tmp_id')\n",
    "consumer_all = consumer_rows_7.join(consumer_rows_1.drop('consumer_id').drop('event_time'), \"_tmp_id\", \"outer\").drop('_tmp_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acf9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff72c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cc_num_all.orderBy(cc_num_all.event_time.desc()).show(10)\n",
    "cc_num_all.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "consumer_all.orderBy(consumer_all.event_time.desc()).show(10)\n",
    "consumer_all.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f051490",
   "metadata": {},
   "source": [
    "### Now, we will ingest data into the Feature Store\n",
    "\n",
    "We will use Spark to parallelize the ingest of data into the Feature Store, first for consumers and second for credit cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd0e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.config import Config \n",
    "\n",
    "def ingest_df_to_fg(feature_group_name, rows, columns):\n",
    "    rows = list(rows)\n",
    "    session = boto3.session.Session()\n",
    "    runtime = session.client(service_name='sagemaker-featurestore-runtime',\n",
    "                    config=Config(retries = {'max_attempts': 10, 'mode': 'standard'}))\n",
    "    for index, row in enumerate(rows):\n",
    "        record = [{\"FeatureName\": column, \"ValueAsString\": str(row[column])} \\\n",
    "                   for column in row.__fields__ if row[column] != None]\n",
    "        resp = runtime.put_record(FeatureGroupName=feature_group_name, Record=record)\n",
    "        if not resp['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            raise (f'PutRecord failed: {resp}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc51b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "columns = ['cc_num','event_time','num_trans_last_7d','num_trans_last_1d','avg_amt_last_7d','avg_amt_last_1d']\n",
    "cc_num_all.foreachPartition(lambda rows: ingest_df_to_fg(CARD_FEATURE_GROUP, rows, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9937e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "columns = ['consumer_id','event_time','num_trans_last_7d','num_trans_last_1d','avg_amt_last_7d','avg_amt_last_1d']\n",
    "consumer_all.foreachPartition(lambda rows: ingest_df_to_fg(CONS_FEATURE_GROUP, rows, columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea2e54",
   "metadata": {},
   "source": [
    "## Perform point-in-time correct query\n",
    "We begin by creating an Entity Dataframe which identifies the consumer_ids of interest, paired with an event_time which represents our cutoff time for that entity. The consumer_id is used to join data from the raw transaction dataset, and the event_time is used within filter operations to identity the \"point-in-time\" correct data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8926c134",
   "metadata": {},
   "source": [
    "#### The Entity Dataframe will consist of real Consumer IDs and real event timestamps\n",
    "\n",
    "First, we need to create an Entity Dataframe consisting of a list or our \"target\" Consumer IDs, plus a set of realistic timestamps (event_time) to run the point-in-time queries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_1w_df = spark.sql('select * from trans where event_time >= \"2021-03-25T00:00:00Z\" and event_time <= \"2021-03-31T23:59:59Z\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "cid_ts_tuples = last_1w_df.rdd.map(lambda r: (r.consumer_id, r.cc_num, r.event_time, r.amount, int(r.fraud_label))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db45678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cid_ts_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128bf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the actual Entity Dataframe\n",
    "# (e.g. the dataframe that defines our set of Consumer IDs and timestamps for our point-in-time queries)\n",
    "\n",
    "entity_df_schema = StructType([\n",
    "    StructField('consumer_id', StringType(), False),\n",
    "    StructField('cc_num', StringType(), False),\n",
    "    StructField('query_date', StringType(), False),\n",
    "    StructField('amount', FloatType(), False),\n",
    "    StructField('fraud_label', IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48df3ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity data frame\n",
    "\n",
    "entity_df = spark.createDataFrame(cid_ts_tuples, entity_df_schema)\n",
    "\n",
    "entity_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b3387",
   "metadata": {},
   "source": [
    "#### Use Sagemaker Client to find the location of the offline store in S3\n",
    "We will use the `describe_feature_group` method to lookup the S3 Uri location of the Offline Store data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup S3 Location of Offline Store\n",
    "\n",
    "feature_group_info = sagemaker_client.describe_feature_group(FeatureGroupName=CONS_FEATURE_GROUP)\n",
    "resolved_offline_store_s3_location = feature_group_info['OfflineStoreConfig']['S3StorageConfig']['ResolvedOutputS3Uri']\n",
    "\n",
    "# Spark's Parquet file reader requires replacement of 's3' with 's3a'\n",
    "offline_store_s3a_uri = resolved_offline_store_s3_location.replace(\"s3:\", \"s3a:\")\n",
    "\n",
    "print(offline_store_s3a_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81895a94",
   "metadata": {},
   "source": [
    "#### Read the offline store into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09568356",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Read Offline Store data\n",
    "feature_store_df = spark.read.parquet(offline_store_s3a_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f4070",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f306dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "feature_store_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b9acf",
   "metadata": {},
   "source": [
    "#### Remove records marked as deleted (is_deleted attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b42ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_active_df = feature_store_df.filter(~feature_store_df.is_deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3bd2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_active_df.select('consumer_id', 'avg_amt_last_7d', 'event_time', 'write_time', 'is_deleted').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df72d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "row1 = feature_store_active_df.first()\n",
    "test_consumer_id = row1['consumer_id']\n",
    "print(test_consumer_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dcaa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_active_df.select('consumer_id', 'avg_amt_last_7d', 'event_time', 'write_time', 'api_invocation_time')\\\n",
    "    .where(feature_store_active_df.consumer_id == test_consumer_id)\\\n",
    "    .orderBy('event_time','write_time')\\\n",
    "    .show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df1c40d",
   "metadata": {},
   "source": [
    "#### Filter out history that is outside of our target time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d52f338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This filter is simply a performance optimization\n",
    "# Filter out records from after query max_time and before staleness window prior to the min_time\n",
    "# doing this prior to individual {consumer_id, joindate} filtering will speed up subsequent filters\n",
    "\n",
    "# Choose a \"staleness\" window of time before which we want to ignore records\n",
    "allowed_staleness_days = 14\n",
    "\n",
    "# Eliminate history that is outside of our time window \n",
    "# this window represents the {max_time - min_time} delta, plus our staleness window\n",
    "\n",
    "# entity_df used to define bounded time window\n",
    "minmax_time = entity_df.agg(sql_min(\"query_date\"), sql_max(\"query_date\")).collect()\n",
    "min_time, max_time = minmax_time[0][\"min(query_date)\"], minmax_time[0][\"max(query_date)\"]\n",
    "print(f'min_time: {min_time}, max_time: {max_time}, staleness days: {allowed_staleness_days}')\n",
    "\n",
    "# Via the staleness check, we are actually removing items when event_time is MORE than N days before min_time\n",
    "# Usage: datediff ( enddate, startdate ) - returns days\n",
    "\n",
    "filtered = feature_store_active_df.filter(\n",
    "    (feature_store_active_df.event_time <= max_time) & \n",
    "    (datediff(lit(min_time), feature_store_active_df.event_time) <= allowed_staleness_days)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68045847",
   "metadata": {},
   "source": [
    "#### Perform the actual point-in-time correct history query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38c276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_joined = (filtered.join(entity_df, filtered.consumer_id == entity_df.consumer_id, 'inner')\n",
    "    .drop(entity_df.consumer_id))\n",
    "\n",
    "# Filter out data from after query time to remove future data leakage.\n",
    "# Also filter out data that is older than our allowed staleness window (days before each query time)\n",
    "\n",
    "drop_future_and_stale_df = t_joined.filter(\n",
    "    (t_joined.event_time <= entity_df.query_date)\n",
    "    & (datediff(entity_df.query_date, t_joined.event_time) <= allowed_staleness_days))\n",
    "\n",
    "drop_future_and_stale_df.select('consumer_id','query_date','avg_amt_last_7d','event_time','write_time')\\\n",
    "    .where(drop_future_and_stale_df.consumer_id == test_consumer_id)\\\n",
    "    .orderBy(col('query_date').desc(),col('event_time').desc(),col('write_time').desc())\\\n",
    "    .show(15,False)\n",
    "\n",
    "# Group by record id and query timestamp, select only the latest remaining record by event time,\n",
    "# using write time as a tie breaker to account for any more recent backfills or data corrections.\n",
    "\n",
    "latest = drop_future_and_stale_df.rdd.map(lambda x: (f'{x.consumer_id}-{x.query_date}', x))\\\n",
    "            .reduceByKey(\n",
    "                lambda x, y: x if (x.event_time, x.write_time) > (y.event_time, y.write_time) else y).values()\n",
    "latest_df = latest.toDF(drop_future_and_stale_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d226154",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_df.select('consumer_id', 'query_date', 'avg_amt_last_7d', 'event_time', 'write_time')\\\n",
    "    .where(latest_df.consumer_id == test_consumer_id)\\\n",
    "    .orderBy(col('query_date').desc(),col('event_time').desc(),col('write_time').desc())\\\n",
    "    .show(15,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ee38b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad7559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ('api_invocation_time','write_time','is_deleted','cc_num','year','month','day','hour')\n",
    "latest_df = latest_df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_df.select('query_date','event_time','avg_amt_last_7d','num_trans_last_7d','consumer_id').sample(\n",
    "    withReplacement=False, fraction=0.001).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff318ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cad6ca4",
   "metadata": {},
   "source": [
    "## Create a sample training dataset with point-in-time queries against two feature groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13abc553",
   "metadata": {},
   "source": [
    "### Reusable function for point-in-time correct queries against a single feature group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_feature_values_one_fg(\n",
    "    fg_name: str, entity_df: DataFrame, spark: SparkSession,\n",
    "    allowed_staleness_days: int = 14,\n",
    "    remove_extra_columns: bool = True) -> DataFrame:\n",
    "    \n",
    "    # Get metadata for source feature group\n",
    "    sm_client = boto3.Session().client(service_name='sagemaker')\n",
    "    feature_group_info = sm_client.describe_feature_group(FeatureGroupName=fg_name)\n",
    "\n",
    "    # Get the names of this feature group's RecordId and EventTime features\n",
    "    record_id_name = feature_group_info['RecordIdentifierFeatureName']\n",
    "    event_time_name = feature_group_info['EventTimeFeatureName']\n",
    "    \n",
    "    # Get S3 Location of this feature group's offline store. \n",
    "    # Note Spark's Parquet file reader requires replacement of 's3' with 's3a'\n",
    "    resolved_offline_store_s3_location = \\\n",
    "        feature_group_info['OfflineStoreConfig']['S3StorageConfig']['ResolvedOutputS3Uri']\n",
    "    offline_store_s3a_uri = resolved_offline_store_s3_location.replace(\"s3:\", \"s3a:\")\n",
    "\n",
    "    # Read the offline store into a dataframe\n",
    "    feature_store_df = spark.read.parquet(offline_store_s3a_uri)\n",
    "    \n",
    "    # Filter out deleted records, if any\n",
    "    fs_active_df = feature_store_df.filter(~feature_store_df.is_deleted)\n",
    "    \n",
    "    # Determine min and max time of query timestamps\n",
    "    minmax_time = entity_df.agg(sql_min(\"query_time\"), sql_max(\"query_time\")).collect()\n",
    "    min_time, max_time = minmax_time[0][\"min(query_time)\"], minmax_time[0][\"max(query_time)\"]\n",
    "    \n",
    "    # Remove all rows that are outside of our time window, allowing for a buffer of staleness days\n",
    "    filtered_df = fs_active_df.filter(\n",
    "        (fs_active_df[event_time_name] <= max_time) & \n",
    "        (datediff(lit(min_time), fs_active_df[event_time_name]) <= allowed_staleness_days))\n",
    "    \n",
    "    # Join on record id between the input entity dataframe and the feature history dataframe\n",
    "    joined_df = filtered_df.join(entity_df, \n",
    "                              filtered_df[record_id_name] == entity_df[record_id_name], 'inner')\\\n",
    "                                .drop(entity_df[record_id_name])\n",
    "\n",
    "    # Filter out data from after query time to remove future data leakage\n",
    "    # Also filter out data that is beyond our allowed staleness window (days before each query time)\n",
    "    drop_future_and_stale_df = joined_df.filter(\n",
    "        (joined_df[event_time_name] <= entity_df.query_time)\n",
    "        & (datediff(entity_df.query_time, joined_df[event_time_name]) <= allowed_staleness_days))\n",
    "\n",
    "    # Group by composite key (to uniquely identify the combination of an entity id and a query timestamp),\n",
    "    # and keep only the very latest remaining feature vector coming closest to the input timestamp.\n",
    "    # Use write time as a tie breaker to account for any more recent backfills or data corrections.\n",
    "    latest = drop_future_and_stale_df.rdd.map(lambda x: (f'{x[record_id_name]}-{x.query_time}', x))\\\n",
    "                .reduceByKey(\n",
    "                    lambda x, y: x if (x[event_time_name], x.write_time) > \n",
    "                                      (y[event_time_name], y.write_time) else y).values()\n",
    "    latest_df = latest.toDF(drop_future_and_stale_df.schema)\n",
    "    \n",
    "    # Clean up excess columns\n",
    "    if remove_extra_columns:\n",
    "        cols_to_drop = ('api_invocation_time','write_time','is_deleted',\n",
    "                        record_id_name,'query_time',event_time_name,\n",
    "                        'year','month','day','hour')\n",
    "        latest_df = latest_df.drop(*cols_to_drop)\n",
    "    \n",
    "    # Return results of point in time query\n",
    "    return latest_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbed6ad",
   "metadata": {},
   "source": [
    "### Use a handful of transactions from our transactions history as a base \n",
    "Note that we select a pair of entity identifiers, `consumer_id` and `cc_num` to drive corresponding \n",
    "queries against feature value history for those entities. We also add a monotonically increasing temporary\n",
    "identifier to the query dataset. This will let us do an accurate final join of the results of each\n",
    "entity-specific point-in-time query result into a combined training dataset containing multiple \n",
    "feature vectors for each transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0f8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_count = 10\n",
    "my_entity_df = transactions_df.select('consumer_id','cc_num','event_time','amount','fraud_label')\\\n",
    "                    .orderBy(col('event_time').desc()).limit(sample_count)\n",
    "my_entity_cols = ['consumer_id','cc_num','query_time','amount','fraud_label']\n",
    "my_entity_df = my_entity_df.toDF(*my_entity_cols)\n",
    "\n",
    "my_entity_df = my_entity_df.withColumn(\"_tmp_id\", monotonically_increasing_id())\n",
    "my_entity_df.drop('_tmp_id').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ab62a6",
   "metadata": {},
   "source": [
    "### Do a point-in-time correct query to retrieve Consumer features for each transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cfd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_df = get_historical_feature_values_one_fg(CONS_FEATURE_GROUP, my_entity_df, spark)\n",
    "cons_df.select('num_trans_last_7d', 'avg_amt_last_7d', 'num_trans_last_1d', 'avg_amt_last_1d', \n",
    "               'fraud_label').show(sample_count, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63687ac1",
   "metadata": {},
   "source": [
    "### Do a point-in-time correct query to retrieve Credit Card features for each transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830216a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "card_df = get_historical_feature_values_one_fg(CARD_FEATURE_GROUP, my_entity_df, spark)\n",
    "card_df.select('num_trans_last_7d', 'avg_amt_last_7d', 'num_trans_last_1d', 'avg_amt_last_1d', \n",
    "               'fraud_label').show(sample_count, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547c5e1",
   "metadata": {},
   "source": [
    "### Join the feature vectors from each feature group to form the final training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73edf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop id's, as they aren't used for training\n",
    "cols_to_drop = ('cc_num','consumer_id')\n",
    "\n",
    "# In this example, the column names are not unique. Add a prefix so they can be distinct in the training dataset\n",
    "new_card_col_names = ('card_num_trans_last_7d', 'card_avg_amt_last_7d', \n",
    "                 'card_num_trans_last_1d', 'card_avg_amt_last_1d', 'amount', 'fraud_label', '_tmp_id')\n",
    "card_df = card_df.drop(*cols_to_drop).toDF(*new_card_col_names)\n",
    "card_df.show(sample_count)\n",
    "\n",
    "new_cons_col_names = ('cons_num_trans_last_7d', 'cons_avg_amt_last_7d', \n",
    "                 'cons_num_trans_last_1d', 'cons_avg_amt_last_1d', 'amount', 'fraud_label', '_tmp_id')\n",
    "cons_df = cons_df.drop(*cols_to_drop).toDF(*new_cons_col_names)\n",
    "cons_df.show(sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca92542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the feature vectors from each entity into a single training dataset\n",
    "training_df = card_df.drop('fraud_label').drop('amount').join(cons_df, \"_tmp_id\", \"outer\").drop(\"_tmp_id\").fillna(0)\n",
    "training_df.show(sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325d184",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
