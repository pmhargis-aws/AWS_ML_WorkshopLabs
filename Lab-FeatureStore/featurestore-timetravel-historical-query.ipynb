{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SageMaker Feature Store and Apache Spark to generate point-in-time queries to implement Time Travel\n",
    "The following notebook builds out a set of Dataframes and Spark queries that provide a pattern for using \"Time Travel\" capabilities that leverage SageMaker Feature Store. We will demonstrate how to build a point-in-time feature sets. Techniques include building Spark Dataframes and using cross-table joins and query filters to reduce the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faker\n",
    "from faker import Faker\n",
    "\n",
    "# Import pyspark and build Spark session\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import datediff\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import max as sql_max\n",
    "from pyspark.sql.functions import min as sql_min\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import sagemaker_pyspark\n",
    "import datetime\n",
    "\n",
    "# Configure Spark to use the SageMaker Spark dependency jars\n",
    "classpath = \":\".join(sagemaker_pyspark.classpath_jars())\n",
    "\n",
    "#print(f\"Spark classpath_jars: {classpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .getOrCreate())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.4\n"
     ]
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 123456\n",
    "faker = Faker()\n",
    "faker.seed_locale('en_US', 0)\n",
    "faker.seed_instance(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-us-east-1-572539092864\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "BUCKET = sagemaker.Session().default_bucket()\n",
    "print(BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo\n",
      "sagemaker-featurestore-demo/aggregated\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "BASE_PREFIX = \"sagemaker-featurestore-demo\"\n",
    "OFFLINE_STORE_BASE_URI = f's3://{BUCKET}/{BASE_PREFIX}'\n",
    "print(OFFLINE_STORE_BASE_URI)\n",
    "\n",
    "AGG_PREFIX = os.path.join(BASE_PREFIX, 'aggregated')\n",
    "print(AGG_PREFIX)\n",
    "\n",
    "AGG_FEATURES_PATH_S3 = f\"s3://{BUCKET}/{AGG_PREFIX}/\"\n",
    "AGG_FEATURES_PATH_PARQUET = f\"s3a://{BUCKET}/{AGG_PREFIX}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using S3 path: s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/aggregated/\n",
      "Found files: \n",
      "s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/aggregated/_SUCCESS\n",
      "s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/aggregated/part-00000-8516931f-40c6-4755-b0a2-79f831d24d50-c000.csv\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "file_list = S3Downloader.list(AGG_FEATURES_PATH_S3)\n",
    "\n",
    "print(f'Using S3 path: {AGG_FEATURES_PATH_S3}')\n",
    "print(\"Found files: \\n\" + \"\\n\".join(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions_df = spark.read.options(Header=True).csv(AGG_FEATURES_PATH_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tid: string (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- cc_num: string (nullable = true)\n",
      " |-- amount: string (nullable = true)\n",
      " |-- fraud_label: string (nullable = true)\n",
      " |-- num_trans_last_60m: string (nullable = true)\n",
      " |-- avg_amt_last_60m: string (nullable = true)\n",
      " |-- num_trans_last_1d: string (nullable = true)\n",
      " |-- avg_amt_last_1d: string (nullable = true)\n",
      " |-- amt_ratio1: string (nullable = true)\n",
      " |-- amt_ratio2: string (nullable = true)\n",
      " |-- count_ratio: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transactions_df.printSchema()\n",
    "transactions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------+------+-----------+------------------+----------------+-----------------+------------------+-------------------+-------------------+------------------+\n",
      "|                 tid|          event_time|          cc_num|amount|fraud_label|num_trans_last_60m|avg_amt_last_60m|num_trans_last_1d|   avg_amt_last_1d|         amt_ratio1|         amt_ratio2|       count_ratio|\n",
      "+--------------------+--------------------+----------------+------+-----------+------------------+----------------+-----------------+------------------+-------------------+-------------------+------------------+\n",
      "|9f2457e8699918753...|2021-01-01T13:18:...|4006080197832643|  60.0|          0|                 1|            60.0|                1|              60.0|                1.0|                1.0|               1.0|\n",
      "|ea6201607ef021510...|2021-01-02T06:43:...|4006080197832643|   1.8|          0|                 1|             1.8|                2|              30.9|0.05825242718446602|0.05825242718446602|               0.5|\n",
      "|ea7f10d1474da6c00...|2021-01-02T18:51:...|4006080197832643| 340.1|          0|                 1|           340.1|                3|133.96666666666667| 2.5386912167205775| 2.5386912167205775|0.3333333333333333|\n",
      "|dc3df677f3d93761b...|2021-01-02T23:07:...|4006080197832643|  85.2|          0|                 1|            85.2|                4|           121.775| 0.6996509956887703| 0.6996509956887703|              0.25|\n",
      "|313df5414788e7865...|2021-01-03T10:11:...|4006080197832643|677.75|          0|                 1|          677.75|                5|232.96999999999997| 2.9091728548740186| 2.9091728548740186|               0.2|\n",
      "+--------------------+--------------------+----------------+------+-----------+------------------+----------------+-----------------+------------------+-------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transactions_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Sagemaker Client to retrieve info about Feature Group\n",
    "We will use the `describe_feature_group` method to lookup the S3 Uri location of the Offline Store data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.Session().client(service_name='sagemaker')\n",
    "#feature_store_client = boto3.client(service_name='sagemaker-featurestore-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/cc-agg-batch-fg', 'FeatureGroupName': 'cc-agg-batch-fg', 'RecordIdentifierFeatureName': 'cc_num', 'EventTimeFeatureName': 'trans_time', 'FeatureDefinitions': [{'FeatureName': 'tid', 'FeatureType': 'String'}, {'FeatureName': 'cc_num', 'FeatureType': 'Integral'}, {'FeatureName': 'num_trans_last_1d', 'FeatureType': 'Integral'}, {'FeatureName': 'avg_amt_last_1d', 'FeatureType': 'Fractional'}, {'FeatureName': 'event_time', 'FeatureType': 'String'}, {'FeatureName': 'trans_time', 'FeatureType': 'Fractional'}], 'CreationTime': datetime.datetime(2021, 4, 15, 12, 27, 20, 670000, tzinfo=tzlocal()), 'OnlineStoreConfig': {'EnableOnlineStore': True}, 'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': 's3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo'}, 'DisableGlueTableCreation': False, 'DataCatalogConfig': {'TableName': 'cc-agg-batch-fg-1618489640', 'Catalog': 'AwsDataCatalog', 'Database': 'sagemaker_featurestore'}}, 'RoleArn': 'arn:aws:iam::572539092864:role/service-role/AmazonSageMaker-ExecutionRole-20200407T174741', 'FeatureGroupStatus': 'Created', 'OfflineStoreStatus': {'Status': 'Active'}, 'Description': 'Aggregated features for each credit card, streamed intraday', 'ResponseMetadata': {'RequestId': 'ffde0ee5-c746-40d9-acb3-84aa8b151289', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ffde0ee5-c746-40d9-acb3-84aa8b151289', 'content-type': 'application/x-amz-json-1.1', 'content-length': '1327', 'date': 'Thu, 22 Apr 2021 20:08:27 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "FEATURE_GROUP = 'cc-agg-batch-fg'\n",
    "\n",
    "response = sm_client.describe_feature_group(FeatureGroupName=FEATURE_GROUP)\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/572539092864/sagemaker/us-east-1/offline-store/cc-agg-batch-fg-1618489640/data/\n"
     ]
    }
   ],
   "source": [
    "# Lookup S3 Location of Offline Store\n",
    "\n",
    "offline_store_base_s3uri = response['OfflineStoreConfig']['S3StorageConfig']['S3Uri']\n",
    "offline_store_full_s3uri = (offline_store_base_s3uri + \n",
    "    '/572539092864/sagemaker/us-east-1/offline-store' +\n",
    "    '/cc-agg-batch-fg-1618489640' + \n",
    "    '/data/')\n",
    "\n",
    "print (offline_store_full_s3uri)\n",
    "\n",
    "# TODO: replace 's3' with 's3a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/572539092864/sagemaker/us-east-1/offline-store/cc-agg-batch-fg-1618489640/data/\n"
     ]
    }
   ],
   "source": [
    "# Load S3 location for Offline Store \n",
    "OFFLINE_STORE_URI = \"s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/572539092864/sagemaker/us-east-1/offline-store/cc-agg-batch-fg-1618489640/data/\"\n",
    "print (OFFLINE_STORE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tid: string (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- num_trans_last_1d: long (nullable = true)\n",
      " |-- avg_amt_last_1d: double (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- trans_time: double (nullable = true)\n",
      " |-- write_time: timestamp (nullable = true)\n",
      " |-- api_invocation_time: timestamp (nullable = true)\n",
      " |-- is_deleted: boolean (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read Offline Store data\n",
    "feature_store_df = spark.read.parquet(OFFLINE_STORE_URI)\n",
    "feature_store_df.printSchema()\n",
    "feature_store_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+\n",
      "|                 tid|          cc_num|num_trans_last_1d|avg_amt_last_1d|         event_time|   trans_time|          write_time|api_invocation_time|is_deleted|year|month|day|hour|\n",
      "+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+\n",
      "|8f3aac5ed6a43a632...|4006080197832643|               10|         164.94|2021-01-31 10:29:33|1.619115105E9|2021-04-22 18:16:...|2021-04-22 18:11:44|     false|2021|    4| 22|  18|\n",
      "|137c5d260d041c794...|4008569092490794|               11|         327.33|2021-01-31 23:32:13|1.619115105E9|2021-04-22 18:16:...|2021-04-22 18:11:45|     false|2021|    4| 22|  18|\n",
      "|4a4b1df6b77f84f4e...|4015965906982664|               21|        1014.54|2021-01-30 00:03:31|1.619115105E9|2021-04-22 18:16:...|2021-04-22 18:11:45|     false|2021|    4| 22|  18|\n",
      "|2cb37bc2940bfe3b9...|4026315850364369|               11|          210.4|2021-01-31 15:50:51|1.619115105E9|2021-04-22 18:16:...|2021-04-22 18:11:45|     false|2021|    4| 22|  18|\n",
      "|77d6bea33b69f1a70...|4029536382899161|                8|         712.76|2021-01-31 04:21:43|1.619115105E9|2021-04-22 18:16:...|2021-04-22 18:11:45|     false|2021|    4| 22|  18|\n",
      "+--------------------+----------------+-----------------+---------------+-------------------+-------------+--------------------+-------------------+----------+----+-----+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_store_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the raw transactons table to the aggregate feature table \n",
    "\n",
    "enhanced_df = (transactions_df.join(feature_store_df, transactions_df.tid == feature_store_df.tid, \"left_outer\")\n",
    "    .drop(transactions_df.tid)\n",
    "    .drop(transactions_df.cc_num)\n",
    "    .drop(transactions_df.num_trans_last_1d)\n",
    "    .drop(transactions_df.avg_amt_last_1d)\n",
    "    .drop(transactions_df.event_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: string (nullable = true)\n",
      " |-- fraud_label: string (nullable = true)\n",
      " |-- num_trans_last_60m: string (nullable = true)\n",
      " |-- avg_amt_last_60m: string (nullable = true)\n",
      " |-- amt_ratio1: string (nullable = true)\n",
      " |-- amt_ratio2: string (nullable = true)\n",
      " |-- count_ratio: string (nullable = true)\n",
      " |-- tid: string (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- num_trans_last_1d: long (nullable = true)\n",
      " |-- avg_amt_last_1d: double (nullable = true)\n",
      " |-- event_time: string (nullable = true)\n",
      " |-- trans_time: double (nullable = true)\n",
      " |-- write_time: timestamp (nullable = true)\n",
      " |-- api_invocation_time: timestamp (nullable = true)\n",
      " |-- is_deleted: boolean (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "enhanced_df.printSchema()\n",
    "enhanced_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early cutoff to prune extra data\n",
    "#import calendar\n",
    "\n",
    "# create cutoff time\n",
    "#cutoff_time = datetime.datetime(2021, 2, 1)\n",
    "#cutoff_cal = calendar.timegm(cutoff_time.timetuple())\n",
    "\n",
    "#print(f'Cutoff time: {cutoff_time}')\n",
    "#print(f'Cutoff cal:  {cutoff_cal}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Time Travel query from Studio\n",
    "\n",
    "##### Simplified query for datasets up to 100 GB\n",
    "\n",
    "SELECT *\n",
    "FROM\n",
    "    (SELECT *,\n",
    "         row_number()\n",
    "        OVER (PARTITION BY EventTime\n",
    "    ORDER BY  EventTime desc, Api_Invocation_Time DESC, write_time DESC) AS row_number\n",
    "    FROM sagemaker_featurestore.identity-feature-group-03-20-32-44-1614803787\n",
    "    where EventTime <= timestamp '<timestamp>')\n",
    "    -- replace timestamp '<timestamp>' with just <timestamp>  if EventTimeFeature is of type fractional\n",
    "WHERE row_number = 1 and\n",
    "NOT is_deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Query Dataframe that spans the intended time window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity data frame\n",
    "\n",
    "# add multiple instances for same cc_num\n",
    "#query_df = spark.createDataFrame([\n",
    "#      # instead of tid we use cc_num\n",
    "#      [cc_num_sample[0], \"2021-01-31T18:00:00Z\"], \n",
    "#      [cc_num_sample[1], \"2021-01-31T14:00:00Z\"],\n",
    "#      [cc_num_sample[2], \"2021-01-31T12:00:00Z\"],\n",
    "#      [cc_num_sample[3], \"2021-01-31T17:00:00Z\"],\n",
    "#      [cc_num_sample[4], \"2021-01-31T13:00:00Z\"]\n",
    "#    ],\n",
    "#    query_df_schema)\n",
    "\n",
    "#query_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Num samples in entity dataframe\n",
    "NUM_RANDOM_SAMPLES = 500\n",
    "\n",
    "cc_num_list = transactions_df.rdd.map(lambda x: x.cc_num).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_num_sample = random.sample(cc_num_list, NUM_RANDOM_SAMPLES)\n",
    "#print(cc_num_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build list of faked tuples of cc_num and timestamp\n",
    "start = datetime.datetime.strptime('2021-01-31 00:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "end = datetime.datetime.strptime('2021-01-31 23:00:00', '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "samples = list()\n",
    "for r in range(NUM_RANDOM_SAMPLES):\n",
    "    row = []\n",
    "    fake_timestamp = faker.date_time_between(start_date=start, end_date=end, tzinfo=None).strftime('%Y-%m-%d %H:00:00')\n",
    "    row.append(cc_num_sample[r])\n",
    "    row.append(fake_timestamp)\n",
    "    samples.append(row)\n",
    "    \n",
    "#print(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and show the query DF (e.g. entity_dataframe)\n",
    "\n",
    "query_df_schema = StructType([\n",
    "    # change to transactionid (tid)\n",
    "    StructField('cc_num', StringType(), False),\n",
    "    StructField('joindate', StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create entity data frame\n",
    "\n",
    "query_df = spark.createDataFrame(samples, query_df_schema)\n",
    "query_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Improvement: \n",
    "# Compute min and max times over our query data for filtering, in one pass for performance\n",
    "\n",
    "# query_df used to define bounded time window\n",
    "minmax_time = query_df.agg(sql_min(\"joindate\"), sql_max(\"joindate\")).collect()\n",
    "print(minmax_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_time, max_time = minmax_time[0][\"min(joindate)\"], minmax_time[0][\"max(joindate)\"]\n",
    "print(f'min_time: {min_time}')\n",
    "print(f'max_time: {max_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before filter, count: \" + str(enhanced_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter deleted records out\n",
    "#events_window = events_window.filter(~events_window.is_deleted)\n",
    "#print(\"After count: \" + str(events_window.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Filter out records from after query max_time and before staleness window prior to the min_time\n",
    "# This is a performance optimization; doing this prior to individual {shopper, query_time} filtering will be faster\n",
    "\n",
    "# Choose a window of time before which we want to ignore records\n",
    "allowed_staleness_days = 4\n",
    "\n",
    "# Eliminate CC's (customers) who do NOT have any relevant records within a near timeframe (4 days)\n",
    "filtered = enhanced_df.filter(\n",
    "    # datediff ( enddate, startdate ) - returns days\n",
    "    # we are actually removing items when event_time is MORE than 4 days before min_time (outside our buffer)\n",
    "    (datediff(lit(min_time), enhanced_df.event_time) <= allowed_staleness_days) &\n",
    "    (enhanced_df.event_time <= max_time)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.printSchema()\n",
    "print(\"After filter, count: \" + str(filtered.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered.select(\"cc_num\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join with query set; drop duplicate cc_num field\n",
    "\n",
    "joined = filtered.join(query_df, filtered.cc_num == query_df.cc_num, \"inner\").drop(query_df.cc_num)\n",
    "print(\"Joined count: \" + str(joined.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out data from after query time or before query time minus staleness window\n",
    "# this query removes events outside the time window FOR the SPECIFIC CC (customer)\n",
    "drop_future_and_stale = joined.filter(\n",
    "    (joined.event_time <= query_df.joindate)\n",
    "    & (datediff(query_df.joindate, joined.event_time) <= allowed_staleness_days)\n",
    ")\n",
    "print(\"After drop stale, count: \" + str(drop_future_and_stale.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_future_and_stale.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by id and take latest record\n",
    "take_latest = (\n",
    "    drop_future_and_stale.rdd.map(lambda x: (x.cc_num, x))  # to RDD with KVPs so we can use efficient reduceByKey\n",
    "    .reduceByKey(\n",
    "        lambda x, y: x if ((x.event_time) >= (y.event_time)) else y\n",
    "    )  # Use API invocation time as tie-breaker\n",
    "    .values()  # drop keys\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DF\n",
    "latest_df = take_latest.toDF(drop_future_and_stale.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop extra columns\n",
    "columns_to_drop = [\"write_time\", \"is_deleted\", \"year\", \"month\", \"day\", \"hour\", \"query_time\", \"api_invocation_time\"]\n",
    "selected = latest_df.drop(*columns_to_drop)\n",
    "print('Final selected count: ' + str(selected.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show query result\n",
    "\n",
    "selected.show(10)\n",
    "\n",
    "# To save query result to s3:\n",
    "# OUTPUT_PATH = f\"s3://{BUCKET}/{PREFIX}/test_query_output\"\n",
    "# selected.write.parquet(OUTPUT_PATH, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
