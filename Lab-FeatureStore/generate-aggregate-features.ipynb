{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Feature Store Notebook showing use of Time Travel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook will demonstrate how to build a point-in-time feature set. Techniques include building Spark Dataframes, and using joins and filters to reduce the dataset. Finally, Pyspark code is provided that will generate a query that pulls data within a pre-determined timeframe representing \"Time Travel\" using the SageMaker Feature Store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "\n",
    "role = get_execution_role()\n",
    "sm_client = boto3.Session().client(service_name='sagemaker')\n",
    "smfs_runtime = boto3.Session().client(service_name='sagemaker-featurestore-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by Deleting Feature Groups that we will re-create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo\n"
     ]
    }
   ],
   "source": [
    "# Use SageMaker default bucket\n",
    "BUCKET = sagemaker.Session().default_bucket()\n",
    "BASE_PREFIX = \"sagemaker-featurestore-demo\"\n",
    "\n",
    "OFFLINE_STORE_BASE_URI = f's3://{BUCKET}/{BASE_PREFIX}'\n",
    "# Note that FeatureStore will append this to prefix -> \"sagemaker/us-east-1/offline-store/\"\n",
    "\n",
    "print(OFFLINE_STORE_BASE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted batch fg\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sm_client.delete_feature_group(FeatureGroupName='cc-agg-batch-fg') \n",
    "    print('deleted batch fg')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted fg\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sm_client.delete_feature_group(FeatureGroupName='cc-agg-fg') # use if needed to re-create\n",
    "    print('deleted fg')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure those feature groups are not in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupSummaries': [{'FeatureGroupName': 'transaction-feature-group-03-20-32-44',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/transaction-feature-group-03-20-32-44',\n",
       "   'CreationTime': datetime.datetime(2021, 3, 3, 20, 36, 27, 700000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'identity-feature-group-03-20-32-44',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/identity-feature-group-03-20-32-44',\n",
       "   'CreationTime': datetime.datetime(2021, 3, 3, 20, 36, 27, 163000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}}],\n",
       " 'ResponseMetadata': {'RequestId': 'dd79f567-b9e2-404f-99b7-ec49ed7f738a',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'dd79f567-b9e2-404f-99b7-ec49ed7f738a',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '575',\n",
       "   'date': 'Wed, 14 Apr 2021 22:04:21 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.list_feature_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recreate the Feature Groups using Schema definition files\n",
    "Each feature group contains configuration parameters for Offline and Online stores. The feature group uses a schema definition file (JSON) that dictates the feature names and types. Below we display these local schema files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema files on in the local 'schema' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Aggregated features for each credit card, batch ingestion nightly\"\u001b[39;49;00m,\r\n",
      "    \u001b[94m\"features\"\u001b[39;49;00m: [\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"tid\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"string\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Transaction ID (Unique)\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"cc_num\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"bigint\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Credit Card Number (Unique)\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"num_trans_last_60m\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"bigint\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Aggregated Metric: Average number of transactions for the card aggregated by past 60 minutes\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"avg_amt_last_60m\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"double\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Aggregated Metric: Average transaction amount for the card aggregated by past 60 minutes\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"event_time\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"string\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Required feature for event timestamp\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"trans_time\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"double\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Required feature for event timestamp\"\u001b[39;49;00m\r\n",
      "          }\r\n",
      "      ],\r\n",
      "    \r\n",
      "      \u001b[94m\"record_identifier_feature_name\"\u001b[39;49;00m: \u001b[33m\"cc_num\"\u001b[39;49;00m,\r\n",
      "      \u001b[94m\"event_time_feature_name\"\u001b[39;49;00m: \u001b[33m\"trans_time\"\u001b[39;49;00m,\r\n",
      "      \u001b[94m\"tags\"\u001b[39;49;00m: [{\u001b[94m\"Key\"\u001b[39;49;00m: \u001b[33m\"Environment\"\u001b[39;49;00m, \u001b[94m\"Value\"\u001b[39;49;00m : \u001b[33m\"DEV\"\u001b[39;49;00m}, \r\n",
      "               {\u001b[94m\"Key\"\u001b[39;49;00m: \u001b[33m\"IngestionType\"\u001b[39;49;00m, \u001b[94m\"Value\"\u001b[39;49;00m: \u001b[33m\"Streaming\"\u001b[39;49;00m},\r\n",
      "               {\u001b[94m\"Key\"\u001b[39;49;00m: \u001b[33m\"CostCenter\"\u001b[39;49;00m, \u001b[94m\"Value\"\u001b[39;49;00m: \u001b[33m\"C20\"\u001b[39;49;00m}]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize schema/cc-agg-fg-schema.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Aggregated features for each credit card, streamed intraday\"\u001b[39;49;00m,\r\n",
      "    \u001b[94m\"features\"\u001b[39;49;00m: [\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"tid\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"string\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Transaction ID (Unique)\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"cc_num\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"bigint\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Credit Card Number (Unique)\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"num_trans_last_1d\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"bigint\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Aggregated Metric: Average number of transactions for the card aggregated by past 1 day\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"avg_amt_last_1d\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"double\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Aggregated Metric: Average transaction amount for the card aggregated by past 1 day\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"event_time\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"string\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Externally generated event timestamp\"\u001b[39;49;00m\r\n",
      "          },\r\n",
      "          {\r\n",
      "              \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"trans_time\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"type\"\u001b[39;49;00m: \u001b[33m\"double\"\u001b[39;49;00m,\r\n",
      "              \u001b[94m\"description\"\u001b[39;49;00m: \u001b[33m\"Required feature for transaction timestamp\"\u001b[39;49;00m\r\n",
      "          }\r\n",
      "      ],\r\n",
      "    \r\n",
      "      \u001b[94m\"record_identifier_feature_name\"\u001b[39;49;00m: \u001b[33m\"cc_num\"\u001b[39;49;00m,\r\n",
      "      \u001b[94m\"event_time_feature_name\"\u001b[39;49;00m: \u001b[33m\"trans_time\"\u001b[39;49;00m,\r\n",
      "      \u001b[94m\"tags\"\u001b[39;49;00m: [{\u001b[94m\"Key\"\u001b[39;49;00m: \u001b[33m\"Environment\"\u001b[39;49;00m, \u001b[94m\"Value\"\u001b[39;49;00m : \u001b[33m\"DEV\"\u001b[39;49;00m}, \r\n",
      "               {\u001b[94m\"Key\"\u001b[39;49;00m: \u001b[33m\"IngestionType\"\u001b[39;49;00m, \u001b[94m\"Value\"\u001b[39;49;00m: \u001b[33m\"Batch\"\u001b[39;49;00m},\r\n",
      "               {\u001b[94m\"Key\"\u001b[39;49;00m: \u001b[33m\"CostCenter\"\u001b[39;49;00m, \u001b[94m\"Value\"\u001b[39;49;00m: \u001b[33m\"C18\"\u001b[39;49;00m}]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize schema/cc-agg-batch-fg-schema.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema_to_defs(filename):\n",
    "    schema = json.loads(open(filename).read())\n",
    "    \n",
    "    feature_definitions = []\n",
    "    \n",
    "    for col in schema['Features']:\n",
    "        feature = {'FeatureName': col['name']}\n",
    "        if col['type'] == 'double':\n",
    "            feature['FeatureType'] = 'Fractional'\n",
    "        elif col['type'] == 'bigint':\n",
    "            feature['FeatureType'] = 'Integral'\n",
    "        else:\n",
    "            feature['FeatureType'] = 'String'\n",
    "        feature_definitions.append(feature)\n",
    "\n",
    "    return feature_definitions, schema['record_identifier_feature_name'], schema['event_time_feature_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_group_from_schema(filename, fg_name, role_arn=None, s3_uri=None):\n",
    "    schema = json.loads(open(filename).read())\n",
    "    \n",
    "    feature_defs = []\n",
    "    \n",
    "    for col in schema['features']:\n",
    "        feature = {'FeatureName': col['name']}\n",
    "        if col['type'] == 'double':\n",
    "            feature['FeatureType'] = 'Fractional'\n",
    "        elif col['type'] == 'bigint':\n",
    "            feature['FeatureType'] = 'Integral'\n",
    "        else:\n",
    "            feature['FeatureType'] = 'String'\n",
    "        feature_defs.append(feature)\n",
    "\n",
    "    record_identifier_name = schema['record_identifier_feature_name']\n",
    "    event_time_name = schema['event_time_feature_name']\n",
    "\n",
    "    if role_arn is None:\n",
    "        role_arn = get_execution_role()\n",
    "\n",
    "    if s3_uri is None:\n",
    "        offline_config = {}\n",
    "    else:\n",
    "        print(f'Creating Offline Store at: {s3_uri}')\n",
    "        offline_config = {'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': s3_uri}}}\n",
    "        \n",
    "    sm_client.create_feature_group(\n",
    "        FeatureGroupName = fg_name,\n",
    "        RecordIdentifierFeatureName = record_identifier_name,\n",
    "        EventTimeFeatureName = event_time_name,\n",
    "        FeatureDefinitions = feature_defs,\n",
    "        Description = schema['description'],\n",
    "        Tags = schema['tags'],\n",
    "        OnlineStoreConfig = {'EnableOnlineStore': True},\n",
    "        RoleArn = role_arn,\n",
    "        **offline_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the new Feature Groups using the schema definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Offline Store at: s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo\n"
     ]
    }
   ],
   "source": [
    "create_feature_group_from_schema('schema/cc-agg-fg-schema.json', 'cc-agg-fg', \n",
    "                                 role_arn=role, s3_uri=OFFLINE_STORE_BASE_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Offline Store at: s3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo\n"
     ]
    }
   ],
   "source": [
    "create_feature_group_from_schema('schema/cc-agg-batch-fg-schema.json', 'cc-agg-batch-fg', \n",
    "                                 role_arn=role, s3_uri=OFFLINE_STORE_BASE_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure the new Feature Groups exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupSummaries': [{'FeatureGroupName': 'transaction-feature-group-03-20-32-44',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/transaction-feature-group-03-20-32-44',\n",
       "   'CreationTime': datetime.datetime(2021, 3, 3, 20, 36, 27, 700000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'identity-feature-group-03-20-32-44',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/identity-feature-group-03-20-32-44',\n",
       "   'CreationTime': datetime.datetime(2021, 3, 3, 20, 36, 27, 163000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Created',\n",
       "   'OfflineStoreStatus': {'Status': 'Active'}},\n",
       "  {'FeatureGroupName': 'cc-agg-fg',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/cc-agg-fg',\n",
       "   'CreationTime': datetime.datetime(2021, 4, 14, 22, 4, 56, 422000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Creating'},\n",
       "  {'FeatureGroupName': 'cc-agg-batch-fg',\n",
       "   'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/cc-agg-batch-fg',\n",
       "   'CreationTime': datetime.datetime(2021, 4, 14, 22, 4, 57, 528000, tzinfo=tzlocal()),\n",
       "   'FeatureGroupStatus': 'Creating'}],\n",
       " 'ResponseMetadata': {'RequestId': '8c5e9d8c-56b2-4563-848d-d5bcbf9d244b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '8c5e9d8c-56b2-4563-848d-d5bcbf9d244b',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '951',\n",
       "   'date': 'Wed, 14 Apr 2021 22:04:58 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.list_feature_groups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Describe each feature group\n",
    "Note that each feature group gets its own ARN, allowing you to manage IAM policies that control access to individual feature groups. The feature names and types are displayed, and the record identifier and event time features are called out specifically. Notice that there is only an `OnlineStoreConfig` and no `OfflineStoreConfig`, as we have decided not to replicate features offline for these groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/cc-agg-fg',\n",
       " 'FeatureGroupName': 'cc-agg-fg',\n",
       " 'RecordIdentifierFeatureName': 'cc_num',\n",
       " 'EventTimeFeatureName': 'trans_time',\n",
       " 'FeatureDefinitions': [{'FeatureName': 'tid', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'cc_num', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'num_trans_last_60m', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'avg_amt_last_60m', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'event_time', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'trans_time', 'FeatureType': 'Fractional'}],\n",
       " 'CreationTime': datetime.datetime(2021, 4, 14, 22, 4, 56, 422000, tzinfo=tzlocal()),\n",
       " 'OnlineStoreConfig': {'EnableOnlineStore': True},\n",
       " 'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': 's3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo'},\n",
       "  'DisableGlueTableCreation': False},\n",
       " 'RoleArn': 'arn:aws:iam::572539092864:role/service-role/AmazonSageMaker-ExecutionRole-20200407T174741',\n",
       " 'FeatureGroupStatus': 'Creating',\n",
       " 'Description': 'Aggregated features for each credit card, batch ingestion nightly',\n",
       " 'ResponseMetadata': {'RequestId': 'eb00c6a3-9788-4921-87ad-5273722982e7',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'eb00c6a3-9788-4921-87ad-5273722982e7',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1152',\n",
       "   'date': 'Wed, 14 Apr 2021 22:05:04 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.describe_feature_group(FeatureGroupName='cc-agg-fg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FeatureGroupArn': 'arn:aws:sagemaker:us-east-1:572539092864:feature-group/cc-agg-batch-fg',\n",
       " 'FeatureGroupName': 'cc-agg-batch-fg',\n",
       " 'RecordIdentifierFeatureName': 'cc_num',\n",
       " 'EventTimeFeatureName': 'trans_time',\n",
       " 'FeatureDefinitions': [{'FeatureName': 'tid', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'cc_num', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'num_trans_last_1d', 'FeatureType': 'Integral'},\n",
       "  {'FeatureName': 'avg_amt_last_1d', 'FeatureType': 'Fractional'},\n",
       "  {'FeatureName': 'event_time', 'FeatureType': 'String'},\n",
       "  {'FeatureName': 'trans_time', 'FeatureType': 'Fractional'}],\n",
       " 'CreationTime': datetime.datetime(2021, 4, 14, 22, 4, 57, 528000, tzinfo=tzlocal()),\n",
       " 'OnlineStoreConfig': {'EnableOnlineStore': True},\n",
       " 'OfflineStoreConfig': {'S3StorageConfig': {'S3Uri': 's3://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo'},\n",
       "  'DisableGlueTableCreation': False,\n",
       "  'DataCatalogConfig': {'TableName': 'cc-agg-batch-fg-1618437897',\n",
       "   'Catalog': 'AwsDataCatalog',\n",
       "   'Database': 'sagemaker_featurestore'}},\n",
       " 'RoleArn': 'arn:aws:iam::572539092864:role/service-role/AmazonSageMaker-ExecutionRole-20200407T174741',\n",
       " 'FeatureGroupStatus': 'Creating',\n",
       " 'Description': 'Aggregated features for each credit card, streamed intraday',\n",
       " 'ResponseMetadata': {'RequestId': '83f0896a-4c5c-4766-a348-f7d19816cfd0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '83f0896a-4c5c-4766-a348-f7d19816cfd0',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1288',\n",
       "   'date': 'Wed, 14 Apr 2021 22:05:08 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.describe_feature_group(FeatureGroupName='cc-agg-batch-fg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Batch Ingestion\n",
    "**This section of the notebook aggregates raw features into new derived features that is used for Fraud Detection model training/inference.**\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Create PySpark Processing Script](#Create-PySpark-Processing-Script)\n",
    "1. [Run SageMaker Processing Job](#Run-SageMaker-Processing-Job)\n",
    "1. [Explore Aggregated Features](#Explore-Aggregated-Features)\n",
    "1. [Validate Feature Group for Records](#Validate-Feature-Group-for-Records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Background\n",
    "\n",
    "- This notebook takes raw credit card transactions data (csv) generated by \n",
    "[notebook 0](./0_prepare_transactions_dataset.ipynb) and aggregates the raw features to create new features (ratios) via <b>SageMaker Processing</b> PySpark Job. These aggregated features alongside the raw original features will be leveraged in the training phase of a Credit Card Fraud Detection model in the next step (see notebook [notebook 3](./3_train_and_deploy_model.ipynb)).\n",
    "\n",
    "- As part of the Spark job, we also select the latest daily aggregated features - `num_trans_last_1d` and `avg_amt_last_1d` grouped by `cc_num` (credit card number) and populate these features into the <b>SageMaker Online Feature Store</b> as a feature group. This feature group (`cc-agg-batch-fg`) was created in notebook [notebook 1](./1_setup.ipynb).\n",
    "\n",
    "- [Amazon SageMaker Processing](https://aws.amazon.com/about-aws/whats-new/2020/09/amazon-sagemaker-processing-now-supports-built-in-spark-containers-for-big-data-processing/) lets customers run analytics jobs for data engineering and model evaluation on Amazon SageMaker easily and at scale. It provides a fully managed Spark environment for data processing or feature engineering workloads.\n",
    "\n",
    "<img src=\"./images/batch_ingestion.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import logging\n",
    "import random\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SageMaker version: 2.29.1\n"
     ]
    }
   ],
   "source": [
    "print(f'Using SageMaker version: {sagemaker.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Batch Aggregation using SageMaker PySpark Processing Job]\n"
     ]
    }
   ],
   "source": [
    "logger.info('[Batch Aggregation using SageMaker PySpark Processing Job]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-featurestore-demo/raw\n",
      "sagemaker-featurestore-demo/aggregated\n"
     ]
    }
   ],
   "source": [
    "# Setup S3 prefixes for Spark Job\n",
    "\n",
    "INPUT_KEY_PREFIX = os.path.join(BASE_PREFIX, 'raw')\n",
    "OUTPUT_KEY_PREFIX = os.path.join(BASE_PREFIX, 'aggregated')\n",
    "LOCAL_DIR = './data'\n",
    "\n",
    "print(INPUT_KEY_PREFIX)\n",
    "print(OUTPUT_KEY_PREFIX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Create PySpark Script\n",
    "This PySpark script does the following:\n",
    "\n",
    "1. Aggregates raw features to derive new features (ratios).\n",
    "2. Saves the aggregated features alongside the original raw features into a CSV file and writes it to S3 - will be used in the next step for model training.\n",
    "3. Groups the aggregated features by credit card number and picks selected aggregated features to write to SageMaker Feature Store (Online). <br>\n",
    "<b>Note: </b> The feature group was created in the previous notebook (`1_setup.ipynb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch_aggregation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch_aggregation.py\n",
    "from pyspark.sql.types import StructField, StructType, StringType, DoubleType, TimestampType, LongType\n",
    "from pyspark.sql.functions import desc, dense_rank\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from  argparse import Namespace, ArgumentParser\n",
    "from pyspark.sql.window import Window\n",
    "import argparse\n",
    "import logging\n",
    "import boto3\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Modified unique user count -pmh\n",
    "##TOTAL_UNIQUE_USERS = 1000 \n",
    "FEATURE_GROUP = 'cc-agg-batch-fg'\n",
    "\n",
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "feature_store_client = boto3.client(service_name='sagemaker-featurestore-runtime')\n",
    "\n",
    "\n",
    "def parse_args() -> Namespace:\n",
    "    parser = ArgumentParser(description='Spark Job Input and Output Args')\n",
    "    parser.add_argument('--s3_input_bucket', type=str, help='S3 Input Bucket')\n",
    "    parser.add_argument('--s3_input_key_prefix', type=str, help='S3 Input Key Prefix')\n",
    "    parser.add_argument('--s3_output_bucket', type=str, help='S3 Output Bucket')\n",
    "    parser.add_argument('--s3_output_key_prefix', type=str, help='S3 Output Key Prefix')\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "    \n",
    "\n",
    "def define_schema() -> StructType:\n",
    "    # Added 'tid' -pmh\n",
    "    schema = StructType([StructField('tid', StringType(), True),\n",
    "                         StructField('event_time', TimestampType(), True),\n",
    "                         StructField('cc_num', LongType(), True),\n",
    "                         StructField('amount', DoubleType(), True),\n",
    "                         StructField('fraud_label', StringType(), True)])\n",
    "    return schema\n",
    "\n",
    "\n",
    "def aggregate_features(args: Namespace, schema: StructType, spark: SparkSession) -> DataFrame:\n",
    "    logger.info('[Read Raw Transactions Data as Spark DataFrame]')\n",
    "    transactions_df = spark.read.csv(f's3a://{os.path.join(args.s3_input_bucket, args.s3_input_key_prefix)}', \\\n",
    "                                     header=False, \\\n",
    "                                     schema=schema)\n",
    "    \n",
    "    logger.info('[Aggregate Transactions to Derive New Features using Spark SQL]')\n",
    "    query = \"\"\"\n",
    "    SELECT *, \\\n",
    "           avg_amt_last_60m/avg_amt_last_1d AS amt_ratio1, \\\n",
    "           amount/avg_amt_last_1d AS amt_ratio2, \\\n",
    "           num_trans_last_60m/num_trans_last_1d AS count_ratio \\\n",
    "    FROM \\\n",
    "        ( \\\n",
    "        SELECT *, \\\n",
    "               COUNT(*) OVER w1 as num_trans_last_60m, \\\n",
    "               AVG(amount) OVER w1 as avg_amt_last_60m, \\\n",
    "               COUNT(*) OVER w2 as num_trans_last_1d, \\\n",
    "               AVG(amount) OVER w2 as avg_amt_last_1d \\\n",
    "        FROM transactions_df \\\n",
    "        WINDOW \\\n",
    "               w1 AS (PARTITION BY cc_num order by cast(event_time AS timestamp) RANGE INTERVAL 60 MINUTES PRECEDING), \\\n",
    "               w2 AS (PARTITION BY cc_num order by cast(event_time AS timestamp) RANGE INTERVAL 24 HOURS PRECEDING) \\\n",
    "        ) \n",
    "    \"\"\"\n",
    "    transactions_df.registerTempTable('transactions_df')\n",
    "    aggregated_features = spark.sql(query)\n",
    "    return aggregated_features\n",
    "\n",
    "\n",
    "def write_to_s3(args: Namespace, aggregated_features: DataFrame) -> None:\n",
    "    logger.info('[Write Aggregated Features to S3]')\n",
    "    aggregated_features.coalesce(1) \\\n",
    "                       .write.format('com.databricks.spark.csv') \\\n",
    "                       .option('header', True) \\\n",
    "                       .mode('overwrite') \\\n",
    "                       .option('sep', ',') \\\n",
    "                       .save('s3a://' + os.path.join(args.s3_output_bucket, args.s3_output_key_prefix))\n",
    "    \n",
    "def group_by_card_number(aggregated_features: DataFrame) -> DataFrame: \n",
    "    logger.info('[Group Aggregated Features by Card Number]')\n",
    "    window = Window.partitionBy('cc_num').orderBy(desc('event_time'))\n",
    "    sorted_df = aggregated_features.withColumn('rank', dense_rank().over(window))\n",
    "    grouped_df = sorted_df.filter(sorted_df.rank == 1).drop(sorted_df.rank)\n",
    "    # Added 'tid' and 'event_time' features -pmh\n",
    "    sliced_df = grouped_df.select('tid', 'cc_num', 'num_trans_last_1d', 'avg_amt_last_1d', 'event_time')\n",
    "    #sliced_df = grouped_df.select('cc_num', 'num_trans_last_1d', 'avg_amt_last_1d')\n",
    "    return sliced_df\n",
    "\n",
    "\n",
    "def transform_row(sliced_df: DataFrame) -> list:\n",
    "    logger.info('[Transform Spark DataFrame Row to SageMaker Feature Store Record]')\n",
    "    records = []\n",
    "    for row in sliced_df.rdd.collect():\n",
    "        record = []\n",
    "        # added tid -pmh\n",
    "        tid, cc_num, num_trans_last_1d, avg_amt_last_1d, event_time = row\n",
    "        if cc_num:\n",
    "            # Added 'tid' -pmh\n",
    "            record.append({'ValueAsString': str(tid), 'FeatureName': 'tid'})\n",
    "            record.append({'ValueAsString': str(cc_num), 'FeatureName': 'cc_num'})\n",
    "            record.append({'ValueAsString': str(num_trans_last_1d), 'FeatureName': 'num_trans_last_1d'})\n",
    "            record.append({'ValueAsString': str(round(avg_amt_last_1d, 2)), 'FeatureName': 'avg_amt_last_1d'})\n",
    "            # Added 'event_time' -pmh\n",
    "            record.append({'ValueAsString': str(event_time), 'FeatureName': 'event_time'})\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "\n",
    "def write_to_feature_store(records: list) -> None:\n",
    "    logger.info('[Write Grouped Features to SageMaker Online Feature Store]')\n",
    "    success, fail = 0, 0\n",
    "    for record in records:\n",
    "        event_time_feature = {\n",
    "                'FeatureName': 'trans_time',\n",
    "                'ValueAsString': str(int(round(time.time())))\n",
    "            }\n",
    "        record.append(event_time_feature)\n",
    "        response = feature_store_client.put_record(FeatureGroupName=FEATURE_GROUP, Record=record)\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            success += 1\n",
    "        else:\n",
    "            fail += 1\n",
    "    logger.info('Success = {}'.format(success))\n",
    "    logger.info('Fail = {}'.format(fail))\n",
    "    ##assert success == TOTAL_UNIQUE_USERS\n",
    "    assert fail == 0\n",
    "\n",
    "\n",
    "def run_spark_job():\n",
    "    spark = SparkSession.builder.appName('PySparkJob').getOrCreate()\n",
    "    args = parse_args()\n",
    "    schema = define_schema()\n",
    "    aggregated_features = aggregate_features(args, schema, spark)\n",
    "    write_to_s3(args, aggregated_features)\n",
    "    sliced_df = group_by_card_number(aggregated_features)\n",
    "    records = transform_row(sliced_df)\n",
    "    write_to_feature_store(records)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run_spark_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Run SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "spark_processor = PySparkProcessor(base_job_name='sagemaker-processing', \n",
    "                                   framework_version='2.4', # spark version\n",
    "                                   role=role, \n",
    "                                   instance_count=1, \n",
    "                                   instance_type='ml.r5.4xlarge', \n",
    "                                   env={'AWS_DEFAULT_REGION': boto3.Session().region_name},\n",
    "                                   max_runtime_in_seconds=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating processing-job with name sagemaker-processing-2021-04-14-22-05-55-763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-processing-2021-04-14-22-05-55-763\n",
      "Inputs:  [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-572539092864/sagemaker-processing-2021-04-14-22-05-55-763/input/code/batch_aggregation.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-572539092864/logs', 'LocalPath': '/opt/ml/processing/spark-events/', 'S3UploadMode': 'Continuous'}}]\n",
      ".............................\u001b[34m04-14 22:10 smspark.cli  INFO     Parsing arguments. argv: ['/usr/local/bin/smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/batch_aggregation.py', '--s3_input_bucket', 'sagemaker-us-east-1-572539092864', '--s3_input_key_prefix', 'sagemaker-featurestore-demo/raw', '--s3_output_bucket', 'sagemaker-us-east-1-572539092864', '--s3_output_key_prefix', 'sagemaker-featurestore-demo/aggregated']\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark.cli  INFO     Raw spark options before processing: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark.cli  INFO     App and app arguments: ['/opt/ml/processing/input/code/batch_aggregation.py', '--s3_input_bucket', 'sagemaker-us-east-1-572539092864', '--s3_input_key_prefix', 'sagemaker-featurestore-demo/raw', '--s3_output_bucket', 'sagemaker-us-east-1-572539092864', '--s3_output_key_prefix', 'sagemaker-featurestore-demo/aggregated']\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark.cli  INFO     Rendered spark options: {'class_': None, 'jars': None, 'py_files': None, 'files': None, 'verbose': False}\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark.cli  INFO     Initializing processing job.\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     {'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:572539092864:processing-job/sagemaker-processing-2021-04-14-22-05-55-763', 'ProcessingJobName': 'sagemaker-processing-2021-04-14-22-05-55-763', 'Environment': {'AWS_DEFAULT_REGION': 'us-east-1'}, 'AppSpecification': {'ImageUri': '173754725891.dkr.ecr.us-east-1.amazonaws.com/sagemaker-spark-processing:2.4-cpu', 'ContainerEntrypoint': ['smspark-submit', '--local-spark-event-logs-dir', '/opt/ml/processing/spark-events/', '/opt/ml/processing/input/code/batch_aggregation.py'], 'ContainerArguments': ['--s3_input_bucket', 'sagemaker-us-east-1-572539092864', '--s3_input_key_prefix', 'sagemaker-featurestore-demo/raw', '--s3_output_bucket', 'sagemaker-us-east-1-572539092864', '--s3_output_key_prefix', 'sagemaker-featurestore-demo/aggregated']}, 'ProcessingInputs': [{'InputName': 'code', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/code', 'S3Uri': 's3://sagemaker-us-east-1-572539092864/sagemaker-processing-2021-04-14-22-05-55-763/input/code/batch_aggregation.py', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/spark-events/', 'S3Uri': 's3://sagemaker-us-east-1-572539092864/logs', 'S3UploadMode': 'Continuous'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.r5.4xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::572539092864:role/service-role/AmazonSageMaker-ExecutionRole-20200407T174741', 'StoppingCondition': {'MaxRuntimeInSeconds': 1200}}\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark.cli  INFO     running spark submit command: spark-submit --master yarn --deploy-mode client /opt/ml/processing/input/code/batch_aggregation.py --s3_input_bucket sagemaker-us-east-1-572539092864 --s3_input_key_prefix sagemaker-featurestore-demo/raw --s3_output_bucket sagemaker-us-east-1-572539092864 --s3_output_key_prefix sagemaker-featurestore-demo/aggregated\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     waiting for hosts\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     starting status server\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     Status server listening on algo-1:5555\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     bootstrapping cluster\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     transitioning from status INITIALIZING to BOOTSTRAPPING\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     copying aws jars\u001b[0m\n",
      "\u001b[34mServing on http://algo-1:5555\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     Found hadoop jar hadoop-aws-2.10.0-amzn-0.jar\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     Copying optional jar jets3t-0.9.0.jar from /usr/lib/hadoop/lib to /usr/lib/spark/jars\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     copying cluster config\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     copying /opt/hadoop-config/hdfs-site.xml to /usr/lib/hadoop/etc/hadoop/hdfs-site.xml\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     copying /opt/hadoop-config/core-site.xml to /usr/lib/hadoop/etc/hadoop/core-site.xml\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     copying /opt/hadoop-config/yarn-site.xml to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     copying /opt/hadoop-config/spark-defaults.conf to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     copying /opt/hadoop-config/spark-env.sh to /usr/lib/spark/conf/spark-env.sh\u001b[0m\n",
      "\u001b[34m04-14 22:10 root         INFO     Detected instance type: r5.4xlarge with total memory: 131072M and total cores: 16\u001b[0m\n",
      "\u001b[34m04-14 22:10 root         INFO     Writing default config to /usr/lib/hadoop/etc/hadoop/yarn-site.xml\u001b[0m\n",
      "\u001b[34m04-14 22:10 root         INFO     Configuration at /usr/lib/hadoop/etc/hadoop/yarn-site.xml is: \u001b[0m\n",
      "\u001b[34m<?xml version=\"1.0\"?>\u001b[0m\n",
      "\u001b[34m<!-- Site specific YARN configuration properties -->\n",
      " <configuration>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.hostname</name>\n",
      "         <value>10.2.212.241</value>\n",
      "         <description>The hostname of the RM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.hostname</name>\n",
      "         <value>algo-1</value>\n",
      "         <description>The hostname of the NM.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.webapp.address</name>\n",
      "         <value>algo-1:8042</value>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.vmem-pmem-ratio</name>\n",
      "         <value>5</value>\n",
      "         <description>Ratio between virtual memory to physical memory.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.resourcemanager.am.max-attempts</name>\n",
      "         <value>1</value>\n",
      "         <description>The maximum number of application attempts.</description>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>yarn.nodemanager.env-whitelist</name>\n",
      "         <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,YARN_HOME,AWS_CONTAINER_CREDENTIALS_RELATIVE_URI</value>\n",
      "         <description>Environment variable whitelist</description>\n",
      "     </property>\n",
      "\n",
      " \n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-mb</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-mb</name>\n",
      "    <value>127139</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.minimum-allocation-vcores</name>\n",
      "    <value>1</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.scheduler.maximum-allocation-vcores</name>\n",
      "    <value>16</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.memory-mb</name>\n",
      "    <value>127139</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.resource.cpu-vcores</name>\n",
      "    <value>16</value>\n",
      "  </property>\u001b[0m\n",
      "\u001b[34m</configuration>\n",
      "\u001b[0m\n",
      "\u001b[34m04-14 22:10 root         INFO     Writing default config to /usr/lib/spark/conf/spark-defaults.conf\u001b[0m\n",
      "\u001b[34m04-14 22:10 root         INFO     Configuration at /usr/lib/spark/conf/spark-defaults.conf is: \u001b[0m\n",
      "\u001b[34mspark.driver.extraClassPath      /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.driver.extraLibraryPath    /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.executor.extraClassPath    /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\u001b[0m\n",
      "\u001b[34mspark.executor.extraLibraryPath  /usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\u001b[0m\n",
      "\u001b[34mspark.driver.host=10.2.212.241\u001b[0m\n",
      "\u001b[34mspark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2\u001b[0m\n",
      "\u001b[34mspark.driver.memory 2048m\u001b[0m\n",
      "\u001b[34mspark.driver.memoryOverhead 204m\u001b[0m\n",
      "\u001b[34mspark.driver.defaultJavaOptions -XX:OnOutOfMemoryError='kill -9 %p' -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled\u001b[0m\n",
      "\u001b[34mspark.executor.memory 113533m\u001b[0m\n",
      "\u001b[34mspark.executor.memoryOverhead 11353m\u001b[0m\n",
      "\u001b[34mspark.executor.cores 16\u001b[0m\n",
      "\u001b[34mspark.executor.defaultJavaOptions -verbose:gc -XX:OnOutOfMemoryError='kill -9 %p' -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseParallelGC -XX:InitiatingHeapOccupancyPercent=70 -XX:ConcGCThreads=4 -XX:ParallelGCThreads=12 \u001b[0m\n",
      "\u001b[34mspark.executor.instances 1\u001b[0m\n",
      "\u001b[34mspark.default.parallelism 32\n",
      "\u001b[0m\n",
      "\u001b[34m04-14 22:10 root         INFO     Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m04-14 22:10 root         INFO     No file at /opt/ml/processing/input/conf/configuration.json exists, skipping user configuration\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.212.241\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annota\u001b[0m\n",
      "\u001b[34mtions-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-8223e0bb-23d0-4123-a502-d032e128ef7a\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Apr 14 22:10:33\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: 2.0% max memory 958.5 MB = 19.2 MB\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSNamesystem: Append Enabled: true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: 1.0% max memory 958.5 MB = 9.6 MB\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: 0.25% max memory 958.5 MB = 2.4 MB\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: 0.029999999329447746% max memory 958.5 MB = 294.5 KB\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO util.GSet: capacity      = 2^15 = 32768 entries\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSImage: Allocated new BlockPoolId: BP-592339435-10.2.212.241-1618438233732\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 323 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid = 0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:33 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.212.241\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     waiting for cluster to be up\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO resourcemanager.ResourceManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting ResourceManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.212.241\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1\u001b[0m\n",
      "\u001b[34m.9.13.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/etc/hadoop/rm-config/log4j.properties:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.8.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.212.241\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annota\u001b[0m\n",
      "\u001b[34mtions-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO resourcemanager.ResourceManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO nodemanager.NodeManager: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NodeManager\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.212.241\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1\u001b[0m\n",
      "\u001b[34m.9.13.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/etc/hadoop/nm-config/log4j.properties:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-coprocessor-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/hadoop-yarn-server-timelineservice-hbase-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-client-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/htrace-core-3.1.0-incubating.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/metrics-core-2.2.0.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-common-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/joni-2.1.2.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jsr311-api-1.1.1.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/jcodings-1.0.8.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-annotations-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/hbase-protocol-1.2.6.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-yarn/.//timelineservice/lib/commons-csv-1.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO nodemanager.NodeManager: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO datanode.DataNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting DataNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.212.241\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = []\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 2.10.0-amzn-0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/lib/hadoop/etc/hadoop:/usr/lib/hadoop/lib/activation-1.1.jar:/usr/lib/hadoop/lib/commons-net-3.1.jar:/usr/lib/hadoop/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop/lib/json-smart-1.3.1.jar:/usr/lib/hadoop/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop/lib/commons-codec-1.4.jar:/usr/lib/hadoop/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop/lib/asm-3.2.jar:/usr/lib/hadoop/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop/lib/jsch-0.1.54.jar:/usr/lib/hadoop/lib/jersey-server-1.9.jar:/usr/lib/hadoop/lib/servlet-api-2.5.jar:/usr/lib/hadoop/lib/commons-lang-2.6.jar:/usr/lib/hadoop/lib/commons-cli-1.2.jar:/usr/lib/hadoop/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop/lib/jets3t-0.9.0.jar:/usr/lib/hadoop/lib/curator-client-2.7.1.jar:/usr/lib/hadoop/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop/lib/commons-digester-1.8.jar:/usr/lib/hadoop/lib/junit-4.11.jar:/usr/lib/hadoop/lib/log4j-1.2.17.jar:/usr/lib/hadoop/lib/mockito-all-1.8.5.jar:/usr/lib/hadoop/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop/lib/commons-io-2.4.jar:/usr/lib/hadoop/lib/slf4j-log4j12-1.7.25.jar:/usr/lib/hadoop/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop/lib/jettison-1.1.jar:/usr/lib/hadoop/lib/slf4j-api-1.7.25.jar:/usr/lib/hadoop/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop/lib/avro-1.7.7.jar:/usr/lib/hadoop/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop/lib/guava-11.0.2.jar:/usr/lib/hadoop/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop/lib/paranamer-2.3.jar:/usr/lib/hadoop/lib/jsp-api-2.1.jar:/usr/lib/hadoop/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop/lib/commons-compress-1.19.jar:/usr/lib/hadoop/lib/commons-lang3-3.4.jar:/usr/lib/hadoop/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop/lib/jersey-core-1.9.jar:/usr/lib/hadoop/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop/lib/jersey-json-1.9.jar:/usr/lib/hadoop/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop/lib/httpcore-4.4.11.jar:/usr/lib/hadoop/lib/httpclient-4.5.9.jar:/usr/lib/hadoop/lib/gson-2.2.4.jar:/usr/lib/hadoop/lib/stax-api-1.0-2.jar:/usr/lib/hadoop/lib/xmlenc-0.52.jar:/usr/lib/hadoop/lib/jsr305-3.0.0.jar:/usr/lib/hadoop/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop/lib/commons-configuration-1.6.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop/.//hadoop-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archives.jar:/usr/lib/hadoop/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-datalake.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop/.//hadoop-extras.jar:/usr/lib/hadoop/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator.jar:/usr/lib/hadoop/.//hadoop-auth.jar:/usr/lib/hadoop/.//hadoop-archive-logs.jar:/usr/lib/hadoop/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop/.//hadoop-annotations-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure.jar:/usr/lib/hadoop/.//hadoop-common-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-annotations.jar:/usr/lib/hadoop/.//hadoop-aliyun.jar:/usr/lib/hadoop/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-nfs.jar:/usr/lib/hadoop/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop/.//hadoop-streaming.jar:/usr/lib/hadoop/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-common.jar:/usr/lib/hadoop/.//hadoop-yarn-api.jar:/usr/lib/hadoop/.//hadoop-ant.jar:/usr/lib/hadoop/.//hadoop-aws.jar:/usr/lib/hadoop/.//hadoop-sls.jar:/usr/lib/hadoop/.//hadoop-datajoin.jar:/usr/lib/hadoop/.//hadoop-gridmix.jar:/usr/lib/hadoop/.//hadoop-common.jar:/usr/lib/hadoop/.//hadoop-distcp.jar:/usr/lib/hadoop/.//hadoop-openstack.jar:/usr/lib/hadoop/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop/.//hadoop-rumen.jar:/usr/lib/hadoop/.//hadoop-yarn-registry.jar:/usr/lib/hadoop/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-hdfs/./:/usr/lib/hadoop-hdfs/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-hdfs/lib/okhttp-2.7.5.jar:/usr/lib/hadoop-hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/commons-codec-1.4.jar:/usr/lib/hadoop-hdfs/lib/asm-3.2.jar:/usr/lib/hadoop-hdfs/lib/jersey-server-1.9.jar:/usr/lib/hadoop-hdfs/lib/servlet-api-2.5.jar:/usr/lib/hadoop-hdfs/lib/commons-lang-2.6.jar:/usr/lib/hadoop-hdfs/lib/commons-cli-1.2.jar:/usr/lib/hadoop-hdfs/lib/okio-1.6.0.jar:/usr/lib/hadoop-hdfs/lib/log4j-1.2.17.jar:/usr/lib/hadoop-hdfs/lib/commons-io-2.4.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-hdfs/lib/guava-11.0.2.jar:/usr/lib/hadoop-hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-hdfs/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-hdfs/lib/jackson-annotations-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/netty-all-4.0.23.Final.jar:/usr/lib/hadoop-hdfs/lib/jackson-databind-2.6.7.jar:/usr/lib/hadoop-hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-hdfs/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-hdfs/lib/jersey-core-1.9.jar:/usr/lib/hadoop-hdfs/lib/xercesImpl-2.12.0.jar:/usr/lib/hadoop-hdfs/lib/xml-apis-1.4.01.jar:/usr/lib/hadoop-hdfs/lib/xmlenc-0.52.jar:/usr/lib/hadoop-hdfs/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-hdfs/lib/commons-daemon-1.0.13.jar:/usr/lib/hadoop-hdfs/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-native-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-client-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-rbf-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-hdfs/.//hadoop-hdfs-nfs-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-yarn/lib/activation-1.1.jar:/usr/lib/hadoop-yarn/lib/json-io-2.5.1.jar:/usr/lib/hadoop-yarn/lib/commons-net-3.1.jar:/usr/lib/hadoop-yarn/lib/commons-beanutils-1.9.4.jar:/usr/lib/hadoop-yarn/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-yarn/lib/json-smart-1.3.1.jar:/usr/lib/hadoop-yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/commons-codec-1.4.jar:/usr/lib/hadoop-yarn/lib/java-xmlbuilder-0.4.jar:/usr/lib/hadoop-yarn/lib/asm-3.2.jar:/usr/lib/hadoop-yarn/lib/commons-collections-3.2.2.jar:/usr/lib/hadoop-yarn/lib/jersey-client-1.9.jar:/usr/lib/hadoop-yarn/lib/metrics-core-3.0.1.jar:/usr/lib/hadoop-yarn/lib/jsch-0.1.54.jar:/usr/lib/hadoop-yarn/lib/jersey-server-1.9.jar:/usr/lib/hadoop-yarn/lib/servlet-api-2.5.jar:/usr/lib/hadoop-yarn/lib/commons-lang-2.6.jar:/usr/lib/hadoop-yarn/lib/commons-cli-1.2.jar:/usr/lib/hadoop-yarn/lib/jackson-xc-1.9.13.jar:/usr/lib/hadoop-yarn/lib/spotbugs-annotations-3.1.9.jar:/usr/lib/hadoop-yarn/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-yarn/lib/jets3t-0.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-client-2.7.1.jar:/usr/lib/hadoop-yarn/lib/curator-framework-2.7.1.jar:/usr/lib/hadoop-yarn/lib/jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/commons-digester-1.8.jar:/usr/lib/hadoop-yarn/lib/log4j-1.2.17.jar:/usr/lib/hadoop-yarn/lib/aopalliance-1.0.jar:/usr/lib/hadoop-yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-yarn/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/fst-2.50.jar:/usr/lib/hadoop-yarn/lib/commons-io-2.4.jar:/usr/lib/hadoop-yarn/lib/jettison-1.1.jar:/usr/lib/hadoop-yarn/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-yarn/lib/avro-1.7.7.jar:/usr/lib/hadoop-yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jaxb-api-2.2.2.jar:/usr/lib/hadoop-yarn/lib/guava-11.0.2.jar:/usr/lib/hadoop-yarn/lib/htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-yarn/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-yarn/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-yarn/lib/paranamer-2.3.jar:/usr/lib/hadoop-yarn/lib/guice-3.0.jar:/usr/lib/hadoop-yarn/lib/jsp-api-2.1.jar:/usr/lib/hadoop-yarn/lib/commons-math3-3.1.1.jar:/usr/lib/hadoop-yarn/lib/ehcache-3.3.1.jar:/usr/lib/hadoop-yarn/lib/jetty-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-yarn/lib/java-util-1.9.0.jar:/usr/lib/hadoop-yarn/lib/curator-recipes-2.7.1.jar:/usr/lib/hadoop-yarn/lib/nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-yarn/lib/woodstox-core-5.0.3.jar:/usr/lib/hadoop-yarn/lib/commons-compress-1.19.jar:/usr/lib/hadoop-yarn/lib/commons-lang3-3.4.jar:/usr/lib/hadoop-yarn/lib/stax2-api-3.1.4.jar:/usr/lib/hadoop-yarn/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-yarn/lib/jcip-annotations-1.0-1.jar:/usr/lib/hadoop-yarn/lib/jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-yarn/lib/jersey-core-1.9.jar:/usr/lib/hadoop-yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-yarn/lib/api-util-1.0.0-M20.jar:/usr/lib/hadoop-yarn/lib/apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-yarn/lib/jersey-json-1.9.jar:/usr/lib/hadoop-yarn/lib/zookeeper-3.4.14.jar:/usr/lib/hadoop-yarn/lib/javax.inject-1.jar:/usr/lib/hadoop-yarn/lib/httpcore-4.4.11.jar:/usr/lib/hadoop-yarn/lib/httpclient-4.5.9.jar:/usr/lib/hadoop-yarn/lib/gson-2.2.4.jar:/usr/lib/hadoop-yarn/lib/stax-api-1.0-2.jar:/usr/lib/hadoop-yarn/lib/xmlenc-0.52.jar:/usr/lib/hadoop-yarn/lib/jsr305-3.0.0.jar:/usr/lib/hadoop-yarn/lib/HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-yarn/lib/commons-logging-1.1.3.jar:/usr/lib/hadoop-yarn/lib/audience-annotations-0.5.0.jar:/usr/lib/hadoop-yarn/lib/commons-configuration-1.6.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-distributedshell.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-client.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-sharedcachemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-common.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-router-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-tests-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-nodemanager.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-applications-unmanaged-am-launcher-2.10.0-amzn-0.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-timeline-pluginstorage.jar:/usr/lib/hadoop-yarn/.//hadoop-yarn-server-applicationhistoryservice.jar:/usr/lib/hadoop-mapreduce/lib/guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/lib/protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/asm-3.2.jar:/usr/lib/hadoop-mapreduce/lib/jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/lib/snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/lib/junit-4.11.jar:/usr/lib/hadoop-mapreduce/lib/log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/lib/aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/lib/hamcrest-core-1.3.jar:/usr/lib/hadoop-mapreduce/lib/netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/lib/avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/lib/jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/lib/leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/lib/paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/lib/guice-3.0.jar:/usr/lib/hadoop-mapreduce/lib/commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/lib/jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/lib/javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//guice-servlet-3.0.jar:/usr/lib/hadoop-mapreduce/.//activation-1.1.jar:/usr/lib/hadoop-mapreduce/.//json-io-2.5.1.jar:/usr/lib/hadoop-mapreduce/.//commons-net-3.1.jar:/usr/lib/hadoop-mapreduce/.//commons-beanutils-1.9.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//protobuf-java-2.5.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//json-smart-1.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-mapper-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//commons-codec-1.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common.jar:/usr/lib/hadoop-mapreduce/.//java-xmlbuilder-0.4.jar:/usr/lib/hadoop-mapreduce/.//asm-3.2.jar:/usr/lib/hadoop-mapreduce/.//commons-collections-3.2.2.jar:/usr/lib/hadoop-mapreduce/.//jersey-client-1.9.jar:/usr/lib/hadoop-mapreduce/.//metrics-core-3.0.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsch-0.1.54.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//azure-data-lake-store-sdk-2.2.3.jar:/usr/lib/hadoop-mapreduce/.//jersey-server-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy.jar:/usr/lib/hadoop-mapreduce/.//servlet-api-2.5.jar:/usr/lib/hadoop-mapreduce/.//commons-lang-2.6.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-core-3.4.0.jar:/usr/lib/hadoop-mapreduce/.//commons-cli-1.2.jar:/usr/lib/hadoop-mapreduce/.//jackson-xc-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//spotbugs-annota\u001b[0m\n",
      "\u001b[34mtions-3.1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-extras.jar:/usr/lib/hadoop-mapreduce/.//commons-httpclient-3.1.jar:/usr/lib/hadoop-mapreduce/.//azure-storage-5.4.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-web-proxy-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//snappy-java-1.1.7.3.jar:/usr/lib/hadoop-mapreduce/.//jets3t-0.9.0.jar:/usr/lib/hadoop-mapreduce/.//curator-client-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//curator-framework-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs.jar:/usr/lib/hadoop-mapreduce/.//jetty-sslengine-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//aliyun-sdk-oss-3.4.1.jar:/usr/lib/hadoop-mapreduce/.//commons-digester-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0-tests.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app.jar:/usr/lib/hadoop-mapreduce/.//log4j-1.2.17.jar:/usr/lib/hadoop-mapreduce/.//aopalliance-1.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-core.jar:/usr/lib/hadoop-mapreduce/.//aws-java-sdk-bundle-1.11.852.jar:/usr/lib/hadoop-mapreduce/.//mssql-jdbc-6.2.1.jre7.jar:/usr/lib/hadoop-mapreduce/.//apacheds-kerberos-codec-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//fst-2.50.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archives-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure.jar:/usr/lib/hadoop-mapreduce/.//commons-io-2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-app-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jettison-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-auth-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//netty-3.10.6.Final.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun.jar:/usr/lib/hadoop-mapreduce/.//avro-1.7.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-azure-datalake-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//ojalgo-43.0.jar:/usr/lib/hadoop-mapreduce/.//jackson-jaxrs-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jaxb-api-2.2.2.jar:/usr/lib/hadoop-mapreduce/.//guava-11.0.2.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ram-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//htrace-core4-4.1.0-incubating.jar:/usr/lib/hadoop-mapreduce/.//jersey-guice-1.9.jar:/usr/lib/hadoop-mapreduce/.//leveldbjni-all-1.8.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//paranamer-2.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-plugins.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//guice-3.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jsp-api-2.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-resourcemanager.jar:/usr/lib/hadoop-mapreduce/.//commons-math3-3.1.1.jar:/usr/lib/hadoop-mapreduce/.//ehcache-3.3.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-annotations-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//hadoop-streaming.jar:/usr/lib/hadoop-mapreduce/.//jetty-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//api-asn1-api-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aliyun-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-resourceestimator-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-common.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-ecs-4.2.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-ant.jar:/usr/lib/hadoop-mapreduce/.//hadoop-aws.jar:/usr/lib/hadoop-mapreduce/.//hadoop-sls.jar:/usr/lib/hadoop-mapreduce/.//java-util-1.9.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-datajoin.jar:/usr/lib/hadoop-mapreduce/.//curator-recipes-2.7.1.jar:/usr/lib/hadoop-mapreduce/.//nimbus-jose-jwt-4.41.1.jar:/usr/lib/hadoop-mapreduce/.//jackson-databind-2.6.7.jar:/usr/lib/hadoop-mapreduce/.//woodstox-core-5.0.3.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix.jar:/usr/lib/hadoop-mapreduce/.//commons-compress-1.19.jar:/usr/lib/hadoop-mapreduce/.//commons-lang3-3.4.jar:/usr/lib/hadoop-mapreduce/.//stax2-api-3.1.4.jar:/usr/lib/hadoop-mapreduce/.//jackson-core-asl-1.9.13.jar:/usr/lib/hadoop-mapreduce/.//jcip-annotations-1.0-1.jar:/usr/lib/hadoop-mapreduce/.//aliyun-java-sdk-sts-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//jetty-util-6.1.26-emr.jar:/usr/lib/hadoop-mapreduce/.//jersey-core-1.9.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-jobclient-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//jaxb-impl-2.2.3-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-distcp.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-examples.jar:/usr/lib/hadoop-mapreduce/.//azure-keyvault-core-0.8.0.jar:/usr/lib/hadoop-mapreduce/.//api-util-1.0.0-M20.jar:/usr/lib/hadoop-mapreduce/.//apacheds-i18n-2.0.0-M15.jar:/usr/lib/hadoop-mapreduce/.//hadoop-openstack.jar:/usr/lib/hadoop-mapreduce/.//jersey-json-1.9.jar:/usr/lib/hadoop-mapreduce/.//zookeeper-3.4.14.jar:/usr/lib/hadoop-mapreduce/.//hadoop-gridmix-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-shuffle.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//javax.inject-1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-api-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//httpcore-4.4.11.jar:/usr/lib/hadoop-mapreduce/.//httpclient-4.5.9.jar:/usr/lib/hadoop-mapreduce/.//jdom-1.1.jar:/usr/lib/hadoop-mapreduce/.//hadoop-archive-logs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//gson-2.2.4.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-hs-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//stax-api-1.0-2.jar:/usr/lib/hadoop-mapreduce/.//xmlenc-0.52.jar:/usr/lib/hadoop-mapreduce/.//jsr305-3.0.0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-mapreduce-client-common-2.10.0-amzn-0.jar:/usr/lib/hadoop-mapreduce/.//hadoop-rumen.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-registry.jar:/usr/lib/hadoop-mapreduce/.//HikariCP-java7-2.4.12.jar:/usr/lib/hadoop-mapreduce/.//commons-logging-1.1.3.jar:/usr/lib/hadoop-mapreduce/.//audience-annotations-0.5.0.jar:/usr/lib/hadoop-mapreduce/.//commons-configuration-1.6.jar:/usr/lib/hadoop-mapreduce/.//hadoop-yarn-server-applicationhistoryservice.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = git@aws157git.com:/pkg/Aws157BigTop -r d1e860a34cc1aea3d600c57c5c0270ea41579e8c; compiled by 'ec2-user' on 2020-09-19T02:05Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_272\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO namenode.NameNode: createNameNode []\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO impl.MetricsSystemImpl: NameNode metrics system started\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO namenode.NameNode: fs.defaultFS is hdfs://10.2.212.241/\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO conf.Configuration: found resource core-site.xml at file:/etc/hadoop/conf.empty/core-site.xml\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO security.Groups: clearing userToGroupsMap cache\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO conf.Configuration: found resource yarn-site.xml at file:/etc/hadoop/conf.empty/yarn-site.xml\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMFatalEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMFatalEventDispatcher\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: 86400000ms and NMTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: 86400000ms and ContainerTokenKeyActivationDelay: 900000ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO nodemanager.NodeManager: Node Manager health check script is not available or doesn't have execute permission, so not starting the node health script runner.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: 86400000ms and AMRMTokenKeyActivationDelay: 900000 ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStoreEventType for class org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.NodesListManager\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/opt/amazon/hadoop/hdfs/datanode/\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$LocalizationEventHandlerWrapper\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServicesEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncherEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerSchedulerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.scheduler.ContainerScheduler\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEventType for class org.apache.hadoop.yarn.event.EventDispatcher\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.ContainerManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.NodeManagerEventType for class org.apache.hadoop.yarn.server.nodemanager.NodeManager\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType for class org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$NodeEventDispatcher\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO http.HttpRequestLog: Http request log for http.requests.namenode is not defined\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO impl.MetricsConfig: loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:34 INFO impl.MetricsSystemImpl: NodeManager metrics system started\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO impl.MetricsSystemImpl: DataNode metrics system started\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO impl.MetricsSystemImpl: ResourceManager metrics system started\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO nodemanager.DirectoryCollection: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instantiated.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.RMAppManagerEventType for class org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncherEventType for class org.apache.hadoop.yarn.server.resourcemanager.amlauncher.ApplicationMasterLauncher\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO resourcemanager.RMNMInfo: Registered RMNMInfo MBean\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO monitor.RMAppLifetimeMonitor: Application lifelime monitor interval set to 3000 ms.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.HostsFileReader: Refreshing hosts (include/exclude) list\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO conf.Configuration: found resource capacity-scheduler.xml at file:/etc/hadoop/conf.empty/capacity-scheduler.xml\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Jetty bound to port 50070\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO scheduler.AbstractYarnScheduler: Minimum allocation = <memory:1, vCores:1>\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO scheduler.AbstractYarnScheduler: Maximum allocation = <memory:127139, vCores:16>\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO nodemanager.NodeResourceMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@1349883\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.sharedcache.SharedCacheUploadService\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO containermanager.ContainerManagerImpl: AMRMProxyService is disabled\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO localizer.ResourceLocalizationService: per directory file limit = 8192\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root is undefined\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root is undefined\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO capacity.ParentQueue: root, capacity=1.0, absoluteCapacity=1.0, maxCapacity=1.0, absoluteMaxCapacity=1.0, state=RUNNING, acls=ADMINISTER_QUEUE:*SUBMIT_APP:*, labels=*,\u001b[0m\n",
      "\u001b[34m, reservationsContinueLooking=true, orderingPolicy=utilization, priority=0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO capacity.ParentQueue: Initialized parent-queue root name=root, fullname=root\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO localizer.ResourceLocalizationService: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizerEventType for class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$LocalizerTracker\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorPlugin : org.apache.hadoop.yarn.util.ResourceCalculatorPlugin@5386659f\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO monitor.ContainersMonitorImpl:  Using ResourceCalculatorProcessTree : null\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO monitor.ContainersMonitorImpl: Physical memory check enabled: true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO monitor.ContainersMonitorImpl: Virtual memory check enabled: true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO monitor.ContainersMonitorImpl: ContainersMonitor enabled: true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO capacity.CapacitySchedulerConfiguration: max alloc mb per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO capacity.CapacitySchedulerConfiguration: max alloc vcore per queue for root.default is undefined\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 WARN monitor.ContainersMonitorImpl: NodeManager configured with 124.2 G physical memory allocated to containers, which is more than 80% of the total physical memory available (124.5 G). Thrashing might happen.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO containermanager.ContainerManagerImpl: Not a recoverable state store. Nothing to recover.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO capacity.LeafQueue: Initializing default\u001b[0m\n",
      "\u001b[34mcapacity = 1.0 [= (float) configuredCapacity / 100 ]\u001b[0m\n",
      "\u001b[34mabsoluteCapacity = 1.0 [= parentAbsoluteCapacity * capacity ]\u001b[0m\n",
      "\u001b[34mmaxCapacity = 1.0 [= configuredMaxCapacity ]\u001b[0m\n",
      "\u001b[34mabsoluteMaxCapacity = 1.0 [= 1.0 maximumCapacity undefined, (parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\u001b[0m\n",
      "\u001b[34muserLimit = 100 [= configuredUserLimit ]\u001b[0m\n",
      "\u001b[34muserLimitFactor = 1.0 [= configuredUserLimitFactor ]\u001b[0m\n",
      "\u001b[34mmaxApplications = 10000 [= configuredMaximumSystemApplicationsPerQueue or (int)(configuredMaximumSystemApplications * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mmaxApplicationsPerUser = 10000 [= (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor) ]\u001b[0m\n",
      "\u001b[34musedCapacity = 0.0 [= usedResourcesMemory / (clusterResourceMemory * absoluteCapacity)]\u001b[0m\n",
      "\u001b[34mabsoluteUsedCapacity = 0.0 [= usedResourcesMemory / clusterResourceMemory]\u001b[0m\n",
      "\u001b[34mmaxAMResourcePerQueuePercent = 0.1 [= configuredMaximumAMResourcePercent ]\u001b[0m\n",
      "\u001b[34mminimumAllocationFactor = 0.99999213 [= (float)(maximumAllocationMemory - minimumAllocationMemory) / maximumAllocationMemory ]\u001b[0m\n",
      "\u001b[34mmaximumAllocation = <memory:127139, vCores:16> [= configuredMaxAllocation ]\u001b[0m\n",
      "\u001b[34mnumContainers = 0 [= currentNumContainers ]\u001b[0m\n",
      "\u001b[34mstate = RUNNING [= configuredState ]\u001b[0m\n",
      "\u001b[34macls = ADMINISTER_QUEUE:*SUBMIT_APP:* [= configuredAcls ]\u001b[0m\n",
      "\u001b[34mnodeLocalityDelay = 40\u001b[0m\n",
      "\u001b[34mrackLocalityAdditionalDelay = -1\u001b[0m\n",
      "\u001b[34mlabels=*,\u001b[0m\n",
      "\u001b[34mreservationsContinueLooking = true\u001b[0m\n",
      "\u001b[34mpreemptionDisabled = true\u001b[0m\n",
      "\u001b[34mdefaultAppPriorityPerQueue = 0\u001b[0m\n",
      "\u001b[34mpriority = 0\u001b[0m\n",
      "\u001b[34mmaxLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34mdefaultLifetime = -1 seconds\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: default: capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO capacity.CapacitySchedulerQueueManager: Initialized queue: root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO capacity.CapacitySchedulerQueueManager: Initialized root queue root: numChildQueue= 1, capacity=1.0, absoluteCapacity=1.0, usedResources=<memory:0, vCores:0>usedCapacity=0.0, numApps=0, numContainers=0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO placement.UserGroupMappingPlacementRule: Initialized queue mappings, override: false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO capacity.WorkflowPriorityMappingsManager: Initialized workflow priority mappings, override: false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=class org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:1, vCores:1>>, maximumAllocation=<<memory:127139, vCores:16>>, asynchronousScheduling=false, asyncScheduleInterval=5ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO conf.Configuration: dynamic-resources.xml not found\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO resourcemanager.AMSProcessingChain: Initializing AMS Processing chain. Root Processor=[org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor].\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO resourcemanager.ResourceManager: TimelineServicePublisher is not configured\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO conf.Configuration: node-resources.xml not found\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO resource.ResourceUtils: Unable to find 'node-resources.xml'.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO nodemanager.NodeStatusUpdaterImpl: Nodemanager resources is set to: <memory:127139, vCores:16>\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO nodemanager.NodeStatusUpdaterImpl: Initialized nodemanager with : physical-memory=127139 virtual-memory=635695 virtual-cores=16\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO datanode.BlockScanner: Initialized block scanner with targetBytesPerSec 1048576\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO datanode.DataNode: Configured hostname is algo-1\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 WARN conf.Configuration: No unit for dfs.datanode.outliers.report.interval(1800000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO datanode.DataNode: Starting DataNode with maxLockedMemory = 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 2000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO datanode.DataNode: Opened streaming server at /0.0.0.0:50010\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO datanode.DataNode: Balancing bandwidth is 10485760 bytes/s\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO datanode.DataNode: Number threads for balancing is 50\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpRequestLog: Http request log for http.requests.resourcemanager is not defined\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context cluster\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context logs\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter RMAuthenticationFilter (class=org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter) to context static\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context cluster\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: adding path spec: /cluster/*\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO ipc.Server: Starting Socket Reader #1 for port 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpRequestLog: Http request log for http.requests.datanode is not defined\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Jetty bound to port 43639\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ContainerManagementProtocolPB to the server\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO ipc.Server: IPC Server listener on 0: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO security.NMContainerTokenSecretManager: Updating node address : algo-1:35883\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 500 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO ipc.Server: Starting Socket Reader #1 for port 8040\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.nodemanager.api.LocalizationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO ipc.Server: IPC Server listener on 8040: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO localizer.ResourceLocalizationService: Localizer started on port 8040\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO containermanager.ContainerManagerImpl: ContainerManager started at /10.2.212.241:35883\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO containermanager.ContainerManagerImpl: ContainerManager bound to algo-1/10.2.212.241:0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO webapp.WebServer: Instantiating NMWebApp at algo-1:8042\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpRequestLog: Http request log for http.requests.nodemanager is not defined\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Jetty bound to port 8088\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@localhost:43639\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context node\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context node\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context static\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Added filter authentication (class=org.apache.hadoop.security.AuthenticationWithProxyUserFilter) to context logs\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: adding path spec: /node/*\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: adding path spec: /ws/*\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 WARN namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 WARN namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop/hadoop-yarn-common-2.10.0-amzn-0.jar!/webapps/cluster to work/Jetty_10_2_212_241_8088_cluster____6c2o84/webapp\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO web.DatanodeHttpServer: Listening HTTP traffic on /0.0.0.0:50075\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO datanode.DataNode: dnUserName = root\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO datanode.DataNode: supergroup = supergroup\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.BlockManager: The block deletion will start around 2021 Apr 14 22:10:35\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: 2.0% max memory 889 MB = 17.8 MB\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: capacity      = 2^21 = 2097152 entries\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.BlockManager: dfs.block.access.token.enable=false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 WARN conf.Configuration: No unit for dfs.heartbeat.interval(3) assuming SECONDS\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 WARN conf.Configuration: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.BlockManager: replicationRecheckInterval = 3000\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSNamesystem: Append Enabled: true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSDirectory: GLOBAL serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO ipc.Server: Starting Socket Reader #1 for port 50020\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: 1.0% max memory 889 MB = 8.9 MB\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: falseskipCaptureAccessTimeOnlyChange: false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: 0.25% max memory 889 MB = 2.2 MB\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: capacity      = 2^18 = 262144 entries\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO util.GSet: capacity      = 2^15 = 32768 entries\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/namenode/in_use.lock acquired by nodename 114@algo-1\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FileJournalManager: Recovering unfinalized segments in /opt/amazon/hadoop/hdfs/namenode/current\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSImage: No edit log streams selected.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSImage: Planning to load image: FSImageFile(file=/opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO datanode.DataNode: Opened IPC server at /0.0.0.0:50020\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO datanode.DataNode: Refresh request received for nameservices: null\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO webapp.WebApps: Registered webapp guice modules\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO http.HttpServer2: Jetty bound to port 8042\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO mortbay.log: jetty-6.1.26-emr\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO datanode.DataNode: Starting BPOfferServices for nameservices: <default>\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSImageFormatPBINode: Loading 1 INodes.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO datanode.DataNode: Block pool <registering> (Datanode Uuid unassigned) service to algo-1/10.2.212.241:8020 starting to offer service\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSImageFormatPBINode: Successfully loaded 1 inodes\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO ipc.Server: IPC Server listener on 50020: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSImage: Loaded image for txid 0 from /opt/amazon/hadoop/hdfs/namenode/current/fsimage_0000000000000000000\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO namenode.FSEditLog: Starting log segment at 1\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:35 INFO mortbay.log: Extract jar:file:/usr/lib/hadoop/hadoop-yarn-common-2.10.0-amzn-0.jar!/webapps/node to work/Jetty_algo.1_8042_node____.afclh/webapp\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:35 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:35 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:35 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:35 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO namenode.NameCache: initialized with 0 entries 0 lookups\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO namenode.FSNamesystem: Finished loading FSImage in 192 msecs\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:36 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO namenode.NameNode: RPC server is binding to algo-1:8020\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO namenode.NameNode: Enable NameNode state context:false\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.Server: Starting Socket Reader #1 for port 8020\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO namenode.NameNode: Clients are to use algo-1:8020 to access this namenode/service.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO namenode.FSNamesystem: Registered FSNamesystemState MBean\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO namenode.LeaseManager: Number of blocks under construction: 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO blockmanagement.BlockManager: initializing replication queues\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO hdfs.StateChange: STATE* Leaving safe mode after 0 secs\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:36 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices as a root resource class\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:36 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.webapp.GenericExceptionHandler as a provider class\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:36 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory register\u001b[0m\n",
      "\u001b[34mINFO: Registering org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver as a provider class\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:36 PM com.sun.jersey.server.impl.application.WebApplicationImpl _initiate\u001b[0m\n",
      "\u001b[34mINFO: Initiating Jersey application, version 'Jersey: 1.9 09/02/2011 11:17 AM'\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO blockmanagement.BlockManager: Total number of blocks            = 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO blockmanagement.BlockManager: Number of invalid blocks          = 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO blockmanagement.BlockManager: Number of under-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO blockmanagement.BlockManager: Number of  over-replicated blocks = 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO blockmanagement.BlockManager: Number of blocks being written    = 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 8 msec\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.Server: IPC Server listener on 8020: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO namenode.NameNode: NameNode RPC up at: algo-1/10.2.212.241:8020\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO namenode.FSNamesystem: Starting services required for active state\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO namenode.FSDirectory: Initializing quota with 4 thread(s)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO namenode.FSDirectory: Quota initialization completed in 4 milliseconds\u001b[0m\n",
      "\u001b[34mname space=1\u001b[0m\n",
      "\u001b[34mstorage space=0\u001b[0m\n",
      "\u001b[34mstorage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:36 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.JAXBContextResolver to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:36 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:36 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.webapp.GenericExceptionHandler to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:36 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@10.2.212.241:8088\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO webapp.WebApps: Web app cluster started at 8088\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 100 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.Server: Starting Socket Reader #1 for port 8033\u001b[0m\n",
      "\u001b[34mApr 14, 2021 10:10:36 PM com.sun.jersey.guice.spi.container.GuiceComponentProviderFactory getComponentProvider\u001b[0m\n",
      "\u001b[34mINFO: Binding org.apache.hadoop.yarn.server.nodemanager.webapp.NMWebServices to GuiceManagedComponentProvider with the scope \"Singleton\"\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@algo-1:8042\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO webapp.WebApps: Web app node started at 8042\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO nodemanager.NodeStatusUpdaterImpl: Node ID assigned is : algo-1:35883\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO client.RMProxy: Connecting to ResourceManager at /10.2.212.241:8031\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceManagerAdministrationProtocolPB to the server\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.Server: IPC Server listener on 8033: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO resourcemanager.ResourceManager: Transitioning to active state\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO recovery.RMStateStore: Updating AMRMToken\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO security.RMContainerTokenSecretManager: Rolling master-key for container-tokens\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO security.NMTokenSecretManagerInRM: Rolling master-key for nm-tokens\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 1\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO security.RMDelegationTokenSecretManager: storing master key with keyID 2\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO recovery.RMStateStore: Storing RMDTMasterKey.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO nodemanager.NodeStatusUpdaterImpl: Sending out 0 NM container statuses: []\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO event.AsyncDispatcher: Registering class org.apache.hadoop.yarn.nodelabels.event.NodeLabelsStoreEventType for class org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager$ForwardingEventHandler\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO nodemanager.NodeStatusUpdaterImpl: Registering with RM using containers :[]\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.Server: Starting Socket Reader #1 for port 8031\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.server.api.ResourceTrackerPB to the server\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.Server: IPC Server listener on 8031: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO util.JvmPauseMonitor: Starting JVM pause monitor\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.Server: Starting Socket Reader #1 for port 8030\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB to the server\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.Client: Retrying connect to server: algo-1/10.2.212.241:8020. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:36 INFO ipc.Server: IPC Server listener on 8030: starting\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     cluster is up\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     transitioning from status BOOTSTRAPPING to WAITING\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     starting executor logs watcher\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     start log event log publisher\u001b[0m\n",
      "\u001b[34mStarting executor logs watcher on log_dir: /var/log/yarn\u001b[0m\n",
      "\u001b[34m04-14 22:10 sagemaker-spark-event-logs-publisher INFO     Start to copy the spark event logs file.\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     Waiting for hosts to bootstrap: ['algo-1']\u001b[0m\n",
      "\u001b[34m04-14 22:10 sagemaker-spark-event-logs-publisher INFO     Writing event log config to spark-defaults.conf\u001b[0m\n",
      "\u001b[34m04-14 22:10 sagemaker-spark-event-logs-publisher INFO     Event log file does not exist.\u001b[0m\n",
      "\u001b[34m04-14 22:10 smspark-submit INFO     Received host statuses: dict_items([('algo-1', StatusMessage(status='WAITING', timestamp='2021-04-14T22:10:36.984663'))])\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO resourcemanager.ResourceTrackerService: NodeManager from node algo-1(cmPort: 35883 httpPort: 8042) registered with capability: <memory:127139, vCores:16>, assigned nodeId algo-1:35883\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO rmnode.RMNodeImpl: algo-1:35883 Node Transitioned from NEW to RUNNING\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO capacity.CapacityScheduler: Added node algo-1:35883 clusterResource: <memory:127139, vCores:16>\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO security.NMContainerTokenSecretManager: Rolling master-key for container-tokens, got key with id 1314542793\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO security.NMTokenSecretManagerInNM: Rolling master-key for container-tokens, got key with id -2030723584\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO nodemanager.NodeStatusUpdaterImpl: Registered with ResourceManager as algo-1:35883 with total resource of <memory:127139, vCores:16>\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 5000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO ipc.Server: Starting Socket Reader #1 for port 8032\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO pb.RpcServerFactoryPBImpl: Adding protocol org.apache.hadoop.yarn.api.ApplicationClientProtocolPB to the server\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO ipc.Server: IPC Server Responder: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO ipc.Server: IPC Server listener on 8032: starting\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO datanode.DataNode: Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to algo-1/10.2.212.241:8020\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO common.Storage: Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO resourcemanager.ResourceManager: Transitioned to active state\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO common.Storage: Lock on /opt/amazon/hadoop/hdfs/datanode/in_use.lock acquired by nodename 115@algo-1\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/datanode is not formatted for namespace 1513520005. Formatting...\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO common.Storage: Generated new storageID DS-e384b50a-1682-4938-8fea-e2ac141e76fa for directory /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO common.Storage: Analyzing storage directories for bpid BP-592339435-10.2.212.241-1618438233732\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO common.Storage: Locking is disabled for /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO common.Storage: Block pool storage directory /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732 is not formatted for BP-592339435-10.2.212.241-1618438233732. Formatting ...\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO common.Storage: Formatting block pool BP-592339435-10.2.212.241-1618438233732 directory /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO datanode.DataNode: Setting up storage: nsid=1513520005;bpid=BP-592339435-10.2.212.241-1618438233732;lv=-57;nsInfo=lv=-63;cid=CID-8223e0bb-23d0-4123-a502-d032e128ef7a;nsid=1513520005;c=1618438233732;bpid=BP-592339435-10.2.212.241-1618438233732;dnuuid=null\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO datanode.DataNode: Generated and persisted new Datanode UUID 53eeda0c-78a7-4507-81fe-a70e70993d4c\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO impl.FsDatasetImpl: Added new volume: DS-e384b50a-1682-4938-8fea-e2ac141e76fa\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO impl.FsDatasetImpl: Added volume - /opt/amazon/hadoop/hdfs/datanode/current, StorageType: DISK\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO impl.FsDatasetImpl: Registered FSDatasetState MBean\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO checker.ThrottledAsyncChecker: Scheduling a check for /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO checker.DatasetVolumeChecker: Scheduled health check for volume /opt/amazon/hadoop/hdfs/datanode/current\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO impl.FsDatasetImpl: Adding block pool BP-592339435-10.2.212.241-1618438233732\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO impl.FsDatasetImpl: Scanning block pool BP-592339435-10.2.212.241-1618438233732 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO impl.FsDatasetImpl: Time taken to scan block pool BP-592339435-10.2.212.241-1618438233732 on /opt/amazon/hadoop/hdfs/datanode/current: 28ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO impl.FsDatasetImpl: Total time to scan all replicas for block pool BP-592339435-10.2.212.241-1618438233732: 31ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO impl.FsDatasetImpl: Adding replicas to map for block pool BP-592339435-10.2.212.241-1618438233732 on volume /opt/amazon/hadoop/hdfs/datanode/current...\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO impl.BlockPoolSlice: Replica Cache file: /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/replicas doesn't exist \u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO impl.FsDatasetImpl: Time to add replicas to map for block pool BP-592339435-10.2.212.241-1618438233732 on volume /opt/amazon/hadoop/hdfs/datanode/current: 3ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO impl.FsDatasetImpl: Total time to add all replicas to map for block pool BP-592339435-10.2.212.241-1618438233732: 4ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO datanode.VolumeScanner: Now scanning bpid BP-592339435-10.2.212.241-1618438233732 on volume /opt/amazon/hadoop/hdfs/datanode\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-e384b50a-1682-4938-8fea-e2ac141e76fa): finished scanning block pool BP-592339435-10.2.212.241-1618438233732\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO datanode.DirectoryScanner: Periodic Directory Tree Verification scan starting at 4/14/21 11:42 PM with interval of 21600000ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO datanode.DataNode: Block pool BP-592339435-10.2.212.241-1618438233732 (Datanode Uuid 53eeda0c-78a7-4507-81fe-a70e70993d4c) service to algo-1/10.2.212.241:8020 beginning handshake with NN\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO datanode.VolumeScanner: VolumeScanner(/opt/amazon/hadoop/hdfs/datanode, DS-e384b50a-1682-4938-8fea-e2ac141e76fa): no suitable block pools found to scan.  Waiting 1814399970 ms.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.2.212.241:50010, datanodeUuid=53eeda0c-78a7-4507-81fe-a70e70993d4c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-8223e0bb-23d0-4123-a502-d032e128ef7a;nsid=1513520005;c=1618438233732) storage 53eeda0c-78a7-4507-81fe-a70e70993d4c\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO net.NetworkTopology: Adding a new node: /default-rack/10.2.212.241:50010\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO blockmanagement.BlockReportLeaseManager: Registered DN 53eeda0c-78a7-4507-81fe-a70e70993d4c (10.2.212.241:50010).\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO datanode.DataNode: Block pool Block pool BP-592339435-10.2.212.241-1618438233732 (Datanode Uuid 53eeda0c-78a7-4507-81fe-a70e70993d4c) service to algo-1/10.2.212.241:8020 successfully registered with NN\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO datanode.DataNode: For namenode algo-1/10.2.212.241:8020 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO blockmanagement.DatanodeDescriptor: Adding new storage ID DS-e384b50a-1682-4938-8fea-e2ac141e76fa for DN 10.2.212.241:50010\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO BlockStateChange: BLOCK* processReport 0x63b72e4142fe2890: Processing first storage report for DS-e384b50a-1682-4938-8fea-e2ac141e76fa from datanode 53eeda0c-78a7-4507-81fe-a70e70993d4c\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO BlockStateChange: BLOCK* processReport 0x63b72e4142fe2890: from storage DS-e384b50a-1682-4938-8fea-e2ac141e76fa node DatanodeRegistration(10.2.212.241:50010, datanodeUuid=53eeda0c-78a7-4507-81fe-a70e70993d4c, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-57;cid=CID-8223e0bb-23d0-4123-a502-d032e128ef7a;nsid=1513520005;c=1618438233732), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO datanode.DataNode: Successfully sent block report 0x63b72e4142fe2890,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 43 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:37 INFO datanode.DataNode: Got finalize command for block pool BP-592339435-10.2.212.241-1618438233732\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO spark.SparkContext: Running Spark version 2.4.6-amzn-0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO spark.SparkContext: Submitted application: PySparkJob\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO util.Utils: Successfully started service 'sparkDriver' on port 33213.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-0bb081e2-8692-4d3e-94c5-edb8f5c115f0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO memory.MemoryStore: MemoryStore started with capacity 1007.8 MB\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:39 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO util.log: Logging initialized @2720ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: unknown, git hash: unknown\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO server.Server: Started @2804ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO server.AbstractConnector: Started ServerConnector@4cc13af9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5f185059{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7128d16{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7fbfc6f2{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ed9f528{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4db0a9d8{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4cb96eca{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7637f749{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28a84341{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@616c74a2{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@370cf221{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1d7cc166{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@77a42bc3{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@756f98f1{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@25a2e187{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5eae73a4{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@882a181{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a31870{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@33bf7f1a{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e21199a{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3b22d76{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1a4bc5e8{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b09c001{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6281fb54{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@3aa0054d{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4456b198{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:40 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.2.212.241:4040\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:41 INFO client.RMProxy: Connecting to ResourceManager at /10.2.212.241:8032\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:41 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:41 INFO resourcemanager.ClientRMService: Allocated new applicationId: 1\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:41 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:41 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:41 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:41 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:41 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (127139 MB per container)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:41 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:41 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:41 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:41 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:41 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:45 INFO yarn.Client: Uploading resource file:/tmp/spark-12c9073e-38f9-4dab-8421-6e6983e8b4fa/__spark_libs__1279074281364756631.zip -> hdfs://10.2.212.241/user/root/.sparkStaging/application_1618438236794_0001/__spark_libs__1279074281364756631.zip\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:45 INFO hdfs.StateChange: BLOCK* allocate blk_1073741825_1001, replicas=10.2.212.241:50010 for /user/root/.sparkStaging/application_1618438236794_0001/__spark_libs__1279074281364756631.zip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m21/04/14 22:10:47 INFO datanode.DataNode: Receiving BP-592339435-10.2.212.241-1618438233732:blk_1073741825_1001 src: /10.2.212.241:55672 dest: /10.2.212.241:50010\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO DataNode.clienttrace: src: /10.2.212.241:55672, dest: /10.2.212.241:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1306063416_27, offset: 0, srvID: 53eeda0c-78a7-4507-81fe-a70e70993d4c, blockid: BP-592339435-10.2.212.241-1618438233732:blk_1073741825_1001, duration(ns): 192986952\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO datanode.DataNode: PacketResponder: BP-592339435-10.2.212.241-1618438233732:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO hdfs.StateChange: BLOCK* allocate blk_1073741826_1002, replicas=10.2.212.241:50010 for /user/root/.sparkStaging/application_1618438236794_0001/__spark_libs__1279074281364756631.zip\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO datanode.DataNode: Receiving BP-592339435-10.2.212.241-1618438233732:blk_1073741826_1002 src: /10.2.212.241:55674 dest: /10.2.212.241:50010\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO DataNode.clienttrace: src: /10.2.212.241:55674, dest: /10.2.212.241:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1306063416_27, offset: 0, srvID: 53eeda0c-78a7-4507-81fe-a70e70993d4c, blockid: BP-592339435-10.2.212.241-1618438233732:blk_1073741826_1002, duration(ns): 160983888\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO datanode.DataNode: PacketResponder: BP-592339435-10.2.212.241-1618438233732:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO hdfs.StateChange: BLOCK* allocate blk_1073741827_1003, replicas=10.2.212.241:50010 for /user/root/.sparkStaging/application_1618438236794_0001/__spark_libs__1279074281364756631.zip\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO datanode.DataNode: Receiving BP-592339435-10.2.212.241-1618438233732:blk_1073741827_1003 src: /10.2.212.241:55676 dest: /10.2.212.241:50010\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO DataNode.clienttrace: src: /10.2.212.241:55676, dest: /10.2.212.241:50010, bytes: 134217728, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1306063416_27, offset: 0, srvID: 53eeda0c-78a7-4507-81fe-a70e70993d4c, blockid: BP-592339435-10.2.212.241-1618438233732:blk_1073741827_1003, duration(ns): 179117139\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO datanode.DataNode: PacketResponder: BP-592339435-10.2.212.241-1618438233732:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO hdfs.StateChange: BLOCK* allocate blk_1073741828_1004, replicas=10.2.212.241:50010 for /user/root/.sparkStaging/application_1618438236794_0001/__spark_libs__1279074281364756631.zip\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO datanode.DataNode: Receiving BP-592339435-10.2.212.241-1618438233732:blk_1073741828_1004 src: /10.2.212.241:55678 dest: /10.2.212.241:50010\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO DataNode.clienttrace: src: /10.2.212.241:55678, dest: /10.2.212.241:50010, bytes: 13532460, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1306063416_27, offset: 0, srvID: 53eeda0c-78a7-4507-81fe-a70e70993d4c, blockid: BP-592339435-10.2.212.241-1618438233732:blk_1073741828_1004, duration(ns): 19527575\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO datanode.DataNode: PacketResponder: BP-592339435-10.2.212.241-1618438233732:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1618438236794_0001/__spark_libs__1279074281364756631.zip is closed by DFSClient_NONMAPREDUCE_1306063416_27\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://10.2.212.241/user/root/.sparkStaging/application_1618438236794_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO hdfs.StateChange: BLOCK* allocate blk_1073741829_1005, replicas=10.2.212.241:50010 for /user/root/.sparkStaging/application_1618438236794_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO datanode.DataNode: Receiving BP-592339435-10.2.212.241-1618438233732:blk_1073741829_1005 src: /10.2.212.241:55680 dest: /10.2.212.241:50010\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO DataNode.clienttrace: src: /10.2.212.241:55680, dest: /10.2.212.241:50010, bytes: 596339, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1306063416_27, offset: 0, srvID: 53eeda0c-78a7-4507-81fe-a70e70993d4c, blockid: BP-592339435-10.2.212.241-1618438233732:blk_1073741829_1005, duration(ns): 1634077\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO datanode.DataNode: PacketResponder: BP-592339435-10.2.212.241-1618438233732:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:47 INFO namenode.FSNamesystem: BLOCK* blk_1073741829_1005 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/root/.sparkStaging/application_1618438236794_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1618438236794_0001/pyspark.zip is closed by DFSClient_NONMAPREDUCE_1306063416_27\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO yarn.Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.7-src.zip -> hdfs://10.2.212.241/user/root/.sparkStaging/application_1618438236794_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO hdfs.StateChange: BLOCK* allocate blk_1073741830_1006, replicas=10.2.212.241:50010 for /user/root/.sparkStaging/application_1618438236794_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO datanode.DataNode: Receiving BP-592339435-10.2.212.241-1618438233732:blk_1073741830_1006 src: /10.2.212.241:55682 dest: /10.2.212.241:50010\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO DataNode.clienttrace: src: /10.2.212.241:55682, dest: /10.2.212.241:50010, bytes: 42437, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1306063416_27, offset: 0, srvID: 53eeda0c-78a7-4507-81fe-a70e70993d4c, blockid: BP-592339435-10.2.212.241-1618438233732:blk_1073741830_1006, duration(ns): 969156\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO datanode.DataNode: PacketResponder: BP-592339435-10.2.212.241-1618438233732:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO namenode.FSNamesystem: BLOCK* blk_1073741830_1006 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/root/.sparkStaging/application_1618438236794_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1618438236794_0001/py4j-0.10.7-src.zip is closed by DFSClient_NONMAPREDUCE_1306063416_27\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO yarn.Client: Uploading resource file:/tmp/spark-12c9073e-38f9-4dab-8421-6e6983e8b4fa/__spark_conf__2958256639371090954.zip -> hdfs://10.2.212.241/user/root/.sparkStaging/application_1618438236794_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO hdfs.StateChange: BLOCK* allocate blk_1073741831_1007, replicas=10.2.212.241:50010 for /user/root/.sparkStaging/application_1618438236794_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO datanode.DataNode: Receiving BP-592339435-10.2.212.241-1618438233732:blk_1073741831_1007 src: /10.2.212.241:55684 dest: /10.2.212.241:50010\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO DataNode.clienttrace: src: /10.2.212.241:55684, dest: /10.2.212.241:50010, bytes: 245200, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1306063416_27, offset: 0, srvID: 53eeda0c-78a7-4507-81fe-a70e70993d4c, blockid: BP-592339435-10.2.212.241-1618438233732:blk_1073741831_1007, duration(ns): 1245664\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO datanode.DataNode: PacketResponder: BP-592339435-10.2.212.241-1618438233732:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:48 INFO namenode.FSNamesystem: BLOCK* blk_1073741831_1007 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /user/root/.sparkStaging/application_1618438236794_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:49 INFO hdfs.StateChange: DIR* completeFile: /user/root/.sparkStaging/application_1618438236794_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_1306063416_27\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:49 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:49 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:49 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m21/04/14 22:10:49 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m21/04/14 22:10:49 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO yarn.Client: Submitting application application_1618438236794_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO capacity.CapacityScheduler: Application 'application_1618438236794_0001' is submitted without priority hence considering default queue/cluster priority: 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO capacity.CapacityScheduler: Priority '0' is acceptable in queue : default for application: application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 WARN rmapp.RMAppImpl: The specific max attempts: 0 for application: 1 is invalid, because it is out of the range [1, 1]. Use the global max attempts instead.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO resourcemanager.ClientRMService: Application with id 1 submitted by user root\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO rmapp.RMAppImpl: Storing application with id application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.2.212.241#011OPERATION=Submit Application Request#011TARGET=ClientRMService#011RESULT=SUCCESS#011APPID=application_1618438236794_0001#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO rmapp.RMAppImpl: application_1618438236794_0001 State change from NEW to NEW_SAVING on event = START\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO recovery.RMStateStore: Storing info for app: application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO rmapp.RMAppImpl: application_1618438236794_0001 State change from NEW_SAVING to SUBMITTED on event = APP_NEW_SAVED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO capacity.ParentQueue: Application added - appId: application_1618438236794_0001 user: root leaf-queue of parent: root #applications: 1\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO capacity.CapacityScheduler: Accepted application application_1618438236794_0001 from user: root, in queue: default\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO rmapp.RMAppImpl: application_1618438236794_0001 State change from SUBMITTED to ACCEPTED on event = APP_ACCEPTED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO resourcemanager.ApplicationMasterService: Registering app attempt : appattempt_1618438236794_0001_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO attempt.RMAppAttemptImpl: appattempt_1618438236794_0001_000001 State change from NEW to SUBMITTED on event = START\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO capacity.LeafQueue: Application application_1618438236794_0001 from user: root activated in queue: default\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO capacity.LeafQueue: Application added - appId: application_1618438236794_0001 user: root, leaf-queue: default #user-pending-applications: 0 #user-active-applications: 1 #queue-pending-applications: 0 #queue-active-applications: 1\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO capacity.CapacityScheduler: Added Application Attempt appattempt_1618438236794_0001_000001 to scheduler from user root in queue default\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO attempt.RMAppAttemptImpl: appattempt_1618438236794_0001_000001 State change from SUBMITTED to SCHEDULED on event = ATTEMPT_ADDED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO impl.YarnClientImpl: Submitted application application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:50 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1618438236794_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1618438236794_0001_000001 container=null queue=default clusterResource=<memory:127139, vCores:16> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.0 absoluteUsedCapacity=0.0 used=<memory:0, vCores:0> cluster=<memory:127139, vCores:16>\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO rmcontainer.RMContainerImpl: container_1618438236794_0001_01_000001 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1618438236794_0001#011CONTAINERID=container_1618438236794_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:35883 for container : container_1618438236794_0001_01_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO rmcontainer.RMContainerImpl: container_1618438236794_0001_01_000001 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO security.NMTokenSecretManagerInRM: Clear node set for appattempt_1618438236794_0001_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.007047405 absoluteUsedCapacity=0.007047405 used=<memory:896, vCores:1> cluster=<memory:127139, vCores:16>\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO attempt.RMAppAttemptImpl: Storing attempt: AppId: application_1618438236794_0001 AttemptId: appattempt_1618438236794_0001_000001 MasterContainer: Container: [ContainerId: container_1618438236794_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-1:35883, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.2.212.241:35883 }, ExecutionType: GUARANTEED, ]\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO attempt.RMAppAttemptImpl: appattempt_1618438236794_0001_000001 State change from SCHEDULED to ALLOCATED_SAVING on event = CONTAINER_ALLOCATED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO attempt.RMAppAttemptImpl: appattempt_1618438236794_0001_000001 State change from ALLOCATED_SAVING to ALLOCATED on event = ATTEMPT_NEW_SAVED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO amlauncher.AMLauncher: Launching masterappattempt_1618438236794_0001_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO amlauncher.AMLauncher: Setting up container Container: [ContainerId: container_1618438236794_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-1:35883, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.2.212.241:35883 }, ExecutionType: GUARANTEED, ] for AM appattempt_1618438236794_0001_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: appattempt_1618438236794_0001_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO security.AMRMTokenSecretManager: Creating password for appattempt_1618438236794_0001_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO ipc.Server: Auth successful for appattempt_1618438236794_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO containermanager.ContainerManagerImpl: Start request for container_1618438236794_0001_01_000001 by user root\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO containermanager.ContainerManagerImpl: Creating a new application reference for app application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO yarn.Client: Application report for application_1618438236794_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO application.ApplicationImpl: Application application_1618438236794_0001 transitioned from NEW to INITING\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO application.ApplicationImpl: Adding container_1618438236794_0001_01_000001 to application application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.2.212.241#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1618438236794_0001#011CONTAINERID=container_1618438236794_0001_01_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Wed Apr 14 22:10:51 +0000 2021] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1618438250376\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1618438236794_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO application.ApplicationImpl: Application application_1618438236794_0001 transitioned from INITING to RUNNING\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO container.ContainerImpl: Container container_1618438236794_0001_01_000001 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO amlauncher.AMLauncher: Done launching container Container: [ContainerId: container_1618438236794_0001_01_000001, AllocationRequestId: 0, Version: 0, NodeId: algo-1:35883, NodeHttpAddress: algo-1:8042, Resource: <memory:896, vCores:1>, Priority: 0, Token: Token { kind: ContainerToken, service: 10.2.212.241:35883 }, ExecutionType: GUARANTEED, ] for AM appattempt_1618438236794_0001_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO attempt.RMAppAttemptImpl: appattempt_1618438236794_0001_000001 State change from ALLOCATED to LAUNCHED on event = LAUNCHED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO rmapp.RMAppImpl: update the launch time for applicationId: application_1618438236794_0001, attemptId: appattempt_1618438236794_0001_000001launchTime: 1618438251474\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO recovery.RMStateStore: Updating info for app: application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO localizer.ResourceLocalizationService: Created localizer for container_1618438236794_0001_01_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO localizer.ResourceLocalizationService: Writing credentials to the nmPrivate file /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1618438236794_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO nodemanager.DefaultContainerExecutor: Initializing user root\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO nodemanager.DefaultContainerExecutor: Copying from /tmp/hadoop-root/nm-local-dir/nmPrivate/container_1618438236794_0001_01_000001.tokens to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1618438236794_0001/container_1618438236794_0001_01_000001.tokens\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO nodemanager.DefaultContainerExecutor: Localizer CWD set to /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1618438236794_0001 = file:/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:51 INFO localizer.ContainerLocalizer: Disk Validator: yarn.nodemanager.disk-validator is loaded.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:52 INFO rmcontainer.RMContainerImpl: container_1618438236794_0001_01_000001 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:52 INFO yarn.Client: Application report for application_1618438236794_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:53 INFO yarn.Client: Application report for application_1618438236794_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:53 INFO container.ContainerImpl: Container container_1618438236794_0001_01_000001 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:53 INFO scheduler.ContainerScheduler: Starting container [container_1618438236794_0001_01_000001]\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:53 INFO container.ContainerImpl: Container container_1618438236794_0001_01_000001 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:53 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1618438236794_0001_01_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:53 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1618438236794_0001/container_1618438236794_0001_01_000001/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/prelaunch.out\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/prelaunch.err\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:54 INFO yarn.Client: Application report for application_1618438236794_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:55 INFO yarn.Client: Application report for application_1618438236794_0001 (state: ACCEPTED)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m21/04/14 22:10:56 INFO ipc.Server: Auth successful for appattempt_1618438236794_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO resourcemanager.DefaultAMSProcessor: AM registration appattempt_1618438236794_0001_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO resourcemanager.RMAuditLogger: USER=root#011IP=10.2.212.241#011OPERATION=Register App Master#011TARGET=ApplicationMasterService#011RESULT=SUCCESS#011APPID=application_1618438236794_0001#011APPATTEMPTID=appattempt_1618438236794_0001_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO attempt.RMAppAttemptImpl: appattempt_1618438236794_0001_000001 State change from LAUNCHED to RUNNING on event = REGISTERED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO rmapp.RMAppImpl: application_1618438236794_0001 State change from ACCEPTED to RUNNING on event = ATTEMPT_REGISTERED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO yarn.Client: Application report for application_1618438236794_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.212.241\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1618438250376\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1618438236794_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO cluster.YarnClientSchedulerBackend: Application application_1618438236794_0001 has started running.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO monitor.ContainersMonitorImpl: container_1618438236794_0001_01_000001's ip = 10.2.212.241, and hostname = algo-1\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33343.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO netty.NettyBlockTransferService: Server created on 10.2.212.241:33343\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1618438236794_0001_01_000001 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.2.212.241, 33343, None)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.2.212.241:33343 with 1007.8 MB RAM, BlockManagerId(driver, 10.2.212.241, 33343, None)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.2.212.241, 33343, None)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.2.212.241, 33343, None)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1618438236794_0001), /proxy/application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9eea1d{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO scheduler.EventLoggingListener: Logging events to file:/tmp/spark-events/application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:56 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0\u001b[0m\n",
      "\u001b[34m04-14 22:10 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1618438236794_0001.inprogress\u001b[0m\n",
      "\u001b[34m04-14 22:10 root         INFO     copying /tmp/spark-events/application_1618438236794_0001.inprogress to /opt/ml/processing/spark-events/application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO allocator.AbstractContainerAllocator: assignedContainer application attempt=appattempt_1618438236794_0001_000001 container=null queue=default clusterResource=<memory:127139, vCores:16> type=OFF_SWITCH requestedPartition=\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.007047405 absoluteUsedCapacity=0.007047405 used=<memory:896, vCores:1> cluster=<memory:127139, vCores:16>\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO rmcontainer.RMContainerImpl: container_1618438236794_0001_01_000002 Container Transitioned from NEW to ALLOCATED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Allocated Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1618438236794_0001#011CONTAINERID=container_1618438236794_0001_01_000002#011RESOURCE=<memory:124886, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO capacity.ParentQueue: assignedContainer queue=root usedCapacity=0.98932666 absoluteUsedCapacity=0.98932666 used=<memory:125782, vCores:2> cluster=<memory:127139, vCores:16>\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO capacity.CapacityScheduler: Allocation proposal accepted\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/lib/spark/spark-warehouse').\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO internal.SharedState: Warehouse path is 'file:/usr/lib/spark/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2409ee11{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@494512b{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@568d8b8c{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@d1b199d{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5741791{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : algo-1:35883 for container : container_1618438236794_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO rmcontainer.RMContainerImpl: container_1618438236794_0001_01_000002 Container Transitioned from ALLOCATED to ACQUIRED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO ipc.Server: Auth successful for appattempt_1618438236794_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO containermanager.ContainerManagerImpl: Start request for container_1618438236794_0001_01_000002 by user root\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO application.ApplicationImpl: Adding container_1618438236794_0001_01_000002 to application application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.2.212.241#011OPERATION=Start Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1618438236794_0001#011CONTAINERID=container_1618438236794_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO container.ContainerImpl: Container container_1618438236794_0001_01_000002 transitioned from NEW to LOCALIZING\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO containermanager.AuxServices: Got event CONTAINER_INIT for appId application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO container.ContainerImpl: Container container_1618438236794_0001_01_000002 transitioned from LOCALIZING to SCHEDULED\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO scheduler.ContainerScheduler: Starting container [container_1618438236794_0001_01_000002]\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO container.ContainerImpl: Container container_1618438236794_0001_01_000002 transitioned from SCHEDULED to RUNNING\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO monitor.ContainersMonitorImpl: Starting resource-monitoring for container_1618438236794_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO nodemanager.DefaultContainerExecutor: launchContainer: [bash, /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1618438236794_0001/container_1618438236794_0001_01_000002/default_container_executor.sh]\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/prelaunch.out\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/prelaunch.err\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout\u001b[0m\n",
      "\u001b[34mHandling create event for file: /var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:54 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:54 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:54 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:54 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:54 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:54 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:54 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:54 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:55 INFO yarn.ApplicationMaster: Preparing Local resources\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:56 INFO yarn.ApplicationMaster: ApplicationAttemptId: appattempt_1618438236794_0001_000001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:56 INFO client.RMProxy: Connecting to ResourceManager at /10.2.212.241:8030\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:56 INFO yarn.YarnRMClient: Registering the ApplicationMaster\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:56 INFO client.TransportClientFactory: Successfully created connection to /10.2.212.241:33213 after 71 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:56 INFO yarn.ApplicationMaster: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] YARN executor launch context:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]   env:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]     CLASSPATH -> /usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar<CPS>{{PWD}}<CPS>{{PWD}}/__spark_conf__<CPS>{{PWD}}/__spark_libs__/*<CPS>$HADOOP_CONF_DIR<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/*<CPS>$HADOOP_COMMON_HOME/share/hadoop/common/lib/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/*<CPS>$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/*<CPS>$HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*<CPS>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<CPS>{{PWD}}/__spark_conf__/__hadoop_conf__\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]     SPARK_YARN_STAGING_DIR -> hdfs://10.2.212.241/user/root/.sparkStaging/application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]     SPARK_NO_DAEMONIZE -> TRUE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]     SPARK_USER -> root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]     SPARK_MASTER_HOST -> 10.2.212.241\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]     SPARK_HOME -> /usr/lib/spark\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]     PYTHONPATH -> {{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]   command:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]     LD_LIBRARY_PATH=\\\"/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:$LD_LIBRARY_PATH\\\" \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       {{JAVA_HOME}}/bin/java \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       -server \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       -Xmx113533m \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       '-verbose:gc' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       '-XX:OnOutOfMemoryError=kill -9 %p' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       '-XX:+PrintGCDetails' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       '-XX:+PrintGCDateStamps' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       '-XX:+UseParallelGC' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       '-XX:InitiatingHeapOccupancyPercent=70' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       '-XX:ConcGCThreads=4' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       '-XX:ParallelGCThreads=12' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       -Djava.io.tmpdir={{PWD}}/tmp \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       '-Dspark.driver.port=33213' \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       -Dspark.yarn.app.container.log.dir=<LOG_DIR> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       org.apache.spark.executor.CoarseGrainedExecutorBackend \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       --driver-url \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       spark://CoarseGrainedScheduler@10.2.212.241:33213 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       --executor-id \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       <executorId> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       --hostname \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       <hostname> \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       --cores \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       16 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       --app-id \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       application_1618438236794_0001 \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       --user-class-path \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       file:$PWD/__app__.jar \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       1><LOG_DIR>/stdout \\ \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]       2><LOG_DIR>/stderr\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]   resources:\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]     pyspark.zip -> resource { scheme: \"hdfs\" host: \"10.2.212.241\" port: -1 file: \"/user/root/.sparkStaging/application_1618438236794_0001/pyspark.zip\" } size: 596339 timestamp: 1618438248303 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]     py4j-0.10.7-src.zip -> resource { scheme: \"hdfs\" host: \"10.2.212.241\" port: -1 file: \"/user/root/.sparkStaging/application_1618438236794_0001/py4j-0.10.7-src.zip\" } size: 42437 timestamp: 1618438248723 type: FILE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]     __spark_libs__ -> resource { scheme: \"hdfs\" host: \"10.2.212.241\" port: -1 file: \"/user/root/.sparkStaging/application_1618438236794_0001/__spark_libs__1279074281364756631.zip\" } size: 416185644 timestamp: 1618438247784 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr]     __spark_conf__ -> resource { scheme: \"hdfs\" host: \"10.2.212.241\" port: -1 file: \"/user/root/.sparkStaging/application_1618438236794_0001/__spark_conf__.zip\" } size: 245200 timestamp: 1618438249243 type: ARCHIVE visibility: PRIVATE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] ===============================================================================\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:56 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:56 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:56 INFO resource.ResourceUtils: Adding resource type - name = memory-mb, units = Mi, type = COUNTABLE\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000001/stderr] 21/04/14 22:10:56 INFO resource.ResourceUtils: Adding resource type - name = vcores, units = , type = COUNTABLE\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:57 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438[Read Raw Transactions Data as Spark DataFrame]\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:58 INFO rmcontainer.RMContainerImpl: container_1618438236794_0001_01_000002 Container Transitioned from ACQUIRED to RUNNING\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:58 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:59 INFO datasources.InMemoryFileIndex: It took 118 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m[Aggregate Transactions to Derive New Features using Spark SQL]\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:59 INFO monitor.ContainersMonitorImpl: container_1618438236794_0001_01_000002's ip = 10.2.212.241, and hostname = algo-1\u001b[0m\n",
      "\u001b[34m21/04/14 22:10:59 INFO monitor.ContainersMonitorImpl: Skipping monitoring container container_1618438236794_0001_01_000002 since CPU usage is not yet available.\u001b[0m\n",
      "\u001b[34m[Write Aggregated Features to S3]\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:00 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.212.241:40610) with ID 1\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:00 INFO scheduler.AppSchedulingInfo: checking for deactivate of application :application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:00 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:00 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:00 INFO datasources.FileSourceStrategy: Output Data Schema: struct<tid: string, event_time: timestamp, cc_num: bigint, amount: double, fraud_label: string ... 3 more fields>\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:00 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:00 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:43869 with 59.0 GB RAM, BlockManagerId(1, algo-1, 43869, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:58 INFO executor.CoarseGrainedExecutorBackend: Started daemon with process name: 1168@algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:58 INFO util.SignalUtils: Registered signal handler for TERM\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:58 INFO util.SignalUtils: Registered signal handler for HUP\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:58 INFO util.SignalUtils: Registered signal handler for INT\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO client.TransportClientFactory: Successfully created connection to /10.2.212.241:33213 after 88 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO client.TransportClientFactory: Successfully created connection to /10.2.212.241:33213 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO storage.DiskBlockManager: Created local directory at /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1618438236794_0001/blockmgr-f9dc0d26-e52b-491b-91d7-3dbd019eb787\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:10:59 INFO memory.MemoryStore: MemoryStore started with capacity 59.0 GB\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:00 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:01 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:01 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:01 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:01 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:01 INFO spark.ContextCleaner: Cleaned accumulator 2\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:01 INFO codegen.CodeGenerator: Code generated in 257.492572 ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.1 KB, free 1007.6 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KB, free 1007.5 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.2.212.241:33343 (size: 27.3 KB, free: 1007.8 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO spark.SparkContext: Created broadcast 0 from save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 10, prefetch: false\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,10))\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.DAGScheduler: Registering RDD 2 (save at NativeMethodAccessorImpl.java:0) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.DAGScheduler: Got map stage job 0 (save at NativeMethodAccessorImpl.java:0) with 10 output partitions\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 0 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KB, free 1007.5 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.4 KB, free 1007.5 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.2.212.241:33343 (size: 7.4 KB, free: 1007.8 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.DAGScheduler: Submitting 10 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO cluster.YarnScheduler: Adding task set 0.0 with 10 tasks\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2, algo-1, executor 1, partition 2, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3, algo-1, executor 1, partition 3, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4, algo-1, executor 1, partition 4, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5, algo-1, executor 1, partition 5, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6, algo-1, executor 1, partition 6, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7, algo-1, executor 1, partition 7, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 0.0 (TID 8, algo-1, executor 1, partition 8, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 0.0 (TID 9, algo-1, executor 1, partition 9, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:02 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:43869 (size: 7.4 KB, free: 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:00 INFO executor.CoarseGrainedExecutorBackend: Connecting to driver: spark://CoarseGrainedScheduler@10.2.212.241:33213\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:00 INFO executor.CoarseGrainedExecutorBackend: Successfully registered with driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:00 INFO executor.Executor: Starting executor ID 1 on host algo-1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:00 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43869.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:00 INFO netty.NettyBlockTransferService: Server created on algo-1:43869\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:00 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:00 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(1, algo-1, 43869, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:00 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(1, algo-1, 43869, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:00 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(1, algo-1, 43869, None)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO Configuration.deprecation: fs.s3a.server-side-encryption-key is deprecated. Instead, use fs.s3a.server-side-encryption.key\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.CoarseGrainedExecutorBackend: eagerFSInit: Eagerly initialized FileSystem at s3://does/not/exist in 1920 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 3\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 4\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 5\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 6\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 7\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 8\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:03 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:43869 (size: 27.3 KB, free: 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.Executor: Running task 3.0 in stage 0.0 (TID 3)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.Executor: Running task 4.0 in stage 0.0 (TID 4)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.Executor: Running task 2.0 in stage 0.0 (TID 2)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.Executor: Running task 6.0 in stage 0.0 (TID 6)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.Executor: Running task 7.0 in stage 0.0 (TID 7)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.Executor: Running task 5.0 in stage 0.0 (TID 5)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.Executor: Running task 8.0 in stage 0.0 (TID 8)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 9\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO executor.Executor: Running task 9.0 in stage 0.0 (TID 9)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 1\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO client.TransportClientFactory: Successfully created connection to /10.2.212.241:33343 after 1 ms (0 ms spent in bootstraps)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 7.4 KB, free 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO broadcast.TorrentBroadcast: Reading broadcast variable 1 took 141 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:02 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.6 KB, free 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO codegen.CodeGenerator: Code generated in 277.453332 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO codegen.CodeGenerator: Code generated in 24.367806 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO datasources.FileScanRDD: TID: 5 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 20971520-25165824, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO datasources.FileScanRDD: TID: 3 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 12582912-16777216, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:04 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 0.0 (TID 9) in 2322 ms on algo-1 (executor 1) (1/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:04 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 0.0 (TID 6) in 2627 ms on algo-1 (executor 1) (2/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 0.0 (TID 5) in 2729 ms on algo-1 (executor 1) (3/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 0.0 (TID 7) in 2746 ms on algo-1 (executor 1) (4/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 0.0 (TID 3) in 2750 ms on algo-1 (executor 1) (5/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2768 ms on algo-1 (executor 1) (6/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 0.0 (TID 8) in 2756 ms on algo-1 (executor 1) (7/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 0.0 (TID 2) in 2767 ms on algo-1 (executor 1) (8/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 2776 ms on algo-1 (executor 1) (9/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 0.0 (TID 4) in 2784 ms on algo-1 (executor 1) (10/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (save at NativeMethodAccessorImpl.java:0) finished in 2.889 s\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 2599320.\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO codegen.CodeGenerator: Code generated in 21.021898 ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO codegen.CodeGenerator: Code generated in 26.553437 ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO codegen.CodeGenerator: Code generated in 23.12808 ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO codegen.CodeGenerator: Code generated in 18.993069 ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO codegen.CodeGenerator: Code generated in 13.022966 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO datasources.FileScanRDD: TID: 9 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 37748736-39121874, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO datasources.FileScanRDD: TID: 7 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 29360128-33554432, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO datasources.FileScanRDD: TID: 8 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 33554432-37748736, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO datasources.FileScanRDD: TID: 1 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 4194304-8388608, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO datasources.FileScanRDD: TID: 0 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 0-4194304, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO datasources.FileScanRDD: TID: 4 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 16777216-20971520, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO datasources.FileScanRDD: TID: 2 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 8388608-12582912, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO datasources.FileScanRDD: TID: 6 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 25165824-29360128, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO codegen.CodeGenerator: Code generated in 12.473917 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 0\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.3 KB, free 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO broadcast.TorrentBroadcast: Reading broadcast variable 0 took 9 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:03 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 517.4 KB, free 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:04 INFO executor.Executor: Finished task 9.0 in stage 0.0 (TID 9). 3715 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO spark.SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.DAGScheduler: Got job 1 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (save at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (CoalescedRDD[22] at save at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 209.2 KB, free 1007.3 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 73.5 KB, free 1007.2 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.2.212.241:33343 (size: 73.5 KB, free: 1007.7 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (CoalescedRDD[22] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 10, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8704 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:43869 (size: 73.5 KB, free: 59.0 GB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:05 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.2.212.241:40610\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:04 INFO executor.Executor: Finished task 6.0 in stage 0.0 (TID 6). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO executor.Executor: Finished task 5.0 in stage 0.0 (TID 5). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO executor.Executor: Finished task 7.0 in stage 0.0 (TID 7). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO executor.Executor: Finished task 3.0 in stage 0.0 (TID 3). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO executor.Executor: Finished task 0.0 in stage 0.0 (TID 0). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO executor.Executor: Finished task 8.0 in stage 0.0 (TID 8). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO executor.Executor: Finished task 2.0 in stage 0.0 (TID 2). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO executor.Executor: Finished task 1.0 in stage 0.0 (TID 1). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO executor.Executor: Finished task 4.0 in stage 0.0 (TID 4). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 10\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO executor.Executor: Running task 0.0 in stage 2.0 (TID 10)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO spark.MapOutputTrackerWorker: Updating epoch to 1 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 73.5 KB, free 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO broadcast.TorrentBroadcast: Reading broadcast variable 2 took 8 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 209.2 KB, free 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO output.DirectFileOutputCommitter: Direct Write: DISABLED\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO datasources.SQLConfCommitterProvider: Using output committer class org.apache.hadoop.mapreduce.lib.output.DirectFileOutputCommitter\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 0, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.212.241:33213)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:05 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 15.530635 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 12.304764 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 7.338699 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 18.67613 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 9.942253 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 6.455747 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 8.739682 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 6.777135 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 5.65632 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 14.329038 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 5.483772 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 5.46329 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 14.688558 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 5.697376 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 7.922973 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 5.721591 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 5.532685 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 4.417897 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 4.828951 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 10.763692 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 4.810017 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 13.057238 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 8.915491 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 17.507087 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 7.312908 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:06 INFO codegen.CodeGenerator: Code generated in 9.488607 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:07 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:07 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:07 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:07 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:07 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:08 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:08 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:08 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:08 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:08 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:08 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:08 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:08 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:08 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:08 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:08 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:08 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:09 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:09 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:09 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:09 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:09 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:09 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:10 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:10 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:10 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:10 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:10 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:10 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:10 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:10 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:11 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:11 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:11 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:11 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:11 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:11 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:11 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:11 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:12 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:12 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 10) in 9708 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO scheduler.DAGScheduler: ResultStage 2 (save at NativeMethodAccessorImpl.java:0) finished in 9.732 s\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO scheduler.DAGScheduler: Job 1 finished: save at NativeMethodAccessorImpl.java:0, took 9.743931 s\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 43\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 47\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 46\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 56\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 57\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 45\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 52\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 55\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.2.212.241:33343 in memory (size: 7.4 KB, free: 1007.8 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:43869 in memory (size: 7.4 KB, free: 59.0 GB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 41\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 62\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 61\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 44\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 40\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 60\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 42\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 39\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 59\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 58\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 51\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 53\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 63\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.2.212.241:33343 in memory (size: 73.5 KB, free: 1007.8 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-1:43869 in memory (size: 73.5 KB, free: 59.0 GB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 50\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 49\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 48\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 38\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO spark.ContextCleaner: Cleaned accumulator 54\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO datasources.FileFormatWriter: Write Job 79a084bf-5e51-4533-a10d-253ed12b2b8b committed.\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:15 INFO datasources.FileFormatWriter: Finished processing stats for write job 79a084bf-5e51-4533-a10d-253ed12b2b8b.\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04[Group Aggregated Features by Card Number]\u001b[0m\n",
      "\u001b[34m[Transform Spark DataFrame Row to SageMaker Feature Store Record]\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO datasources.FileSourceStrategy: Output Data Schema: struct<tid: string, event_time: timestamp, cc_num: bigint, amount: double ... 2 more fields>\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO codegen.CodeGenerator: Code generated in 8.103343 ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 303.1 KB, free 1007.2 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.3 KB, free 1007.2 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.2.212.241:33343 (size: 27.3 KB, free: 1007.8 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO spark.SparkContext: Created broadcast 3 from javaToPython at NativeMethodAccessorImpl.java:0\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes, number of split files: 10, prefetch: false\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO execution.FileSourceScanExec: relation: None, fileSplitsInPartitionHistogram: ArrayBuffer((1 fileSplits,10))\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.DAGScheduler: Registering RDD 27 (javaToPython at NativeMethodAccessorImpl.java:0) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.DAGScheduler: Got map stage job 2 (javaToPython at NativeMethodAccessorImpl.java:0) with 10 output partitions\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 3 (javaToPython at NativeMethodAccessorImpl.java:0)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[27] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.8 KB, free 1007.2 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.2 KB, free 1007.2 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.2.212.241:33343 (size: 7.2 KB, free: 1007.8 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.DAGScheduler: Submitting 10 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[27] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9))\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO cluster.YarnScheduler: Adding task set 3.0 with 10 tasks\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 11, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 3.0 (TID 12, algo-1, executor 1, partition 1, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 3.0 (TID 13, algo-1, executor 1, partition 2, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 3.0 (TID 14, algo-1, executor 1, partition 3, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 3.0 (TID 15, algo-1, executor 1, partition 4, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 3.0 (TID 16, algo-1, executor 1, partition 5, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 3.0 (TID 17, algo-1, executor 1, partition 6, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 3.0 (TID 18, algo-1, executor 1, partition 7, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 3.0 (TID 19, algo-1, executor 1, partition 8, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 3.0 (TID 20, algo-1, executor 1, partition 9, PROCESS_LOCAL, 8303 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:43869 (size: 7.2 KB, free: 59.0 GB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:43869 (size: 27.3 KB, free: 59.0 GB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:16 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 3.0 (TID 20) in 437 ms on algo-1 (executor 1) (1/10)\u001b[0m\n",
      "\u001b[34m04-14 22:11 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1618438236794_0001.inprogress\u001b[0m\n",
      "\u001b[34m04-14 22:11 root         INFO     copying /tmp/spark-events/application_1618438236794_0001.inprogress to /opt/ml/processing/spark-events/application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 3.0 (TID 13) in 681 ms on algo-1 (executor 1) (2/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 11) in 692 ms on algo-1 (executor 1) (3/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 3.0 (TID 19) in 716 ms on algo-1 (executor 1) (4/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 3.0 (TID 16) in 729 ms on algo-1 (executor 1) (5/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 3.0 (TID 14) in 738 ms on algo-1 (executor 1) (6/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 3.0 (TID 17) in 769 ms on algo-1 (executor 1) (7/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 3.0 (TID 12) in 795 ms on algo-1 (executor 1) (8/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 3.0 (TID 15) in 813 ms on algo-1 (executor 1) (9/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 3.0 (TID 18) in 834 ms on algo-1 (executor 1) (10/10)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.DAGScheduler: ShuffleMapStage 3 (javaToPython at NativeMethodAccessorImpl.java:0) finished in 0.841 s\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO adaptive.CoalesceShufflePartitions: advisoryTargetPostShuffleInputSize: 67108864, targetPostShuffleInputSize 1932169.\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO codegen.CodeGenerator: Code generated in 8.019714 ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO codegen.CodeGenerator: Code generated in 12.208452 ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO codegen.CodeGenerator: Code generated in 12.460466 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout] 2021-04-14T22:10:59.485+0000: [GC (Allocation Failure) [PSYoungGen: 512512K->18938K(597504K)] 512512K->18962K(1963008K), 0.0134693 secs] [Times: user=0.04 sys=0.00, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout] 2021-04-14T22:10:59.608+0000: [GC (Metadata GC Threshold) [PSYoungGen: 85857K->24993K(597504K)] 85881K->25025K(1963008K), 0.0141573 secs] [Times: user=0.15 sys=0.02, real=0.01 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout] 2021-04-14T22:10:59.622+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 24993K->0K(597504K)] [ParOldGen: 32K->24705K(606720K)] 25025K->24705K(1204224K), [Metaspace: 20803K->20803K(22528K)], 0.0202449 secs] [Times: user=0.09 sys=0.02, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout] 2021-04-14T22:11:00.941+0000: [GC (Metadata GC Threshold) [PSYoungGen: 493807K->35293K(597504K)] 518513K->60007K(1204224K), 0.0132219 secs] [Times: user=0.08 sys=0.00, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout] 2021-04-14T22:11:00.954+0000: [Full GC (Metadata GC Threshold) [PSYoungGen: 35293K->0K(597504K)] [ParOldGen: 24713K->58138K(851968K)] 60007K->58138K(1449472K), [Metaspace: 34847K->34847K(36864K)], 0.0541728 secs] [Times: user=0.45 sys=0.04, real=0.05 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout] 2021-04-14T22:11:03.463+0000: [GC (Allocation Failure) [PSYoungGen: 512512K->46529K(858112K)] 570650K->104683K(1710080K), 0.0215754 secs] [Times: user=0.12 sys=0.05, real=0.02 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout] 2021-04-14T22:11:03.967+0000: [GC (Allocation Failure) [PSYoungGen: 858049K->84986K(982016K)] 916203K->698369K(1833984K), 0.0851402 secs] [Times: user=0.56 sys=0.37, real=0.08 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout] 2021-04-14T22:11:04.052+0000: [Full GC (Ergonomics) [PSYoungGen: 84986K->0K(982016K)] [ParOldGen: 613382K->687342K(2007552K)] 698369K->687342K(2989568K), [Metaspace: 52240K->52131K(53248K)], 0.1189337 secs] [Times: user=0.77 sys=0.06, real=0.12 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout] 2021-04-14T22:11:04.472+0000: [GC (Allocation Failure) [PSYoungGen: 897024K->70528K(1887744K)] 1584366K->757879K(3895296K), 0.0480812 secs] [Times: user=0.52 sys=0.05, real=0.05 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout] 2021-04-14T22:11:04.832+0000: [GC (Allocation Failure) [PSYoungGen: 1496448K->83796K(1858560K)] 2183799K->771155K(3866112K), 0.0519830 secs] [Times: user=0.52 sys=0.09, real=0.05 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout] 2021-04-14T22:11:07.437+0000: [GC (Allocation Failure) [PSYoungGen: 1509716K->207652K(2798080K)] 2197075K->895019K(4805632K), 0.0566966 secs] [Times: user=0.53 sys=0.09, real=0.06 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout] 2021-04-14T22:11:11.491+0000: [GC (Allocation Failure) [PSYoungGen: 2658596K->7068K(2855424K)] 3345963K->1480995K(4862976K), 0.0754715 secs] [Times: user=0.38 sys=0.49, real=0.08 secs] \u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stdout] 2021-04-14T22:11:11.567+0000: [Full GC (Ergonomics) [PSYoungGen: 7068K->0K(2855424K)] [ParOldGen: 1473927K->750720K(2443776K)] 1480995K->750720K(5299200K), [Metaspace: 59329K->59329K(61440K)], 0.0635790 secs] [Times: user=0.22 sys=0.00, real=0.06 secs] \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO spark.SparkContext: Starting job: collect at /opt/ml/processing/input/code/batch_aggregation.py:98\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.DAGScheduler: Got job 3 (collect at /opt/ml/processing/input/code/batch_aggregation.py:98) with 26 output partitions\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at /opt/ml/processing/input/code/batch_aggregation.py:98)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[38] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_0000/14 22:11:12 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:15 INFO output.FileOutputCommitter: Saved output of task 'attempt_20210414221105_0002_m_000000_10' to s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/aggregated\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:15 INFO mapred.SparkHadoopMapRedUtil: attempt_20210414221105_0002_m_000000_10: Committed\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:15 INFO executor.Executor: Finished task 0.0 in stage 2.0 (TID 10). 4669 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 11\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 12\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.Executor: Running task 0.0 in stage 3.0 (TID 11)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.Executor: Running task 1.0 in stage 3.0 (TID 12)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 13\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 14\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.Executor: Running task 2.0 in stage 3.0 (TID 13)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 15\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 16\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.Executor: Running task 4.0 in stage 3.0 (TID 15)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.Executor: Running task 5.0 in stage 3.0 (TID 16)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 17\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 18\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.Executor: Running task 6.0 in stage 3.0 (TID 17)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 4\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.Executor: Running task 7.0 in stage 3.0 (TID 18)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 19\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.Executor: Running task 8.0 in stage 3.0 (TID 19)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 20\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.Executor: Running task 9.0 in stage 3.0 (TID 20)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.Executor: Running task 3.0 in stage 3.0 (TID 14)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.2 KB, free 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO broadcast.TorrentBroadcast: Reading broadcast variable 4 took 10 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 13.8 KB, free 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO codegen.CodeGenerator: Code generated in 11.013574 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO datasources.FileScanRDD: TID: 15 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 16777216-20971520, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO datasources.FileScanRDD: TID: 14 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 12582912-16777216, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO datasources.FileScanRDD: TID: 13 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 8388608-12582912, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO datasources.FileScanRDD: TID: 12 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 4194304-8388608, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO datasources.FileScanRDD: TID: 11 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 0-4194304, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO datasources.FileScanRDD: TID: 17 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 25165824-29360128, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO datasources.FileScanRDD: TID: 18 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 29360128-33554432, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 42.9 KB, free 1007.1 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 17.6 KB, free 1007.1 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.2.212.241:33343 (size: 17.6 KB, free: 1007.8 MB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1203\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.DAGScheduler: Submitting 26 missing tasks from ResultStage 5 (MapPartitionsRDD[38] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO cluster.YarnScheduler: Adding task set 5.0 with 26 tasks\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 21, algo-1, executor 1, partition 0, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 5.0 (TID 22, algo-1, executor 1, partition 1, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 5.0 (TID 23, algo-1, executor 1, partition 2, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 5.0 (TID 24, algo-1, executor 1, partition 3, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 5.0 (TID 25, algo-1, executor 1, partition 4, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 5.0 (TID 26, algo-1, executor 1, partition 5, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 5.0 (TID 27, algo-1, executor 1, partition 6, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 5.0 (TID 28, algo-1, executor 1, partition 7, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 5.0 (TID 29, algo-1, executor 1, partition 8, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 5.0 (TID 30, algo-1, executor 1, partition 9, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 5.0 (TID 31, algo-1, executor 1, partition 10, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 5.0 (TID 32, algo-1, executor 1, partition 11, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 5.0 (TID 33, algo-1, executor 1, partition 12, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 5.0 (TID 34, algo-1, executor 1, partition 13, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 5.0 (TID 35, algo-1, executor 1, partition 14, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 5.0 (TID 36, algo-1, executor 1, partition 15, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:43869 (size: 17.6 KB, free: 59.0 GB)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:17 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.2.212.241:40610\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO datasources.FileScanRDD: TID: 19 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 33554432-37748736, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO datasources.FileScanRDD: TID: 20 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 37748736-39121874, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO datasources.FileScanRDD: TID: 16 - Reading current file: path: s3a://sagemaker-us-east-1-572539092864/sagemaker-featurestore-demo/raw/transactions.csv, range: 20971520-25165824, partition values: [empty row], isDataPresent: false\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO codegen.CodeGenerator: Code generated in 7.615577 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 3\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.3 KB, free 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO broadcast.TorrentBroadcast: Reading broadcast variable 3 took 7 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 517.4 KB, free 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:16 INFO executor.Executor: Finished task 9.0 in stage 3.0 (TID 20). 3629 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Finished task 2.0 in stage 3.0 (TID 13). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Finished task 0.0 in stage 3.0 (TID 11). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Finished task 8.0 in stage 3.0 (TID 19). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Finished task 5.0 in stage 3.0 (TID 16). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Finished task 3.0 in stage 3.0 (TID 14). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Finished task 6.0 in stage 3.0 (TID 17). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Finished task 1.0 in stage 3.0 (TID 12). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Finished task 4.0 in stage 3.0 (TID 15). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Finished task 7.0 in stage 3.0 (TID 18). 3672 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 21\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 22\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 0.0 in stage 5.0 (TID 21)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 1.0 in stage 5.0 (TID 22)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 23\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 2.0 in stage 5.0 (TID 23)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 24\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 3.0 in stage 5.0 (TID 24)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 4.0 in stage 5.0 (TID 25)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 26\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 27\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 5.0 in stage 5.0 (TID 26)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 6.0 in stage 5.0 (TID 27)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Updating epoch to 2 and clearing cache\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 28\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 7.0 in stage 5.0 (TID 28)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 29\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO broadcast.TorrentBroadcast: Started reading broadcast variable 5\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 8.0 in stage 5.0 (TID 29)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 30\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 31\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 32\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 9.0 in stage 5.0 (TID 30)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 11.0 in stage 5.0 (TID 32)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 33\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 10.0 in stage 5.0 (TID 31)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 34\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 12.0 in stage 5.0 (TID 33)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 35\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 13.0 in stage 5.0 (TID 34)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 36\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 14.0 in stage 5.0 (TID 35)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO executor.Executor: Running task 15.0 in stage 5.0 (TID 36)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 17.6 KB, free 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO broadcast.TorrentBroadcast: Reading broadcast variable 5 took 16 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 42.9 KB, free 59.0 GB)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker endpoint = NettyRpcEndpointRef(spark://MapOutputTracker@10.2.212.241:33213)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO spark.MapOutputTrackerWorker: Got the output locations\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 3 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 5.0 (TID 37, algo-1, executor 1, partition 16, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 5.0 (TID 38, algo-1, executor 1, partition 17, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 5.0 (TID 30) in 1248 ms on algo-1 (executor 1) (1/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 5.0 (TID 27) in 1248 ms on algo-1 (executor 1) (2/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 5.0 (TID 32) in 1249 ms on algo-1 (executor 1) (3/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 5.0 (TID 39, algo-1, executor 1, partition 18, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 5.0 (TID 40, algo-1, executor 1, partition 19, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 5.0 (TID 41, algo-1, executor 1, partition 20, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 5.0 (TID 33) in 1252 ms on algo-1 (executor 1) (4/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 5.0 (TID 42, algo-1, executor 1, partition 21, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 5.0 (TID 43, algo-1, executor 1, partition 22, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 5.0 (TID 44, algo-1, executor 1, partition 23, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 5.0 (TID 45, algo-1, executor 1, partition 24, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 5.0 (TID 46, algo-1, executor 1, partition 25, PROCESS_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 5.0 (TID 28) in 1259 ms on algo-1 (executor 1) (5/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 5.0 (TID 26) in 1260 ms on algo-1 (executor 1) (6/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 5.0 (TID 34) in 1259 ms on algo-1 (executor 1) (7/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 5.0 (TID 35) in 1259 ms on algo-1 (executor 1) (8/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 5.0 (TID 25) in 1261 ms on algo-1 (executor 1) (9/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 5.0 (TID 29) in 1261 ms on algo-1 (executor 1) (10/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 21) in 1262 ms on algo-1 (executor 1) (11/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 5.0 (TID 23) in 1263 ms on algo-1 (executor 1) (12/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 5.0 (TID 36) in 1261 ms on algo-1 (executor 1) (13/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 5.0 (TID 31) in 1262 ms on algo-1 (executor 1) (14/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 5.0 (TID 22) in 1263 ms on algo-1 (executor 1) (15/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 5.0 (TID 24) in 1263 ms on algo-1 (executor 1) (16/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 5.0 (TID 40) in 158 ms on algo-1 (executor 1) (17/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 5.0 (TID 41) in 163 ms on algo-1 (executor 1) (18/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 5.0 (TID 46) in 165 ms on algo-1 (executor 1) (19/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 5.0 (TID 44) in 166 ms on algo-1 (executor 1) (20/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 5.0 (TID 37) in 183 ms on algo-1 (executor 1) (21/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 5.0 (TID 43) in 168 ms on algo-1 (executor 1) (22/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 5.0 (TID 39) in 176 ms on algo-1 (executor 1) (23/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 5.0 (TID 45) in 168 ms on algo-1 (executor 1) (24/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 5.0 (TID 38) in 187 ms on algo-1 (executor 1) (25/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 5.0 (TID 42) in 180 ms on algo-1 (executor 1) (26/26)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.DAGScheduler: ResultStage 5 (collect at /opt/ml/processing/input/code/batch_aggregation.py:98) finished in 1.441 s\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:18 INFO scheduler.DAGScheduler: Job 3 finished: collect at /opt/ml/processing/input/code/batch_aggregation.py:98, took 1.444175 s\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_[Write Grouped Features to SageMaker Online Feature Store]\u001b[0m\n",
      "\u001b[34m000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO codegen.CodeGenerator: Code generated in 9.16793 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO codegen.CodeGenerator: Code generated in 9.527124 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO codegen.CodeGenerator: Code generated in 10.852593 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO codegen.CodeGenerator: Code generated in 15.665833 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO codegen.CodeGenerator: Code generated in 7.414921 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO codegen.CodeGenerator: Code generated in 15.98548 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:17 INFO codegen.CodeGenerator: Code generated in 9.161972 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO codegen.CodeGenerator: Code generated in 374.924019 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO codegen.CodeGenerator: Code generated in 9.069783 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO codegen.CodeGenerator: Code generated in 7.106126 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO codegen.CodeGenerator: Code generated in 22.674918 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO codegen.CodeGenerator: Code generated in 9.643575 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO codegen.CodeGenerator: Code generated in 13.41071 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO codegen.CodeGenerator: Code generated in 8.699757 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 9.0 in stage 5.0 (TID 30). 9949 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 11.0 in stage 5.0 (TID 32). 9852 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 6.0 in stage 5.0 (TID 27). 10046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 12.0 in stage 5.0 (TID 33). 10143 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 7.0 in stage 5.0 (TID 28). 9949 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 15.0 in stage 5.0 (TID 36). 10046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 3.0 in stage 5.0 (TID 24). 10143 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 1.0 in stage 5.0 (TID 22). 10106 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 5.0 in stage 5.0 (TID 26). 10046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 13.0 in stage 5.0 (TID 34). 10046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 14.0 in stage 5.0 (TID 35). 9852 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 37\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Running task 16.0 in stage 5.0 (TID 37)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 4.0 in stage 5.0 (TID 25). 10143 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 8.0 in stage 5.0 (TID 29). 10046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 0.0 in stage 5.0 (TID 21). 10143 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 38\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 39\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 2.0 in stage 5.0 (TID 23). 10046 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Running task 17.0 in stage 5.0 (TID 38)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 10.0 in stage 5.0 (TID 31). 9949 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Running task 18.0 in stage 5.0 (TID 39)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 40\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 41\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 42\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 43\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Running task 20.0 in stage 5.0 (TID 41)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 44\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Running task 19.0 in stage 5.0 (TID 40)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 45\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Running task 24.0 in stage 5.0 (TID 45)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Running task 21.0 in stage 5.0 (TID 42)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Running task 23.0 in stage 5.0 (TID 44)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Running task 22.0 in stage 5.0 (TID 43)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 46\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Running task 25.0 in stage 5.0 (TID 46)\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Getting 10 non-empty blocks including 10 local blocks and 0 remote blocks\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO storage.ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 19.0 in stage 5.0 (TID 40). 10100 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 20.0 in stage 5.0 (TID 41). 10100 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 18.0 in stage 5.0 (TID 39). 10003 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 25.0 in stage 5.0 (TID 46). 8340 bytes result sent to driver\u001b[0m\n",
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18 INFO executor.Executor: Finished task 16.0 in stage 5.0 (TID 37). 10100 bytes result sent to driver\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m[/var/log/yarn/userlogs/application_1618438236794_0001/container_1618438236794_0001_01_000002/stderr] 21/04/14 22:11:18Success = 1000\u001b[0m\n",
      "\u001b[34mFail = 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO server.AbstractConnector: Stopped Spark@4cc13af9{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO ui.SparkUI: Stopped Spark web UI at http://10.2.212.241:4040\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO cluster.YarnClientSchedulerBackend: Stopped\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-12c9073e-38f9-4dab-8421-6e6983e8b4fa/pyspark-b9435c3d-d04c-4c4d-95d3-d5e7c96ffa82\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-12c9073e-38f9-4dab-8421-6e6983e8b4fa\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-b6b7349f-bcd6-4521-acde-76e790d847fb\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO attempt.RMAppAttemptImpl: Updating application attempt appattempt_1618438236794_0001_000001 with final state: FINISHING, and exit status: -1000\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO attempt.RMAppAttemptImpl: appattempt_1618438236794_0001_000001 State change from RUNNING to FINAL_SAVING on event = UNREGISTERED\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO rmapp.RMAppImpl: Updating application application_1618438236794_0001 with final state: FINISHING\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO recovery.RMStateStore: Updating info for app: application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO rmapp.RMAppImpl: application_1618438236794_0001 State change from RUNNING to FINAL_SAVING on event = ATTEMPT_UNREGISTERED\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO attempt.RMAppAttemptImpl: appattempt_1618438236794_0001_000001 State change from FINAL_SAVING to FINISHING on event = ATTEMPT_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO rmapp.RMAppImpl: application_1618438236794_0001 State change from FINAL_SAVING to FINISHING on event = APP_UPDATE_SAVED\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO resourcemanager.ApplicationMasterService: application_1618438236794_0001 unregistered successfully. \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO launcher.ContainerLaunch: Container container_1618438236794_0001_01_000002 succeeded \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO container.ContainerImpl: Container container_1618438236794_0001_01_000002 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO launcher.ContainerLaunch: Cleaning up container container_1618438236794_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1618438236794_0001#011CONTAINERID=container_1618438236794_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1618438236794_0001/container_1618438236794_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO container.ContainerImpl: Container container_1618438236794_0001_01_000002 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO application.ApplicationImpl: Removing container_1618438236794_0001_01_000002 from application application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1618438236794_0001_01_000002\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO rmcontainer.RMContainerImpl: container_1618438236794_0001_01_000002 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1618438236794_0001#011CONTAINERID=container_1618438236794_0001_01_000002#011RESOURCE=<memory:124886, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m04-14 22:11 smspark-submit INFO     spark submit was successful. primary node exiting.\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO launcher.ContainerLaunch: Container container_1618438236794_0001_01_000001 succeeded \u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO container.ContainerImpl: Container container_1618438236794_0001_01_000001 transitioned from RUNNING to EXITED_WITH_SUCCESS\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO launcher.ContainerLaunch: Cleaning up container container_1618438236794_0001_01_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1618438236794_0001/container_1618438236794_0001_01_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO nodemanager.NMAuditLogger: USER=root#011OPERATION=Container Finished - Succeeded#011TARGET=ContainerImpl#011RESULT=SUCCESS#011APPID=application_1618438236794_0001#011CONTAINERID=container_1618438236794_0001_01_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO container.ContainerImpl: Container container_1618438236794_0001_01_000001 transitioned from EXITED_WITH_SUCCESS to DONE\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO application.ApplicationImpl: Removing container_1618438236794_0001_01_000001 from application application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO monitor.ContainersMonitorImpl: Stopping resource-monitoring for container_1618438236794_0001_01_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO containermanager.AuxServices: Got event CONTAINER_STOP for appId application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO rmcontainer.RMContainerImpl: container_1618438236794_0001_01_000001 Container Transitioned from RUNNING to COMPLETED\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO resourcemanager.ApplicationMasterService: Unregistering app attempt : appattempt_1618438236794_0001_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=AM Released Container#011TARGET=SchedulerApp#011RESULT=SUCCESS#011APPID=application_1618438236794_0001#011CONTAINERID=container_1618438236794_0001_01_000001#011RESOURCE=<memory:896, vCores:1>#011QUEUENAME=default\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO security.AMRMTokenSecretManager: Application finished, removing password for appattempt_1618438236794_0001_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO attempt.RMAppAttemptImpl: appattempt_1618438236794_0001_000001 State change from FINISHING to FINISHED on event = CONTAINER_FINISHED\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO rmapp.RMAppImpl: application_1618438236794_0001 State change from FINISHING to FINISHED on event = ATTEMPT_FINISHED\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO capacity.CapacityScheduler: Application Attempt appattempt_1618438236794_0001_000001 is done. finalState=FINISHED\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO scheduler.AppSchedulingInfo: Application application_1618438236794_0001 requests cleared\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO amlauncher.AMLauncher: Cleaning master appattempt_1618438236794_0001_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO capacity.LeafQueue: Application removed - appId: application_1618438236794_0001 user: root queue: default #user-pending-applications: 0 #user-active-applications: 0 #queue-pending-applications: 0 #queue-active-applications: 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO resourcemanager.RMAuditLogger: USER=root#011OPERATION=Application Finished - Succeeded#011TARGET=RMAppManager#011RESULT=SUCCESS#011APPID=application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO capacity.ParentQueue: Application removed - appId: application_1618438236794_0001 user: root leaf-queue of parent: root #applications: 0\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO resourcemanager.RMAppManager$ApplicationSummary: appId=application_1618438236794_0001,name=PySparkJob,user=root,queue=default,state=FINISHED,trackingUrl=http://algo-1:8088/proxy/application_1618438236794_0001/,appMasterHost=10.2.212.241,submitTime=1618438250334,startTime=1618438250376,launchTime=1618438251474,finishTime=1618438291477,finalStatus=SUCCEEDED,memorySeconds=4349640,vcoreSeconds=74,preemptedMemorySeconds=0,preemptedVcoreSeconds=0,preemptedAMContainers=0,preemptedNonAMContainers=0,preemptedResources=<memory:0\\, vCores:0>,applicationType=SPARK,resourceSeconds=4349640 MB-seconds\\, 74 vcore-seconds,preemptedResourceSeconds=0 MB-seconds\\, 0 vcore-seconds,applicationTags=,applicationNodeLabel=\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO ipc.Server: Auth successful for appattempt_1618438236794_0001_000001 (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO containermanager.ContainerManagerImpl: Stopping container with container Id: container_1618438236794_0001_01_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:31 INFO nodemanager.NMAuditLogger: USER=root#011IP=10.2.212.241#011OPERATION=Stop Container Request#011TARGET=ContainerManageImpl#011RESULT=SUCCESS#011APPID=application_1618438236794_0001#011CONTAINERID=container_1618438236794_0001_01_000001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:32 INFO nodemanager.NodeStatusUpdaterImpl: Removed completed containers from NM context: [container_1618438236794_0001_01_000001]\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:32 INFO application.ApplicationImpl: Application application_1618438236794_0001 transitioned from RUNNING to APPLICATION_RESOURCES_CLEANINGUP\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:32 INFO nodemanager.DefaultContainerExecutor: Deleting absolute path : /tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:32 INFO containermanager.AuxServices: Got event APPLICATION_STOP for appId application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:32 INFO application.ApplicationImpl: Application application_1618438236794_0001 transitioned from APPLICATION_RESOURCES_CLEANINGUP to FINISHED\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:32 INFO loghandler.NonAggregatingLogHandler: Scheduling Log Deletion for application: application_1618438236794_0001, with delay of 10800 seconds\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741825_1001 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741825 for deletion\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741826_1002 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741826 for deletion\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741827_1003 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741827 for deletion\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741828_1004 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741828 for deletion\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741829_1005 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741829 for deletion\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741830_1006 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741830 for deletion\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Scheduling blk_1073741831_1007 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741831 for deletion\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-592339435-10.2.212.241-1618438233732 blk_1073741825_1001 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741825\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-592339435-10.2.212.241-1618438233732 blk_1073741826_1002 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741826\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-592339435-10.2.212.241-1618438233732 blk_1073741827_1003 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741827\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-592339435-10.2.212.241-1618438233732 blk_1073741828_1004 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741828\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-592339435-10.2.212.241-1618438233732 blk_1073741829_1005 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741829\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-592339435-10.2.212.241-1618438233732 blk_1073741830_1006 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741830\u001b[0m\n",
      "\u001b[34m21/04/14 22:11:34 INFO impl.FsDatasetAsyncDiskService: Deleted BP-592339435-10.2.212.241-1618438233732 blk_1073741831_1007 file /opt/amazon/hadoop/hdfs/datanode/current/BP-592339435-10.2.212.241-1618438233732/current/finalized/subdir0/subdir0/blk_1073741831\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m04-14 22:11 sagemaker-spark-event-logs-publisher INFO     Got spark event logs file: application_1618438236794_0001\u001b[0m\n",
      "\u001b[34m04-14 22:11 root         INFO     copying /tmp/spark-events/application_1618438236794_0001 to /opt/ml/processing/spark-events/application_1618438236794_0001\u001b[0m\n",
      "\n",
      "CPU times: user 744 ms, sys: 50.6 ms, total: 794 ms\n",
      "Wall time: 6min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark_processor.run(submit_app='batch_aggregation.py', \n",
    "                    arguments=['--s3_input_bucket', BUCKET, \n",
    "                               '--s3_input_key_prefix', INPUT_KEY_PREFIX, \n",
    "                               '--s3_output_bucket', BUCKET, \n",
    "                               '--s3_output_key_prefix', OUTPUT_KEY_PREFIX],\n",
    "                    spark_event_logs_s3_uri='s3://{}/logs'.format(BUCKET),\n",
    "                    logs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
